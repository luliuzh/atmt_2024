{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou13kx637G_6",
        "outputId": "01fbd8a2-b4ca-45b6-f633-04389d63131e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/atmt_2024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS_dvh6a74-E",
        "outputId": "61123ff8-e658-465f-aaaa-489ba9d11038"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/atmt_2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dui7xHoh75J4",
        "outputId": "9da48a97-3445-46e9-ac18-574d1bd96240"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Collecting sacrebleu (from -r requirements.txt (line 3))\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.0.1 (from -r requirements.txt (line 4))\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 4)) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 4)) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 4)) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting portalocker (from sacrebleu->-r requirements.txt (line 3))\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r requirements.txt (line 3)) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r requirements.txt (line 3)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->-r requirements.txt (line 3))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r requirements.txt (line 3)) (5.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 4)) (1.3.0)\n",
            "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, portalocker, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, colorama, sacrebleu, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 portalocker-2.10.1 sacrebleu-2.4.3 torch-2.0.1 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install subword_nmt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLIwbdme9N90",
        "outputId": "25ea7455-295f-4c6a-c290-39cac7180c82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting subword_nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting mock (from subword_nmt)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword_nmt) (4.66.6)\n",
            "Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: mock, subword_nmt\n",
            "Successfully installed mock-5.1.0 subword_nmt-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "kupB43EjMwFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E1 bs1000\n",
        "batch-size 1000"
      ],
      "metadata": {
        "id": "OBdlWHqvG0K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "--data data/en-fr/prepared_base \\\n",
        "--source-lang fr \\\n",
        "--target-lang en \\\n",
        "--save-dir assignments/03/experiment_bs/checkpoints \\\n",
        "--batch-size 1000\\\n",
        "--cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjjA0W6h75QQ",
        "outputId": "69c36a0c-174f-4e27-f839-866e64efbed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_bs/checkpoints --batch-size 1000 --cuda\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_bs/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.259 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 1.956 | clip 0\n",
            "INFO: Epoch 000: valid_loss 8.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.68e+03\n",
            "INFO: Epoch 001: loss 8.155 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.797 | clip 0.1\n",
            "INFO: Epoch 001: valid_loss 8.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.1e+03\n",
            "INFO: Epoch 002: loss 7.814 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.519 | clip 0.9\n",
            "INFO: Epoch 002: valid_loss 7.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 1.52e+03\n",
            "INFO: Epoch 003: loss 6.972 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.69 | clip 1\n",
            "INFO: Epoch 003: valid_loss 6.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 641\n",
            "INFO: Epoch 004: loss 6.251 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.51 | clip 1\n",
            "INFO: Epoch 004: valid_loss 5.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 369\n",
            "INFO: Epoch 005: loss 5.818 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.973 | clip 1\n",
            "INFO: Epoch 005: valid_loss 5.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 266\n",
            "INFO: Epoch 006: loss 5.576 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.207 | clip 1\n",
            "INFO: Epoch 006: valid_loss 5.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 223\n",
            "INFO: Epoch 007: loss 5.452 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.355 | clip 0.7\n",
            "INFO: Epoch 007: valid_loss 5.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 204\n",
            "INFO: Epoch 008: loss 5.389 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.567 | clip 0.5\n",
            "INFO: Epoch 008: valid_loss 5.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 193\n",
            "INFO: Epoch 009: loss 5.349 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.08 | clip 0.5\n",
            "INFO: Epoch 009: valid_loss 5.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 185\n",
            "INFO: Epoch 010: loss 5.316 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.281 | clip 0.5\n",
            "INFO: Epoch 010: valid_loss 5.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 178\n",
            "INFO: Epoch 011: loss 5.282 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.419 | clip 0.6\n",
            "INFO: Epoch 011: valid_loss 5.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 172\n",
            "INFO: Epoch 012: loss 5.246 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.061 | clip 0.7\n",
            "INFO: Epoch 012: valid_loss 5.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 166\n",
            "INFO: Epoch 013: loss 5.21 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.461 | clip 0.8\n",
            "INFO: Epoch 013: valid_loss 5.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 159\n",
            "INFO: Epoch 014: loss 5.167 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.495 | clip 0.8\n",
            "INFO: Epoch 014: valid_loss 5.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 154\n",
            "INFO: Epoch 015: loss 5.13 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.62 | clip 0.8\n",
            "INFO: Epoch 015: valid_loss 5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 148\n",
            "INFO: Epoch 016: loss 5.092 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.5 | clip 0.7\n",
            "INFO: Epoch 016: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 144\n",
            "INFO: Epoch 017: loss 5.063 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.551 | clip 0.7\n",
            "INFO: Epoch 017: valid_loss 4.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 139\n",
            "INFO: Epoch 018: loss 5.026 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.346 | clip 0.7\n",
            "INFO: Epoch 018: valid_loss 4.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 135\n",
            "INFO: Epoch 019: loss 4.998 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.368 | clip 0.7\n",
            "INFO: Epoch 019: valid_loss 4.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 131\n",
            "INFO: Epoch 020: loss 4.967 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.084 | clip 0.6\n",
            "INFO: Epoch 020: valid_loss 4.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 128\n",
            "INFO: Epoch 021: loss 4.945 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.095 | clip 0.6\n",
            "INFO: Epoch 021: valid_loss 4.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 126\n",
            "INFO: Epoch 022: loss 4.924 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.061 | clip 0.6\n",
            "INFO: Epoch 022: valid_loss 4.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 123\n",
            "INFO: Epoch 023: loss 4.904 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.982 | clip 0.6\n",
            "INFO: Epoch 023: valid_loss 4.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 121\n",
            "INFO: Epoch 024: loss 4.882 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.927 | clip 0.6\n",
            "INFO: Epoch 024: valid_loss 4.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 119\n",
            "INFO: Epoch 025: loss 4.862 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.745 | clip 0.6\n",
            "INFO: Epoch 025: valid_loss 4.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 117\n",
            "INFO: Epoch 026: loss 4.842 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.622 | clip 0.6\n",
            "INFO: Epoch 026: valid_loss 4.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 115\n",
            "INFO: Epoch 027: loss 4.819 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.223 | clip 0.5\n",
            "INFO: Epoch 027: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112\n",
            "INFO: Epoch 028: loss 4.799 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.315 | clip 0.6\n",
            "INFO: Epoch 028: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 110\n",
            "INFO: Epoch 029: loss 4.775 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.733 | clip 0.4\n",
            "INFO: Epoch 029: valid_loss 4.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 108\n",
            "INFO: Epoch 030: loss 4.756 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.055 | clip 0.5\n",
            "INFO: Epoch 030: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106\n",
            "INFO: Epoch 031: loss 4.735 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.373 | clip 0.3\n",
            "INFO: Epoch 031: valid_loss 4.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104\n",
            "INFO: Epoch 032: loss 4.717 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.384 | clip 0.3\n",
            "INFO: Epoch 032: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 102\n",
            "INFO: Epoch 033: loss 4.699 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.381 | clip 0.3\n",
            "INFO: Epoch 033: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 034: loss 4.681 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.199 | clip 0.3\n",
            "INFO: Epoch 034: valid_loss 4.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 99.1\n",
            "INFO: Epoch 035: loss 4.665 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.149 | clip 0.3\n",
            "INFO: Epoch 035: valid_loss 4.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 97.7\n",
            "INFO: Epoch 036: loss 4.651 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.126 | clip 0.3\n",
            "INFO: Epoch 036: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.4\n",
            "INFO: Epoch 037: loss 4.633 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.995 | clip 0.3\n",
            "INFO: Epoch 037: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95.1\n",
            "INFO: Epoch 038: loss 4.619 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.04 | clip 0.3\n",
            "INFO: Epoch 038: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.9\n",
            "INFO: Epoch 039: loss 4.607 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.185 | clip 0.3\n",
            "INFO: Epoch 039: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 92.7\n",
            "INFO: Epoch 040: loss 4.592 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.184 | clip 0.3\n",
            "INFO: Epoch 040: valid_loss 4.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.3\n",
            "INFO: Epoch 041: loss 4.575 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.648 | clip 0.3\n",
            "INFO: Epoch 041: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90\n",
            "INFO: Epoch 042: loss 4.561 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.613 | clip 0.3\n",
            "INFO: Epoch 042: valid_loss 4.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.1\n",
            "INFO: Epoch 043: loss 4.549 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.267 | clip 0.4\n",
            "INFO: Epoch 043: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.4\n",
            "INFO: Epoch 044: loss 4.544 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.619 | clip 0.7\n",
            "INFO: Epoch 044: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.7\n",
            "INFO: Epoch 045: loss 4.514 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.415 | clip 0.2\n",
            "INFO: Epoch 045: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.5\n",
            "INFO: Epoch 046: loss 4.502 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.632 | clip 0.2\n",
            "INFO: Epoch 046: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.9\n",
            "INFO: Epoch 047: loss 4.497 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.65 | clip 0.6\n",
            "INFO: Epoch 047: valid_loss 4.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 83.1\n",
            "INFO: Epoch 048: loss 4.473 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.173 | clip 0.4\n",
            "INFO: Epoch 048: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.9\n",
            "INFO: Epoch 049: loss 4.456 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.324 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.7\n",
            "INFO: Epoch 050: loss 4.44 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.604 | clip 0.2\n",
            "INFO: Epoch 050: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.8\n",
            "INFO: Epoch 051: loss 4.429 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.591 | clip 0.5\n",
            "INFO: Epoch 051: valid_loss 4.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 78.4\n",
            "INFO: Epoch 052: loss 4.413 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.925 | clip 0.3\n",
            "INFO: Epoch 052: valid_loss 4.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.4\n",
            "INFO: Epoch 053: loss 4.395 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.37 | clip 0.2\n",
            "INFO: Epoch 053: valid_loss 4.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.2\n",
            "INFO: Epoch 054: loss 4.378 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.687 | clip 0.2\n",
            "INFO: Epoch 054: valid_loss 4.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.3\n",
            "INFO: Epoch 055: loss 4.37 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.891 | clip 0.5\n",
            "INFO: Epoch 055: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.9\n",
            "INFO: Epoch 056: loss 4.35 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.761 | clip 0.2\n",
            "INFO: Epoch 056: valid_loss 4.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.1\n",
            "INFO: Epoch 057: loss 4.333 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.327 | clip 0.2\n",
            "INFO: Epoch 057: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.8\n",
            "INFO: Epoch 058: loss 4.318 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.71 | clip 0.2\n",
            "INFO: Epoch 058: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.2\n",
            "INFO: Epoch 059: loss 4.32 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.271 | clip 0.7\n",
            "INFO: Epoch 059: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70.3\n",
            "INFO: Epoch 060: loss 4.288 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.27 | clip 0.2\n",
            "INFO: Epoch 060: valid_loss 4.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.9\n",
            "INFO: Epoch 061: loss 4.272 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.409 | clip 0.2\n",
            "INFO: Epoch 061: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.2\n",
            "INFO: Epoch 062: loss 4.275 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.833 | clip 0.7\n",
            "INFO: Epoch 062: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67\n",
            "INFO: Epoch 063: loss 4.246 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.399 | clip 0.2\n",
            "INFO: Epoch 063: valid_loss 4.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.3\n",
            "INFO: Epoch 064: loss 4.232 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.138 | clip 0.2\n",
            "INFO: Epoch 064: valid_loss 4.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.2\n",
            "INFO: Epoch 065: loss 4.22 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.829 | clip 0.3\n",
            "INFO: Epoch 065: valid_loss 4.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.4\n",
            "INFO: Epoch 066: loss 4.214 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.349 | clip 0.8\n",
            "INFO: Epoch 066: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.9\n",
            "INFO: Epoch 067: loss 4.19 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.35 | clip 0.2\n",
            "INFO: Epoch 067: valid_loss 4.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.1\n",
            "INFO: Epoch 068: loss 4.177 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.445 | clip 0.3\n",
            "INFO: Epoch 068: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.3\n",
            "INFO: Epoch 069: loss 4.175 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.365 | clip 0.5\n",
            "INFO: Epoch 069: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.1\n",
            "INFO: Epoch 070: loss 4.154 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.884 | clip 0.3\n",
            "INFO: Epoch 070: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.5\n",
            "INFO: Epoch 071: loss 4.136 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.947 | clip 0.2\n",
            "INFO: Epoch 071: valid_loss 4.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.6\n",
            "INFO: Epoch 072: loss 4.123 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.126 | clip 0.2\n",
            "INFO: Epoch 072: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59\n",
            "INFO: Epoch 073: loss 4.118 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.272 | clip 0.5\n",
            "INFO: Epoch 073: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.1\n",
            "INFO: Epoch 074: loss 4.104 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.27 | clip 0.5\n",
            "INFO: Epoch 074: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.5\n",
            "INFO: Epoch 075: loss 4.088 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.14 | clip 0.2\n",
            "INFO: Epoch 075: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.2\n",
            "INFO: Epoch 076: loss 4.075 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.598 | clip 0.5\n",
            "INFO: Epoch 076: valid_loss 4.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.5\n",
            "INFO: Epoch 077: loss 4.076 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.305 | clip 0.6\n",
            "INFO: Epoch 077: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.5\n",
            "INFO: Epoch 078: loss 4.057 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.232 | clip 0.4\n",
            "INFO: Epoch 078: valid_loss 4.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55\n",
            "INFO: Epoch 079: loss 4.037 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.814 | clip 0.2\n",
            "INFO: Epoch 079: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.4\n",
            "INFO: Epoch 080: loss 4.026 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.207 | clip 0.2\n",
            "INFO: Epoch 080: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.9\n",
            "INFO: Epoch 081: loss 4.02 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.255 | clip 0.6\n",
            "INFO: Epoch 081: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.1\n",
            "INFO: Epoch 082: loss 4.016 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.068 | clip 0.8\n",
            "INFO: Epoch 082: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53\n",
            "INFO: Epoch 083: loss 3.994 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.341 | clip 0.3\n",
            "INFO: Epoch 083: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.2\n",
            "INFO: Epoch 084: loss 3.981 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.532 | clip 0.4\n",
            "INFO: Epoch 084: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.1\n",
            "INFO: Epoch 085: loss 3.994 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.184 | clip 0.8\n",
            "INFO: Epoch 085: valid_loss 3.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.5\n",
            "INFO: Epoch 086: loss 3.963 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.843 | clip 0.2\n",
            "INFO: Epoch 086: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.4\n",
            "INFO: Epoch 087: loss 3.95 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.805 | clip 0.2\n",
            "INFO: Epoch 087: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50\n",
            "INFO: Epoch 088: loss 3.948 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.007 | clip 0.5\n",
            "INFO: Epoch 088: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.4\n",
            "INFO: Epoch 089: loss 3.929 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.813 | clip 0.2\n",
            "INFO: Epoch 089: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.2\n",
            "INFO: Epoch 090: loss 3.92 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.023 | clip 0.2\n",
            "INFO: Epoch 090: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.5\n",
            "INFO: Epoch 091: loss 3.91 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.406 | clip 0.3\n",
            "INFO: Epoch 091: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.4\n",
            "INFO: Epoch 092: loss 3.918 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.049 | clip 0.8\n",
            "INFO: Epoch 092: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.3\n",
            "INFO: Epoch 093: loss 3.892 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.286 | clip 0.3\n",
            "INFO: Epoch 093: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.4\n",
            "INFO: Epoch 094: loss 3.881 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.111 | clip 0.6\n",
            "INFO: Epoch 094: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.2\n",
            "INFO: Epoch 095: loss 3.926 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.553 | clip 0.9\n",
            "INFO: Epoch 095: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.7\n",
            "INFO: Epoch 096: loss 3.874 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.18 | clip 0.6\n",
            "INFO: Epoch 096: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.6\n",
            "INFO: Epoch 097: loss 3.876 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.786 | clip 0.8\n",
            "INFO: Epoch 097: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.4\n",
            "INFO: Epoch 098: loss 3.848 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.811 | clip 0.2\n",
            "INFO: Epoch 098: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.6\n",
            "INFO: Epoch 099: loss 3.84 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.817 | clip 0.2\n",
            "INFO: Epoch 099: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.2\n",
            "INFO: Epoch 100: loss 3.829 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.536 | clip 0.2\n",
            "INFO: Epoch 100: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.8\n",
            "INFO: Epoch 101: loss 3.817 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.442 | clip 0.2\n",
            "INFO: Epoch 101: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.4\n",
            "INFO: Epoch 102: loss 3.81 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.821 | clip 0.2\n",
            "INFO: Epoch 102: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44\n",
            "INFO: Epoch 103: loss 3.801 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.823 | clip 0.2\n",
            "INFO: Epoch 103: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.6\n",
            "INFO: Epoch 104: loss 3.786 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.251 | clip 0.1\n",
            "INFO: Epoch 104: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.3\n",
            "INFO: Epoch 105: loss 3.777 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.32 | clip 0.1\n",
            "INFO: Epoch 105: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.9\n",
            "INFO: Epoch 106: loss 3.771 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.792 | clip 0.2\n",
            "INFO: Epoch 106: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.6\n",
            "INFO: Epoch 107: loss 3.764 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.845 | clip 0.2\n",
            "INFO: Epoch 107: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.1\n",
            "INFO: Epoch 108: loss 3.751 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.269 | clip 0.1\n",
            "INFO: Epoch 108: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42\n",
            "INFO: Epoch 109: loss 3.742 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.379 | clip 0.1\n",
            "INFO: Epoch 109: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.7\n",
            "INFO: Epoch 110: loss 3.738 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.785 | clip 0.1\n",
            "INFO: Epoch 110: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.3\n",
            "INFO: Epoch 111: loss 3.73 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.187 | clip 0.3\n",
            "INFO: Epoch 111: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.9\n",
            "INFO: Epoch 112: loss 3.718 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.593 | clip 0.1\n",
            "INFO: Epoch 112: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.7\n",
            "INFO: Epoch 113: loss 3.707 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.289 | clip 0\n",
            "INFO: Epoch 113: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.6\n",
            "INFO: Epoch 114: loss 3.701 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.036 | clip 0.3\n",
            "INFO: Epoch 114: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.7\n",
            "INFO: Epoch 115: loss 3.711 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.251 | clip 0.8\n",
            "INFO: Epoch 115: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.2\n",
            "INFO: Epoch 116: loss 3.686 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.534 | clip 0.1\n",
            "INFO: Epoch 116: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.1\n",
            "INFO: Epoch 117: loss 3.684 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.386 | clip 0.6\n",
            "INFO: Epoch 117: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.2\n",
            "INFO: Epoch 118: loss 3.737 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.325 | clip 0.9\n",
            "INFO: Epoch 118: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.4\n",
            "INFO: Epoch 119: loss 3.686 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.214 | clip 0.8\n",
            "INFO: Epoch 119: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.6\n",
            "INFO: Epoch 120: loss 3.687 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.115 | clip 0.8\n",
            "INFO: Epoch 120: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.8\n",
            "INFO: Epoch 121: loss 3.66 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.49 | clip 0.4\n",
            "INFO: Epoch 121: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.9\n",
            "INFO: Epoch 122: loss 3.66 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.904 | clip 0.4\n",
            "INFO: Epoch 122: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39\n",
            "INFO: Epoch 123: loss 3.65 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.165 | clip 0.4\n",
            "INFO: Epoch 123: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.6\n",
            "INFO: Epoch 124: loss 3.636 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.366 | clip 0\n",
            "INFO: Epoch 124: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.4\n",
            "INFO: Epoch 125: loss 3.628 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.181 | clip 0.3\n",
            "INFO: Epoch 125: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.7\n",
            "INFO: Epoch 126: loss 3.648 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.916 | clip 0.8\n",
            "INFO: Epoch 126: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.6\n",
            "INFO: Epoch 127: loss 3.618 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.013 | clip 0.2\n",
            "INFO: Epoch 127: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.6\n",
            "INFO: Epoch 128: loss 3.612 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.39 | clip 0.6\n",
            "INFO: Epoch 128: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.9\n",
            "INFO: Epoch 129: loss 3.626 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.131 | clip 0.8\n",
            "INFO: Epoch 129: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.9\n",
            "INFO: Epoch 130: loss 3.603 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.778 | clip 0.3\n",
            "INFO: Epoch 130: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.1\n",
            "INFO: Epoch 131: loss 3.594 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.398 | clip 0.4\n",
            "INFO: Epoch 131: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.2\n",
            "INFO: Epoch 132: loss 3.592 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.249 | clip 0.5\n",
            "INFO: Epoch 132: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37\n",
            "INFO: Epoch 133: loss 3.577 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.546 | clip 0.1\n",
            "INFO: Epoch 133: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.8\n",
            "INFO: Epoch 134: loss 3.571 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.101 | clip 0.5\n",
            "INFO: Epoch 134: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.5\n",
            "INFO: Epoch 135: loss 3.618 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.25 | clip 0.9\n",
            "INFO: Epoch 135: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37\n",
            "INFO: Epoch 136: loss 3.586 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.087 | clip 0.7\n",
            "INFO: Epoch 136: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.2\n",
            "INFO: Epoch 137: loss 3.576 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.812 | clip 0.7\n",
            "INFO: Epoch 137: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.6\n",
            "INFO: Epoch 138: loss 3.562 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.281 | clip 0.6\n",
            "INFO: Epoch 138: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36\n",
            "INFO: Epoch 139: loss 3.561 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.27 | clip 0.6\n",
            "INFO: Epoch 139: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.1\n",
            "INFO: Epoch 140: loss 3.539 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.837 | clip 0.1\n",
            "INFO: Epoch 140: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.5\n",
            "INFO: Epoch 141: loss 3.531 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.971 | clip 0.1\n",
            "INFO: Epoch 141: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.4\n",
            "INFO: Epoch 142: loss 3.523 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.263 | clip 0.1\n",
            "INFO: Epoch 142: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.1\n",
            "INFO: Epoch 143: loss 3.516 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.339 | clip 0\n",
            "INFO: Epoch 143: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35\n",
            "INFO: Epoch 144: loss 3.513 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.132 | clip 0.3\n",
            "INFO: Epoch 144: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.7\n",
            "INFO: Epoch 145: loss 3.507 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.954 | clip 0.1\n",
            "INFO: Epoch 145: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.7\n",
            "INFO: Epoch 146: loss 3.495 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.329 | clip 0\n",
            "INFO: Epoch 146: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.4\n",
            "INFO: Epoch 147: loss 3.488 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.31 | clip 0.1\n",
            "INFO: Epoch 147: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.7\n",
            "INFO: Epoch 148: loss 3.501 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.655 | clip 0.8\n",
            "INFO: Epoch 148: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.6\n",
            "INFO: Epoch 149: loss 3.483 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.952 | clip 0.2\n",
            "INFO: Epoch 149: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8\n",
            "INFO: Epoch 150: loss 3.476 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.275 | clip 0.6\n",
            "INFO: Epoch 150: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 151: loss 3.49 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.865 | clip 0.9\n",
            "INFO: Epoch 151: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 152: loss 3.459 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.575 | clip 0.3\n",
            "INFO: Epoch 152: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.4\n",
            "INFO: Epoch 153: loss 3.459 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.075 | clip 0.4\n",
            "INFO: Epoch 153: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.6\n",
            "INFO: Epoch 154: loss 3.454 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.733 | clip 0.5\n",
            "INFO: Epoch 154: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.2\n",
            "INFO: Epoch 155: loss 3.44 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.644 | clip 0.2\n",
            "INFO: Epoch 155: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 156: loss 3.436 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.821 | clip 0.4\n",
            "INFO: Epoch 156: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.5\n",
            "INFO: Epoch 157: loss 3.464 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.926 | clip 0.9\n",
            "INFO: Epoch 157: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.3\n",
            "INFO: Epoch 158: loss 3.434 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.268 | clip 0.6\n",
            "INFO: Epoch 158: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.7\n",
            "INFO: Epoch 159: loss 3.431 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.041 | clip 0.6\n",
            "INFO: Epoch 159: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.7\n",
            "INFO: Epoch 160: loss 3.412 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.653 | clip 0.1\n",
            "INFO: Epoch 160: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.2\n",
            "INFO: Epoch 161: loss 3.405 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.936 | clip 0.1\n",
            "INFO: Epoch 161: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.2\n",
            "INFO: Epoch 162: loss 3.405 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.656 | clip 0.3\n",
            "INFO: Epoch 162: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32\n",
            "INFO: Epoch 163: loss 3.389 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.676 | clip 0.1\n",
            "INFO: Epoch 163: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.9\n",
            "INFO: Epoch 164: loss 3.385 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.714 | clip 0.4\n",
            "INFO: Epoch 164: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.2\n",
            "INFO: Epoch 165: loss 3.417 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.108 | clip 0.9\n",
            "INFO: Epoch 165: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.1\n",
            "INFO: Epoch 166: loss 3.386 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.42 | clip 0.7\n",
            "INFO: Epoch 166: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.4\n",
            "INFO: Epoch 167: loss 3.386 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.398 | clip 0.6\n",
            "INFO: Epoch 167: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.5\n",
            "INFO: Epoch 168: loss 3.364 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.048 | clip 0.1\n",
            "INFO: Epoch 168: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31\n",
            "INFO: Epoch 169: loss 3.357 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.584 | clip 0.1\n",
            "INFO: Epoch 169: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31\n",
            "INFO: Epoch 170: loss 3.354 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.515 | clip 0.2\n",
            "INFO: Epoch 170: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.7\n",
            "INFO: Epoch 171: loss 3.341 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.408 | clip 0\n",
            "INFO: Epoch 171: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.5\n",
            "INFO: Epoch 172: loss 3.33 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.912 | clip 0.1\n",
            "INFO: Epoch 172: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.4\n",
            "INFO: Epoch 173: loss 3.338 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.131 | clip 0.5\n",
            "INFO: Epoch 173: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.3\n",
            "INFO: Epoch 174: loss 3.317 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.197 | clip 0\n",
            "INFO: Epoch 174: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.9\n",
            "INFO: Epoch 175: loss 3.309 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.299 | clip 0.1\n",
            "INFO: Epoch 175: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.7\n",
            "INFO: Epoch 176: loss 3.307 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.092 | clip 0.2\n",
            "INFO: Epoch 176: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.5\n",
            "INFO: Epoch 177: loss 3.296 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.467 | clip 0.1\n",
            "INFO: Epoch 177: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.5\n",
            "INFO: Epoch 178: loss 3.285 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.283 | clip 0.1\n",
            "INFO: Epoch 178: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.2\n",
            "INFO: Epoch 179: loss 3.281 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.471 | clip 0.2\n",
            "INFO: Epoch 179: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.2\n",
            "INFO: Epoch 180: loss 3.289 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.935 | clip 0.7\n",
            "INFO: Epoch 180: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.2\n",
            "INFO: Epoch 181: loss 3.27 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.38 | clip 0.2\n",
            "INFO: Epoch 181: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 182: loss 3.259 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.926 | clip 0.4\n",
            "INFO: Epoch 182: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 183: loss 3.282 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.057 | clip 0.8\n",
            "INFO: Epoch 183: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 184: loss 3.254 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.917 | clip 0.4\n",
            "INFO: Epoch 184: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.3\n",
            "INFO: Epoch 185: loss 3.252 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.111 | clip 0.4\n",
            "INFO: Epoch 185: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.4\n",
            "INFO: Epoch 186: loss 3.233 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.601 | clip 0.1\n",
            "INFO: Epoch 186: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28\n",
            "INFO: Epoch 187: loss 3.225 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.561 | clip 0.1\n",
            "INFO: Epoch 187: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 188: loss 3.22 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.384 | clip 0.1\n",
            "INFO: Epoch 188: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.7\n",
            "INFO: Epoch 189: loss 3.209 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.205 | clip 0\n",
            "INFO: Epoch 189: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.4\n",
            "INFO: Epoch 190: loss 3.202 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.307 | clip 0\n",
            "INFO: Epoch 190: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 191: loss 3.197 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.285 | clip 0.1\n",
            "INFO: Epoch 191: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 192: loss 3.19 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.357 | clip 0.1\n",
            "INFO: Epoch 192: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.9\n",
            "INFO: Epoch 193: loss 3.18 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.375 | clip 0.1\n",
            "INFO: Epoch 193: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 194: loss 3.174 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.712 | clip 0.1\n",
            "INFO: Epoch 194: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 195: loss 3.162 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.306 | clip 0.1\n",
            "INFO: Epoch 195: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.4\n",
            "INFO: Epoch 196: loss 3.157 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.288 | clip 0.1\n",
            "INFO: Epoch 196: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 197: loss 3.149 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.644 | clip 0.1\n",
            "INFO: Epoch 197: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 198: loss 3.147 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.55 | clip 0.1\n",
            "INFO: Epoch 198: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 199: loss 3.133 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.485 | clip 0.1\n",
            "INFO: Epoch 199: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.8\n",
            "INFO: Epoch 200: loss 3.132 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.013 | clip 0.1\n",
            "INFO: Epoch 200: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 201: loss 3.123 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.927 | clip 0.1\n",
            "INFO: Epoch 201: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 202: loss 3.117 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.507 | clip 0.1\n",
            "INFO: Epoch 202: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 203: loss 3.108 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.529 | clip 0.2\n",
            "INFO: Epoch 203: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 204: loss 3.131 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.558 | clip 0.8\n",
            "INFO: Epoch 204: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 205: loss 3.099 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.727 | clip 0.2\n",
            "INFO: Epoch 205: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 206: loss 3.098 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.724 | clip 0.5\n",
            "INFO: Epoch 206: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 207: loss 3.091 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.578 | clip 0.2\n",
            "INFO: Epoch 207: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 208: loss 3.078 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.875 | clip 0.2\n",
            "INFO: Epoch 208: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 209: loss 3.076 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.418 | clip 0.6\n",
            "INFO: Epoch 209: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 210: loss 3.102 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.917 | clip 0.9\n",
            "INFO: Epoch 210: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 211: loss 3.069 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.376 | clip 0.7\n",
            "INFO: Epoch 211: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.3\n",
            "INFO: Epoch 212: loss 3.074 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.761 | clip 0.6\n",
            "INFO: Epoch 212: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.4\n",
            "INFO: Epoch 213: loss 3.048 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.747 | clip 0.1\n",
            "INFO: Epoch 213: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 214: loss 3.044 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.539 | clip 0.1\n",
            "INFO: Epoch 214: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 215: loss 3.041 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.709 | clip 0.2\n",
            "INFO: Epoch 215: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 216: loss 3.03 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.839 | clip 0.1\n",
            "INFO: Epoch 216: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 217: loss 3.029 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.963 | clip 0.5\n",
            "INFO: Epoch 217: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 218: loss 3.044 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.001 | clip 0.9\n",
            "INFO: Epoch 218: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 219: loss 3.013 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.505 | clip 0.2\n",
            "INFO: Epoch 219: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 220: loss 3.012 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.286 | clip 0.4\n",
            "INFO: Epoch 220: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 221: loss 3.008 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.866 | clip 0.2\n",
            "INFO: Epoch 221: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2\n",
            "INFO: Epoch 222: loss 2.995 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.607 | clip 0.1\n",
            "INFO: Epoch 222: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 223: loss 2.988 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.567 | clip 0.4\n",
            "INFO: Epoch 223: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 224: loss 3.005 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.911 | clip 0.8\n",
            "INFO: Epoch 224: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2\n",
            "INFO: Epoch 225: loss 2.98 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.374 | clip 0.2\n",
            "INFO: Epoch 225: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 226: loss 2.972 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.458 | clip 0.4\n",
            "INFO: Epoch 226: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 227: loss 2.973 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.703 | clip 0.2\n",
            "INFO: Epoch 227: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 228: loss 2.957 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.607 | clip 0.1\n",
            "INFO: Epoch 228: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.5\n",
            "INFO: Epoch 229: loss 2.957 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.514 | clip 0.3\n",
            "INFO: Epoch 229: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 230: loss 2.97 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.951 | clip 0.9\n",
            "INFO: Epoch 230: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 231: loss 2.946 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.559 | clip 0.1\n",
            "INFO: Epoch 231: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 232: loss 2.946 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.809 | clip 0.3\n",
            "INFO: Epoch 232: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 233: loss 2.933 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.654 | clip 0.1\n",
            "INFO: Epoch 233: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 234: loss 2.921 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.235 | clip 0.1\n",
            "INFO: Epoch 234: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 235: loss 2.922 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.702 | clip 0.1\n",
            "INFO: Epoch 235: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.7\n",
            "INFO: Epoch 236: loss 2.914 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.995 | clip 0.1\n",
            "INFO: Epoch 236: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6\n",
            "INFO: Epoch 237: loss 2.91 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.843 | clip 0.1\n",
            "INFO: Epoch 237: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5\n",
            "INFO: Epoch 238: loss 2.906 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.863 | clip 0.3\n",
            "INFO: Epoch 238: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5\n",
            "INFO: Epoch 239: loss 2.904 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.225 | clip 0.4\n",
            "INFO: Epoch 239: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 240: loss 2.892 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.756 | clip 0.1\n",
            "INFO: Epoch 240: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 241: loss 2.885 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.599 | clip 0.2\n",
            "INFO: Epoch 241: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 242: loss 2.899 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.394 | clip 0.7\n",
            "INFO: Epoch 242: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 243: loss 2.874 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.062 | clip 0.2\n",
            "INFO: Epoch 243: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 244: loss 2.871 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.092 | clip 0.4\n",
            "INFO: Epoch 244: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 245: loss 2.878 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.528 | clip 0.6\n",
            "INFO: Epoch 245: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 246: loss 2.861 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.081 | clip 0.2\n",
            "INFO: Epoch 246: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 247: loss 2.856 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.811 | clip 0.4\n",
            "INFO: Epoch 247: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8\n",
            "INFO: Epoch 248: loss 2.861 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.853 | clip 0.8\n",
            "INFO: Epoch 248: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 249: loss 2.843 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.352 | clip 0.2\n",
            "INFO: Epoch 249: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 250: loss 2.84 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.994 | clip 0.3\n",
            "INFO: Epoch 250: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 251: loss 2.838 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.271 | clip 0.4\n",
            "INFO: Epoch 251: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 252: loss 2.826 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.124 | clip 0.2\n",
            "INFO: Epoch 252: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 253: loss 2.824 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.215 | clip 0.4\n",
            "INFO: Epoch 253: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 254: loss 2.83 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.413 | clip 0.5\n",
            "INFO: Epoch 254: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 255: loss 2.81 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.824 | clip 0.1\n",
            "INFO: Epoch 255: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 256: loss 2.805 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.63 | clip 0.3\n",
            "INFO: Epoch 256: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 257: loss 2.809 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.029 | clip 0.3\n",
            "INFO: Epoch 257: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 258: loss 2.796 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.139 | clip 0.2\n",
            "INFO: Epoch 258: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7\n",
            "INFO: Epoch 259: loss 2.788 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.159 | clip 0.3\n",
            "INFO: Epoch 259: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 260: loss 2.796 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.23 | clip 0.4\n",
            "INFO: Epoch 260: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 261: loss 2.778 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.94 | clip 0.2\n",
            "INFO: Epoch 261: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 262: loss 2.773 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.912 | clip 0.3\n",
            "INFO: Epoch 262: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 263: loss 2.782 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.629 | clip 0.8\n",
            "INFO: Epoch 263: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 264: loss 2.763 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.216 | clip 0.2\n",
            "INFO: Epoch 264: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.3\n",
            "INFO: Epoch 265: loss 2.763 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.327 | clip 0.4\n",
            "INFO: Epoch 265: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 266: loss 2.762 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.148 | clip 0.4\n",
            "INFO: Epoch 266: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 267: loss 2.747 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.989 | clip 0.2\n",
            "INFO: Epoch 267: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19\n",
            "INFO: Epoch 268: loss 2.745 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.487 | clip 0.3\n",
            "INFO: Epoch 268: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 269: loss 2.75 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.689 | clip 0.6\n",
            "INFO: Epoch 269: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 270: loss 2.736 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.453 | clip 0.2\n",
            "INFO: Epoch 270: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 271: loss 2.732 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.624 | clip 0.3\n",
            "INFO: Epoch 271: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 272: loss 2.725 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.655 | clip 0.1\n",
            "INFO: Epoch 272: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 273: loss 2.719 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.519 | clip 0.1\n",
            "INFO: Epoch 273: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 274: loss 2.711 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.264 | clip 0.1\n",
            "INFO: Epoch 274: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 275: loss 2.721 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.07 | clip 0.3\n",
            "INFO: Epoch 275: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 276: loss 2.705 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.118 | clip 0.2\n",
            "INFO: Epoch 276: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 277: loss 2.702 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.42 | clip 0.4\n",
            "INFO: Epoch 277: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 278: loss 2.702 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.498 | clip 0.4\n",
            "INFO: Epoch 278: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 279: loss 2.688 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.204 | clip 0.2\n",
            "INFO: Epoch 279: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 280: loss 2.68 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.841 | clip 0.3\n",
            "INFO: Epoch 280: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 281: loss 2.688 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.067 | clip 0.4\n",
            "INFO: Epoch 281: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 282: loss 2.676 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.026 | clip 0.2\n",
            "INFO: Epoch 282: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 283: loss 2.668 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.875 | clip 0.3\n",
            "INFO: Epoch 283: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 284: loss 2.673 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.359 | clip 0.4\n",
            "INFO: Epoch 284: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 285: loss 2.658 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.838 | clip 0.2\n",
            "INFO: Epoch 285: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 286: loss 2.653 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.679 | clip 0.3\n",
            "INFO: Epoch 286: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 287: loss 2.661 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.567 | clip 0.6\n",
            "INFO: Epoch 287: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 288: loss 2.643 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.016 | clip 0.2\n",
            "INFO: Epoch 288: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 289: loss 2.642 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.295 | clip 0.3\n",
            "INFO: Epoch 289: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 290: loss 2.637 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.663 | clip 0.2\n",
            "INFO: Epoch 290: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 291: loss 2.627 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.664 | clip 0.1\n",
            "INFO: Epoch 291: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 292: loss 2.622 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.649 | clip 0.2\n",
            "INFO: Epoch 292: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 293: loss 2.634 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.02 | clip 0.7\n",
            "INFO: Epoch 293: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 294: loss 2.62 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.499 | clip 0.2\n",
            "INFO: Epoch 294: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 295: loss 2.613 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.915 | clip 0.3\n",
            "INFO: Epoch 295: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 296: loss 2.61 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.556 | clip 0.2\n",
            "INFO: Epoch 296: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 297: loss 2.598 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.511 | clip 0.1\n",
            "INFO: Epoch 297: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 298: loss 2.593 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.315 | clip 0.2\n",
            "INFO: Epoch 298: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 299: loss 2.602 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.168 | clip 0.3\n",
            "INFO: Epoch 299: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 300: loss 2.587 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.146 | clip 0.2\n",
            "INFO: Epoch 300: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 301: loss 2.584 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.429 | clip 0.3\n",
            "INFO: Epoch 301: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 302: loss 2.588 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.903 | clip 0.3\n",
            "INFO: Epoch 302: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 303: loss 2.571 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.774 | clip 0.1\n",
            "INFO: Epoch 303: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 304: loss 2.568 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.525 | clip 0.2\n",
            "INFO: Epoch 304: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 305: loss 2.576 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.603 | clip 0.5\n",
            "INFO: Epoch 305: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 306: loss 2.559 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.249 | clip 0.2\n",
            "INFO: Epoch 306: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 307: loss 2.558 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.278 | clip 0.3\n",
            "INFO: Epoch 307: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 308: loss 2.556 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.776 | clip 0.2\n",
            "INFO: Epoch 308: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 309: loss 2.548 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.618 | clip 0.1\n",
            "INFO: Epoch 309: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 310: loss 2.539 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.17 | clip 0.2\n",
            "INFO: Epoch 310: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 311: loss 2.545 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.076 | clip 0.3\n",
            "INFO: Epoch 311: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 312: loss 2.534 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.134 | clip 0.2\n",
            "INFO: Epoch 312: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 313: loss 2.526 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.347 | clip 0.3\n",
            "INFO: Epoch 313: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 314: loss 2.529 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.262 | clip 0.2\n",
            "INFO: Epoch 314: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 315: loss 2.517 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.941 | clip 0.2\n",
            "INFO: Epoch 315: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 316: loss 2.514 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.796 | clip 0.3\n",
            "INFO: Epoch 316: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 317: loss 2.516 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.978 | clip 0.3\n",
            "INFO: Epoch 317: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 318: loss 2.511 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.073 | clip 0.2\n",
            "INFO: Epoch 318: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 319: loss 2.5 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.2 | clip 0.3\n",
            "INFO: Epoch 319: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 320: loss 2.507 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.486 | clip 0.3\n",
            "INFO: Epoch 320: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 321: loss 2.49 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.283 | clip 0.2\n",
            "INFO: Epoch 321: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 322: loss 2.49 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.158 | clip 0.3\n",
            "INFO: Epoch 322: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 323: loss 2.49 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.342 | clip 0.4\n",
            "INFO: Epoch 323: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 324: loss 2.479 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.131 | clip 0.2\n",
            "INFO: Epoch 324: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 325: loss 2.475 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.112 | clip 0.3\n",
            "INFO: Epoch 325: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 326: loss 2.48 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.144 | clip 0.3\n",
            "INFO: Epoch 326: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 327: loss 2.469 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.023 | clip 0.1\n",
            "INFO: Epoch 327: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 328: loss 2.464 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.107 | clip 0.4\n",
            "INFO: Epoch 328: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 329: loss 2.466 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.484 | clip 0.3\n",
            "INFO: Epoch 329: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 330: loss 2.451 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.139 | clip 0.2\n",
            "INFO: Epoch 330: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 331: loss 2.451 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.37 | clip 0.3\n",
            "INFO: Epoch 331: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 332: loss 2.45 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.337 | clip 0.3\n",
            "INFO: Epoch 332: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 333: loss 2.443 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.131 | clip 0.2\n",
            "INFO: Epoch 333: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 334: loss 2.44 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.267 | clip 0.4\n",
            "INFO: Epoch 334: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 335: loss 2.442 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.298 | clip 0.3\n",
            "INFO: Epoch 335: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 336: loss 2.433 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.392 | clip 0.2\n",
            "INFO: Epoch 336: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 337: loss 2.423 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.307 | clip 0.4\n",
            "INFO: Epoch 337: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 338: loss 2.427 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.362 | clip 0.3\n",
            "INFO: Epoch 338: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 339: loss 2.419 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.284 | clip 0.3\n",
            "INFO: Epoch 339: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 340: loss 2.412 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.387 | clip 0.3\n",
            "INFO: Epoch 340: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 341: loss 2.419 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.63 | clip 0.4\n",
            "INFO: Epoch 341: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 342: loss 2.403 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.127 | clip 0.2\n",
            "INFO: Epoch 342: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 343: loss 2.401 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.135 | clip 0.3\n",
            "INFO: Epoch 343: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 344: loss 2.402 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.34 | clip 0.3\n",
            "INFO: Epoch 344: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 345: loss 2.395 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.442 | clip 0.2\n",
            "INFO: Epoch 345: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 346: loss 2.39 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.76 | clip 0.3\n",
            "INFO: Epoch 346: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 347: loss 2.391 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.214 | clip 0.3\n",
            "INFO: Epoch 347: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 348: loss 2.377 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.119 | clip 0.2\n",
            "INFO: Epoch 348: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 349: loss 2.377 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.128 | clip 0.4\n",
            "INFO: Epoch 349: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 350: loss 2.381 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.116 | clip 0.7\n",
            "INFO: Epoch 350: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 351: loss 2.372 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.732 | clip 0.2\n",
            "INFO: Epoch 351: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 352: loss 2.377 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.558 | clip 0.3\n",
            "INFO: Epoch 352: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 353: loss 2.362 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.328 | clip 0.1\n",
            "INFO: Epoch 353: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 354: loss 2.361 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.328 | clip 0.1\n",
            "INFO: Epoch 354: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 355: loss 2.354 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.062 | clip 0.1\n",
            "INFO: Epoch 355: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 356: loss 2.352 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.909 | clip 0.4\n",
            "INFO: Epoch 356: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 357: loss 2.347 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.47 | clip 0.3\n",
            "INFO: Epoch 357: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 358: loss 2.346 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.362 | clip 0.4\n",
            "INFO: Epoch 358: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 359: loss 2.341 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.572 | clip 0.3\n",
            "INFO: Epoch 359: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 360: loss 2.336 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.354 | clip 0.3\n",
            "INFO: Epoch 360: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 361: loss 2.33 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.174 | clip 0.2\n",
            "INFO: Epoch 361: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 362: loss 2.327 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.17 | clip 0.3\n",
            "INFO: Epoch 362: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 363: loss 2.325 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.187 | clip 0.2\n",
            "INFO: Epoch 363: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 364: loss 2.32 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.453 | clip 0.3\n",
            "INFO: Epoch 364: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 365: loss 2.317 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.395 | clip 0.2\n",
            "INFO: Epoch 365: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 366: loss 2.307 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.406 | clip 0.3\n",
            "INFO: Epoch 366: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 367: loss 2.308 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.786 | clip 0.3\n",
            "INFO: Epoch 367: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 368: loss 2.304 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.475 | clip 0.3\n",
            "INFO: Epoch 368: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 369: loss 2.297 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.167 | clip 0.2\n",
            "INFO: Epoch 369: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 370: loss 2.297 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.651 | clip 0.3\n",
            "INFO: Epoch 370: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 371: loss 2.292 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.543 | clip 0.2\n",
            "INFO: Epoch 371: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 372: loss 2.283 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.087 | clip 0.3\n",
            "INFO: Epoch 372: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 373: loss 2.282 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.668 | clip 0.2\n",
            "INFO: Epoch 373: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 374: loss 2.286 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.266 | clip 0.4\n",
            "INFO: Epoch 374: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 375: loss 2.278 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.191 | clip 0.2\n",
            "INFO: Epoch 375: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 376: loss 2.27 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.437 | clip 0.3\n",
            "INFO: Epoch 376: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 377: loss 2.275 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.037 | clip 0.4\n",
            "INFO: Epoch 377: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 378: loss 2.258 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.065 | clip 0.2\n",
            "INFO: Epoch 378: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 379: loss 2.257 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.303 | clip 0.4\n",
            "INFO: Epoch 379: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 380: loss 2.268 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.976 | clip 0.6\n",
            "INFO: Epoch 380: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 381: loss 2.254 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.6 | clip 0.2\n",
            "INFO: Epoch 381: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 382: loss 2.251 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.609 | clip 0.4\n",
            "INFO: Epoch 382: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 383: loss 2.254 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.535 | clip 0.6\n",
            "INFO: Epoch 383: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 384: loss 2.241 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.286 | clip 0.2\n",
            "INFO: Epoch 384: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 385: loss 2.236 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.264 | clip 0.3\n",
            "INFO: Epoch 385: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 386: loss 2.241 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.459 | clip 0.3\n",
            "INFO: Epoch 386: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 387: loss 2.232 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.441 | clip 0.2\n",
            "INFO: Epoch 387: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 388: loss 2.228 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.268 | clip 0.4\n",
            "INFO: Epoch 388: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 389: loss 2.237 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.381 | clip 0.8\n",
            "INFO: Epoch 389: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 390: loss 2.225 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.193 | clip 0.2\n",
            "INFO: Epoch 390: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 391: loss 2.219 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.827 | clip 0.3\n",
            "INFO: Epoch 391: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 392: loss 2.217 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.721 | clip 0.3\n",
            "INFO: Epoch 392: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 393: loss 2.211 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.862 | clip 0.3\n",
            "INFO: Epoch 393: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 394: loss 2.205 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.969 | clip 0.3\n",
            "INFO: Epoch 394: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 395: loss 2.207 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.823 | clip 0.3\n",
            "INFO: Epoch 395: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 396: loss 2.197 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.802 | clip 0.2\n",
            "INFO: Epoch 396: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 397: loss 2.197 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.254 | clip 0.3\n",
            "INFO: Epoch 397: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 398: loss 2.198 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.303 | clip 0.2\n",
            "INFO: Epoch 398: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 399: loss 2.186 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.226 | clip 0.2\n",
            "INFO: Epoch 399: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 400: loss 2.187 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.13 | clip 0.4\n",
            "INFO: Epoch 400: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 401: loss 2.194 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.89 | clip 0.7\n",
            "INFO: Epoch 401: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 402: loss 2.178 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.79 | clip 0.2\n",
            "INFO: Epoch 402: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 403: loss 2.181 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.269 | clip 0.4\n",
            "INFO: Epoch 403: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 404: loss 2.17 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.464 | clip 0.1\n",
            "INFO: Epoch 404: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 405: loss 2.165 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.996 | clip 0.2\n",
            "INFO: Epoch 405: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 406: loss 2.164 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.546 | clip 0.2\n",
            "INFO: Epoch 406: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 407: loss 2.166 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.159 | clip 0.3\n",
            "INFO: Epoch 407: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 408: loss 2.155 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.243 | clip 0.2\n",
            "INFO: Epoch 408: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 409: loss 2.151 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.457 | clip 0.4\n",
            "INFO: Epoch 409: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 410: loss 2.158 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.594 | clip 0.5\n",
            "INFO: Epoch 410: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 411: loss 2.144 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.378 | clip 0.2\n",
            "INFO: Epoch 411: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 412: loss 2.145 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.273 | clip 0.3\n",
            "INFO: Epoch 412: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 413: loss 2.145 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.519 | clip 0.4\n",
            "INFO: Epoch 413: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 414: loss 2.138 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.501 | clip 0.2\n",
            "INFO: Epoch 414: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 415: loss 2.13 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.679 | clip 0.3\n",
            "INFO: Epoch 415: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 416: loss 2.142 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.961 | clip 0.4\n",
            "INFO: Epoch 416: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 417: loss 2.13 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.764 | clip 0.2\n",
            "INFO: Epoch 417: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 418: loss 2.126 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.984 | clip 0.4\n",
            "INFO: Epoch 418: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 419: loss 2.124 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.515 | clip 0.3\n",
            "INFO: Epoch 419: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 420: loss 2.117 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.748 | clip 0.2\n",
            "INFO: Epoch 420: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 421: loss 2.12 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.971 | clip 0.3\n",
            "INFO: Epoch 421: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 422: loss 2.12 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.726 | clip 0.3\n",
            "INFO: Epoch 422: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 423: loss 2.109 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.442 | clip 0.3\n",
            "INFO: Epoch 423: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 424: loss 2.105 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.841 | clip 0.4\n",
            "INFO: Epoch 424: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 425: loss 2.107 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.578 | clip 0.4\n",
            "INFO: Epoch 425: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 426: loss 2.1 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.604 | clip 0.2\n",
            "INFO: Epoch 426: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 427: loss 2.102 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.698 | clip 0.4\n",
            "INFO: Epoch 427: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 428: loss 2.102 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.849 | clip 0.4\n",
            "INFO: Epoch 428: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 429: loss 2.088 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.364 | clip 0.2\n",
            "INFO: Epoch 429: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 430: loss 2.082 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.422 | clip 0.4\n",
            "INFO: Epoch 430: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 431: loss 2.093 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.459 | clip 0.4\n",
            "INFO: Epoch 431: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 432: loss 2.08 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.307 | clip 0.3\n",
            "INFO: Epoch 432: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 433: loss 2.073 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.469 | clip 0.3\n",
            "INFO: Epoch 433: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 434: loss 2.074 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.121 | clip 0.3\n",
            "INFO: Epoch 434: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 435: loss 2.066 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.395 | clip 0.2\n",
            "INFO: Epoch 435: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 436: loss 2.066 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.819 | clip 0.4\n",
            "INFO: Epoch 436: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 437: loss 2.067 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.349 | clip 0.4\n",
            "INFO: Epoch 437: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 438: loss 2.062 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.475 | clip 0.2\n",
            "INFO: Epoch 438: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 439: loss 2.062 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.442 | clip 0.4\n",
            "INFO: Epoch 439: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 440: loss 2.06 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.508 | clip 0.3\n",
            "INFO: Epoch 440: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 441: loss 2.046 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.307 | clip 0.2\n",
            "INFO: Epoch 441: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 442: loss 2.049 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.748 | clip 0.3\n",
            "INFO: Epoch 442: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 443: loss 2.048 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.438 | clip 0.3\n",
            "INFO: Epoch 443: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 444: loss 2.044 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.933 | clip 0.2\n",
            "INFO: Epoch 444: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 445: loss 2.038 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.931 | clip 0.4\n",
            "INFO: Epoch 445: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 446: loss 2.036 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.088 | clip 0.3\n",
            "INFO: Epoch 446: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 447: loss 2.032 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.3 | clip 0.2\n",
            "INFO: Epoch 447: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 448: loss 2.032 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.753 | clip 0.4\n",
            "INFO: Epoch 448: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 449: loss 2.039 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.351 | clip 0.3\n",
            "INFO: Epoch 449: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 450: loss 2.024 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.459 | clip 0.3\n",
            "INFO: Epoch 450: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 451: loss 2.017 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.867 | clip 0.3\n",
            "INFO: Epoch 451: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 452: loss 2.022 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.085 | clip 0.3\n",
            "INFO: Epoch 452: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 453: loss 2.018 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.296 | clip 0.2\n",
            "INFO: Epoch 453: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 454: loss 2.015 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.566 | clip 0.4\n",
            "INFO: Epoch 454: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 455: loss 2.006 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.848 | clip 0.3\n",
            "INFO: Epoch 455: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 456: loss 2.008 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.251 | clip 0.2\n",
            "INFO: Epoch 456: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 457: loss 2.001 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.175 | clip 0.3\n",
            "INFO: Epoch 457: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 458: loss 2.003 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.938 | clip 0.3\n",
            "INFO: Epoch 458: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 459: loss 1.998 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.282 | clip 0.2\n",
            "INFO: Epoch 459: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 460: loss 1.988 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.312 | clip 0.3\n",
            "INFO: Epoch 460: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 461: loss 1.995 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.176 | clip 0.3\n",
            "INFO: Epoch 461: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 462: loss 1.983 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.364 | clip 0.2\n",
            "INFO: Epoch 462: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 463: loss 1.987 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.583 | clip 0.4\n",
            "INFO: Epoch 463: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 464: loss 1.985 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.13 | clip 0.3\n",
            "INFO: Epoch 464: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 465: loss 1.972 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.154 | clip 0.2\n",
            "INFO: Epoch 465: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 466: loss 1.972 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.832 | clip 0.3\n",
            "INFO: Epoch 466: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 467: loss 1.972 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.751 | clip 0.2\n",
            "INFO: Epoch 467: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 468: loss 1.962 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.938 | clip 0.2\n",
            "INFO: Epoch 468: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 469: loss 1.962 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.741 | clip 0.2\n",
            "INFO: Epoch 469: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 470: loss 1.969 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.347 | clip 0.2\n",
            "INFO: Epoch 470: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 471: loss 1.962 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.905 | clip 0.3\n",
            "INFO: Epoch 471: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 472: loss 1.964 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.183 | clip 0.4\n",
            "INFO: Epoch 472: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 473: loss 1.96 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.106 | clip 0.3\n",
            "INFO: Epoch 473: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 474: loss 1.946 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.438 | clip 0.2\n",
            "INFO: Epoch 474: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 475: loss 1.946 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.683 | clip 0.4\n",
            "INFO: Epoch 475: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 476: loss 1.949 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.521 | clip 0.5\n",
            "INFO: Epoch 476: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 477: loss 1.944 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.732 | clip 0.3\n",
            "INFO: Epoch 477: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 478: loss 1.946 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.615 | clip 0.4\n",
            "INFO: Epoch 478: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 479: loss 1.94 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.146 | clip 0.1\n",
            "INFO: Epoch 479: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 480: loss 1.934 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.462 | clip 0.3\n",
            "INFO: Epoch 480: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 481: loss 1.93 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.909 | clip 0.4\n",
            "INFO: Epoch 481: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 482: loss 1.936 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.41 | clip 0.4\n",
            "INFO: Epoch 482: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 483: loss 1.923 | lr 0.0003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.623 | clip 0.2\n",
            "INFO: Epoch 483: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E2 bs1000+lr0.001"
      ],
      "metadata": {
        "id": "VuydLloqK5Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xi_URiteK4Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E3: bs1000+lr0.003+layer3\n",
        "bs 1000\n",
        "lr 0.003\n",
        "layer 3"
      ],
      "metadata": {
        "id": "4uKJgfrPIJsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_all/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.003 \\\n",
        "    --encoder-num-layers 3 \\\n",
        "    --decoder-num-layers 3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kajeqg7a-XKJ",
        "outputId": "6113a3b9-826a-44e1-b565-0d4b90e0ebd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_all/checkpoints --cuda --batch-size 1000 --lr 0.003 --encoder-num-layers 3 --decoder-num-layers 3\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_all/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 3, 'decoder_num_layers': 3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1771424 parameters\n",
            "INFO: Epoch 000: loss 7.002 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.131 | clip 0.7\n",
            "INFO: Epoch 000: valid_loss 5.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 205\n",
            "INFO: Epoch 001: loss 5.489 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.915 | clip 0.9\n",
            "INFO: Epoch 001: valid_loss 5.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 198\n",
            "INFO: Epoch 002: loss 5.423 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.558 | clip 0.8\n",
            "INFO: Epoch 002: valid_loss 5.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 189\n",
            "INFO: Epoch 003: loss 5.369 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.954 | clip 0.8\n",
            "INFO: Epoch 003: valid_loss 5.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 182\n",
            "INFO: Epoch 004: loss 5.322 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.593 | clip 0.8\n",
            "INFO: Epoch 004: valid_loss 5.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 171\n",
            "INFO: Epoch 005: loss 5.261 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.618 | clip 0.7\n",
            "INFO: Epoch 005: valid_loss 5.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 175\n",
            "INFO: Epoch 006: loss 5.309 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.159 | clip 0.8\n",
            "INFO: Epoch 006: valid_loss 5.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 167\n",
            "INFO: Epoch 007: loss 5.212 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.844 | clip 0.7\n",
            "INFO: Epoch 007: valid_loss 5.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 163\n",
            "INFO: Epoch 008: loss 5.22 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.032 | clip 0.8\n",
            "INFO: Epoch 008: valid_loss 5.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 159\n",
            "INFO: Epoch 009: loss 5.189 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.267 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 5.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 160\n",
            "INFO: Epoch 010: loss 5.231 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.03 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 5.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 157\n",
            "INFO: Epoch 011: loss 5.147 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.416 | clip 0.8\n",
            "INFO: Epoch 011: valid_loss 5.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 155\n",
            "INFO: Epoch 012: loss 5.19 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.05 | clip 0.9\n",
            "INFO: Epoch 012: valid_loss 5.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 152\n",
            "INFO: Epoch 013: loss 5.119 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.996 | clip 0.8\n",
            "INFO: Epoch 013: valid_loss 5.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 153\n",
            "INFO: Epoch 014: loss 5.2 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.95 | clip 1\n",
            "INFO: Epoch 014: valid_loss 5.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 150\n",
            "INFO: Epoch 015: loss 5.095 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.106 | clip 0.8\n",
            "INFO: Epoch 015: valid_loss 5.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 156\n",
            "INFO: Epoch 016: loss 5.182 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.22 | clip 0.9\n",
            "INFO: Epoch 016: valid_loss 4.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 147\n",
            "INFO: Epoch 017: loss 5.086 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.345 | clip 0.8\n",
            "INFO: Epoch 017: valid_loss 4.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 147\n",
            "INFO: Epoch 018: loss 5.116 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.1 | clip 0.8\n",
            "INFO: Epoch 018: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 145\n",
            "INFO: Epoch 019: loss 5.075 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.489 | clip 0.8\n",
            "INFO: Epoch 019: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 144\n",
            "INFO: Epoch 020: loss 5.067 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.338 | clip 0.9\n",
            "INFO: Epoch 020: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 144\n",
            "INFO: Epoch 021: loss 5.056 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.818 | clip 0.8\n",
            "INFO: Epoch 021: valid_loss 4.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 142\n",
            "INFO: Epoch 022: loss 5.061 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.21 | clip 0.9\n",
            "INFO: Epoch 022: valid_loss 4.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 142\n",
            "INFO: Epoch 023: loss 5.041 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.62 | clip 0.8\n",
            "INFO: Epoch 023: valid_loss 4.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 141\n",
            "INFO: Epoch 024: loss 5.045 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.804 | clip 0.9\n",
            "INFO: Epoch 024: valid_loss 4.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 141\n",
            "INFO: Epoch 025: loss 5.04 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.63 | clip 0.8\n",
            "INFO: Epoch 025: valid_loss 4.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 139\n",
            "INFO: Epoch 026: loss 5.031 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.19 | clip 0.8\n",
            "INFO: Epoch 026: valid_loss 4.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 140\n",
            "INFO: Epoch 027: loss 5.025 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.03 | clip 0.8\n",
            "INFO: Epoch 027: valid_loss 4.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 138\n",
            "INFO: Epoch 028: loss 5.02 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.45 | clip 0.9\n",
            "INFO: Epoch 028: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 029: loss 4.986 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.42 | clip 0.8\n",
            "INFO: Epoch 029: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 030: loss 5.045 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 12.79 | clip 1\n",
            "INFO: Epoch 030: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 031: loss 4.915 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.06 | clip 0.5\n",
            "INFO: Epoch 031: valid_loss 5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 148\n",
            "INFO: Epoch 032: loss 5.185 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.62 | clip 0.9\n",
            "INFO: Epoch 032: valid_loss 4.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 140\n",
            "INFO: Epoch 033: loss 5.049 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.41 | clip 0.8\n",
            "INFO: Epoch 033: valid_loss 5.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 150\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E4 bs1000+lr0.003+layer2"
      ],
      "metadata": {
        "id": "-Di7f16cQuSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_4/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.003 \\\n",
        "    --encoder-num-layers 2 \\\n",
        "    --decoder-num-layers 2"
      ],
      "metadata": {
        "id": "Ex1kNnz0Mizt",
        "outputId": "a905a427-1a04-4961-e862-76bcb31768f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_4/checkpoints --cuda --batch-size 1000 --lr 0.003 --encoder-num-layers 2 --decoder-num-layers 2\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_4/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1540000 parameters\n",
            "INFO: Epoch 000: loss 7.046 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.8 | clip 0.7\n",
            "INFO: Epoch 000: valid_loss 5.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 213\n",
            "INFO: Epoch 001: loss 5.494 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.868 | clip 0.8\n",
            "INFO: Epoch 001: valid_loss 5.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 205\n",
            "INFO: Epoch 002: loss 5.459 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.808 | clip 0.8\n",
            "INFO: Epoch 002: valid_loss 5.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 194\n",
            "INFO: Epoch 003: loss 5.405 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.958 | clip 0.6\n",
            "INFO: Epoch 003: valid_loss 5.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 192\n",
            "INFO: Epoch 004: loss 5.38 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.3 | clip 0.8\n",
            "INFO: Epoch 004: valid_loss 5.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 182\n",
            "INFO: Epoch 005: loss 5.331 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.579 | clip 0.7\n",
            "INFO: Epoch 005: valid_loss 5.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 188\n",
            "INFO: Epoch 006: loss 5.365 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.898 | clip 0.7\n",
            "INFO: Epoch 006: valid_loss 5.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 176\n",
            "INFO: Epoch 007: loss 5.306 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.482 | clip 0.8\n",
            "INFO: Epoch 007: valid_loss 5.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 174\n",
            "INFO: Epoch 008: loss 5.296 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.381 | clip 0.8\n",
            "INFO: Epoch 008: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 168\n",
            "INFO: Epoch 009: loss 5.229 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.422 | clip 0.7\n",
            "INFO: Epoch 009: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169\n",
            "INFO: Epoch 010: loss 5.292 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.794 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 5.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 165\n",
            "INFO: Epoch 011: loss 5.196 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.415 | clip 0.8\n",
            "INFO: Epoch 011: valid_loss 5.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 200\n",
            "INFO: Epoch 012: loss 5.415 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.04 | clip 0.9\n",
            "INFO: Epoch 012: valid_loss 5.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 170\n",
            "INFO: Epoch 013: loss 5.245 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.133 | clip 0.8\n",
            "INFO: Epoch 013: valid_loss 5.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 182\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E5 bs1000+lr0.003+layer1"
      ],
      "metadata": {
        "id": "M_c1P8rUUb-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_5/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.003"
      ],
      "metadata": {
        "id": "AMJZWW3JUkt6",
        "outputId": "46c53f44-b373-4e34-8495-d427f35ddbaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_5/checkpoints --cuda --batch-size 1000 --lr 0.003\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_5/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 7.125 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.803 | clip 0.7\n",
            "INFO: Epoch 000: valid_loss 5.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 225\n",
            "INFO: Epoch 001: loss 5.466 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.057 | clip 0.9\n",
            "INFO: Epoch 001: valid_loss 5.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 187\n",
            "INFO: Epoch 002: loss 5.32 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.231 | clip 1\n",
            "INFO: Epoch 002: valid_loss 5.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 165\n",
            "INFO: Epoch 003: loss 5.16 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.308 | clip 0.9\n",
            "INFO: Epoch 003: valid_loss 4.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 145\n",
            "INFO: Epoch 004: loss 5.042 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.017 | clip 1\n",
            "INFO: Epoch 004: valid_loss 4.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 130\n",
            "INFO: Epoch 005: loss 4.968 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.203 | clip 0.9\n",
            "INFO: Epoch 005: valid_loss 4.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 118\n",
            "INFO: Epoch 006: loss 4.841 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.181 | clip 0.6\n",
            "INFO: Epoch 006: valid_loss 4.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 115\n",
            "INFO: Epoch 007: loss 4.821 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.945 | clip 0.8\n",
            "INFO: Epoch 007: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 105\n",
            "INFO: Epoch 008: loss 4.747 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.939 | clip 0.8\n",
            "INFO: Epoch 008: valid_loss 4.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 99.4\n",
            "INFO: Epoch 009: loss 4.669 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.202 | clip 0.6\n",
            "INFO: Epoch 009: valid_loss 4.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95.9\n",
            "INFO: Epoch 010: loss 4.617 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.682 | clip 0.4\n",
            "INFO: Epoch 010: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.4\n",
            "INFO: Epoch 011: loss 4.56 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.654 | clip 0.3\n",
            "INFO: Epoch 011: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.2\n",
            "INFO: Epoch 012: loss 4.522 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.637 | clip 0.8\n",
            "INFO: Epoch 012: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.1\n",
            "INFO: Epoch 013: loss 4.54 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.025 | clip 0.9\n",
            "INFO: Epoch 013: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.8\n",
            "INFO: Epoch 014: loss 4.405 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.014 | clip 0.4\n",
            "INFO: Epoch 014: valid_loss 4.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.5\n",
            "INFO: Epoch 015: loss 4.353 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.442 | clip 0.7\n",
            "INFO: Epoch 015: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.7\n",
            "INFO: Epoch 016: loss 4.28 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.795 | clip 0.6\n",
            "INFO: Epoch 016: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70\n",
            "INFO: Epoch 017: loss 4.241 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.345 | clip 0.7\n",
            "INFO: Epoch 017: valid_loss 4.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65\n",
            "INFO: Epoch 018: loss 4.174 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.509 | clip 0.7\n",
            "INFO: Epoch 018: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.9\n",
            "INFO: Epoch 019: loss 4.096 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.278 | clip 0.4\n",
            "INFO: Epoch 019: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.2\n",
            "INFO: Epoch 020: loss 4.019 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.717 | clip 0.3\n",
            "INFO: Epoch 020: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.6\n",
            "INFO: Epoch 021: loss 3.95 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.313 | clip 0.3\n",
            "INFO: Epoch 021: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.6\n",
            "INFO: Epoch 022: loss 3.888 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.396 | clip 0.3\n",
            "INFO: Epoch 022: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.8\n",
            "INFO: Epoch 023: loss 3.829 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.451 | clip 0.3\n",
            "INFO: Epoch 023: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.6\n",
            "INFO: Epoch 024: loss 3.794 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.476 | clip 0.6\n",
            "INFO: Epoch 024: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.2\n",
            "INFO: Epoch 025: loss 3.804 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.726 | clip 0.8\n",
            "INFO: Epoch 025: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.9\n",
            "INFO: Epoch 026: loss 3.68 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.905 | clip 0.1\n",
            "INFO: Epoch 026: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.4\n",
            "INFO: Epoch 027: loss 3.604 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.454 | clip 0.2\n",
            "INFO: Epoch 027: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.6\n",
            "INFO: Epoch 028: loss 3.564 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.284 | clip 0.3\n",
            "INFO: Epoch 028: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.6\n",
            "INFO: Epoch 029: loss 3.546 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.826 | clip 0.6\n",
            "INFO: Epoch 029: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.3\n",
            "INFO: Epoch 030: loss 3.51 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.852 | clip 0.7\n",
            "INFO: Epoch 030: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.5\n",
            "INFO: Epoch 031: loss 3.433 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.755 | clip 0.1\n",
            "INFO: Epoch 031: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.4\n",
            "INFO: Epoch 032: loss 3.385 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.892 | clip 0.1\n",
            "INFO: Epoch 032: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34\n",
            "INFO: Epoch 033: loss 3.398 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.857 | clip 0.7\n",
            "INFO: Epoch 033: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.6\n",
            "INFO: Epoch 034: loss 3.362 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.488 | clip 0.6\n",
            "INFO: Epoch 034: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.8\n",
            "INFO: Epoch 035: loss 3.284 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.242 | clip 0.1\n",
            "INFO: Epoch 035: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.6\n",
            "INFO: Epoch 036: loss 3.242 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.32 | clip 0\n",
            "INFO: Epoch 036: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.2\n",
            "INFO: Epoch 037: loss 3.246 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.654 | clip 0.3\n",
            "INFO: Epoch 037: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.2\n",
            "INFO: Epoch 038: loss 3.216 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.357 | clip 0.4\n",
            "INFO: Epoch 038: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.1\n",
            "INFO: Epoch 039: loss 3.182 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.948 | clip 0.1\n",
            "INFO: Epoch 039: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.2\n",
            "INFO: Epoch 040: loss 3.131 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.594 | clip 0.1\n",
            "INFO: Epoch 040: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.9\n",
            "INFO: Epoch 041: loss 3.098 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.3 | clip 0.1\n",
            "INFO: Epoch 041: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.2\n",
            "INFO: Epoch 042: loss 3.06 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.296 | clip 0\n",
            "INFO: Epoch 042: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.5\n",
            "INFO: Epoch 043: loss 3.049 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.987 | clip 0.1\n",
            "INFO: Epoch 043: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.6\n",
            "INFO: Epoch 044: loss 3.085 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.242 | clip 0.7\n",
            "INFO: Epoch 044: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.4\n",
            "INFO: Epoch 045: loss 3.001 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.508 | clip 0.1\n",
            "INFO: Epoch 045: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 046: loss 2.96 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.316 | clip 0.1\n",
            "INFO: Epoch 046: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 047: loss 2.937 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.084 | clip 0\n",
            "INFO: Epoch 047: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 048: loss 2.92 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.669 | clip 0\n",
            "INFO: Epoch 048: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 049: loss 2.938 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.307 | clip 0.6\n",
            "INFO: Epoch 049: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 050: loss 2.899 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.859 | clip 0.3\n",
            "INFO: Epoch 050: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.3\n",
            "INFO: Epoch 051: loss 2.847 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.577 | clip 0.1\n",
            "INFO: Epoch 051: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 052: loss 2.806 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.32 | clip 0\n",
            "INFO: Epoch 052: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 053: loss 2.846 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.843 | clip 0.6\n",
            "INFO: Epoch 053: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 054: loss 2.789 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.082 | clip 0.1\n",
            "INFO: Epoch 054: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 055: loss 2.733 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 1.938 | clip 0\n",
            "INFO: Epoch 055: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 056: loss 2.714 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.116 | clip 0\n",
            "INFO: Epoch 056: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 057: loss 2.727 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.486 | clip 0.4\n",
            "INFO: Epoch 057: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.7\n",
            "INFO: Epoch 058: loss 2.714 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.724 | clip 0.4\n",
            "INFO: Epoch 058: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 059: loss 2.659 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.308 | clip 0.1\n",
            "INFO: Epoch 059: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.2\n",
            "INFO: Epoch 060: loss 2.615 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.213 | clip 0\n",
            "INFO: Epoch 060: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 061: loss 2.63 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.704 | clip 0.5\n",
            "INFO: Epoch 061: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 062: loss 2.621 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.853 | clip 0.4\n",
            "INFO: Epoch 062: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 063: loss 2.56 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.212 | clip 0\n",
            "INFO: Epoch 063: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 064: loss 2.532 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.497 | clip 0.1\n",
            "INFO: Epoch 064: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 065: loss 2.582 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.169 | clip 0.6\n",
            "INFO: Epoch 065: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 066: loss 2.501 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.357 | clip 0\n",
            "INFO: Epoch 066: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 067: loss 2.463 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.236 | clip 0.1\n",
            "INFO: Epoch 067: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 068: loss 2.457 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.956 | clip 0.1\n",
            "INFO: Epoch 068: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 069: loss 2.423 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.355 | clip 0.1\n",
            "INFO: Epoch 069: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 070: loss 2.394 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.294 | clip 0.1\n",
            "INFO: Epoch 070: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 071: loss 2.364 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.186 | clip 0.1\n",
            "INFO: Epoch 071: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 072: loss 2.331 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.255 | clip 0.1\n",
            "INFO: Epoch 072: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 073: loss 2.31 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.173 | clip 0.1\n",
            "INFO: Epoch 073: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 074: loss 2.28 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.199 | clip 0.1\n",
            "INFO: Epoch 074: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 075: loss 2.254 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.204 | clip 0.1\n",
            "INFO: Epoch 075: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 076: loss 2.239 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.391 | clip 0\n",
            "INFO: Epoch 076: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 077: loss 2.213 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.274 | clip 0.1\n",
            "INFO: Epoch 077: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 078: loss 2.195 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.28 | clip 0.1\n",
            "INFO: Epoch 078: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 079: loss 2.172 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.27 | clip 0.1\n",
            "INFO: Epoch 079: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 080: loss 2.148 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.275 | clip 0.1\n",
            "INFO: Epoch 080: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 081: loss 2.141 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.652 | clip 0.1\n",
            "INFO: Epoch 081: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 082: loss 2.115 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.273 | clip 0.1\n",
            "INFO: Epoch 082: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 083: loss 2.138 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.421 | clip 0.2\n",
            "INFO: Epoch 083: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 084: loss 2.094 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.626 | clip 0.1\n",
            "INFO: Epoch 084: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 085: loss 2.134 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.518 | clip 0.2\n",
            "INFO: Epoch 085: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 086: loss 2.074 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.187 | clip 0.2\n",
            "INFO: Epoch 086: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 087: loss 2.025 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.196 | clip 0.1\n",
            "INFO: Epoch 087: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 088: loss 1.996 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.277 | clip 0.1\n",
            "INFO: Epoch 088: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 089: loss 2.006 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.906 | clip 0.1\n",
            "INFO: Epoch 089: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 090: loss 1.984 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.619 | clip 0.1\n",
            "INFO: Epoch 090: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 091: loss 1.972 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.753 | clip 0.1\n",
            "INFO: Epoch 091: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 092: loss 1.947 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.623 | clip 0.1\n",
            "INFO: Epoch 092: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 093: loss 1.92 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.461 | clip 0.1\n",
            "INFO: Epoch 093: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 094: loss 1.903 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.593 | clip 0.1\n",
            "INFO: Epoch 094: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 095: loss 1.932 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.488 | clip 0.5\n",
            "INFO: Epoch 095: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 096: loss 1.896 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.164 | clip 0.2\n",
            "INFO: Epoch 096: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 097: loss 1.893 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.821 | clip 0.1\n",
            "INFO: Epoch 097: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 098: loss 1.867 | lr 0.003 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.073 | clip 0.1\n",
            "INFO: Epoch 098: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E6 bs1000+lr0.01+layer1"
      ],
      "metadata": {
        "id": "yIGZwtopXpUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_6/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.01"
      ],
      "metadata": {
        "id": "i5nH2TasYDtv",
        "outputId": "f4444fa7-a823-444b-f343-edbe4a8fdf04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_6/checkpoints --cuda --batch-size 1000 --lr 0.01\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.01, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_6/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 6.218 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.377 | clip 0.8\n",
            "INFO: Epoch 000: valid_loss 5.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 171\n",
            "INFO: Epoch 001: loss 5.195 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.043 | clip 1\n",
            "INFO: Epoch 001: valid_loss 4.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 129\n",
            "INFO: Epoch 002: loss 5.011 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.455 | clip 1\n",
            "INFO: Epoch 002: valid_loss 4.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 131\n",
            "INFO: Epoch 003: loss 4.963 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.98 | clip 1\n",
            "INFO: Epoch 003: valid_loss 4.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 108\n",
            "INFO: Epoch 004: loss 4.739 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.338 | clip 0.9\n",
            "INFO: Epoch 004: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93\n",
            "INFO: Epoch 005: loss 4.604 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.746 | clip 0.8\n",
            "INFO: Epoch 005: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.6\n",
            "INFO: Epoch 006: loss 4.485 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.458 | clip 0.9\n",
            "INFO: Epoch 006: valid_loss 4.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.9\n",
            "INFO: Epoch 007: loss 4.38 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.917 | clip 0.8\n",
            "INFO: Epoch 007: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.1\n",
            "INFO: Epoch 008: loss 4.283 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.51 | clip 0.8\n",
            "INFO: Epoch 008: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.2\n",
            "INFO: Epoch 009: loss 4.179 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.119 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 4.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.6\n",
            "INFO: Epoch 010: loss 4.114 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.172 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 4.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.7\n",
            "INFO: Epoch 011: loss 3.98 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.808 | clip 0.7\n",
            "INFO: Epoch 011: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.1\n",
            "INFO: Epoch 012: loss 3.882 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.572 | clip 0.8\n",
            "INFO: Epoch 012: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.2\n",
            "INFO: Epoch 013: loss 3.799 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.784 | clip 0.8\n",
            "INFO: Epoch 013: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.5\n",
            "INFO: Epoch 014: loss 3.732 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.116 | clip 0.7\n",
            "INFO: Epoch 014: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.5\n",
            "INFO: Epoch 015: loss 3.605 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.721 | clip 0.6\n",
            "INFO: Epoch 015: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.6\n",
            "INFO: Epoch 016: loss 3.543 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.198 | clip 0.7\n",
            "INFO: Epoch 016: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37\n",
            "INFO: Epoch 017: loss 3.444 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.354 | clip 0.5\n",
            "INFO: Epoch 017: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.4\n",
            "INFO: Epoch 018: loss 3.398 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.765 | clip 0.6\n",
            "INFO: Epoch 018: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.5\n",
            "INFO: Epoch 019: loss 3.334 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.92 | clip 0.4\n",
            "INFO: Epoch 019: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 020: loss 3.282 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.547 | clip 0.3\n",
            "INFO: Epoch 020: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.3\n",
            "INFO: Epoch 021: loss 3.224 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.385 | clip 0.3\n",
            "INFO: Epoch 021: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32\n",
            "INFO: Epoch 022: loss 3.193 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.8 | clip 0.5\n",
            "INFO: Epoch 022: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.6\n",
            "INFO: Epoch 023: loss 3.233 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.385 | clip 0.8\n",
            "INFO: Epoch 023: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.6\n",
            "INFO: Epoch 024: loss 3.108 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.265 | clip 0.2\n",
            "INFO: Epoch 024: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.9\n",
            "INFO: Epoch 025: loss 3.041 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.743 | clip 0.1\n",
            "INFO: Epoch 025: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 026: loss 3.009 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.346 | clip 0.2\n",
            "INFO: Epoch 026: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 027: loss 2.975 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.43 | clip 0.3\n",
            "INFO: Epoch 027: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28\n",
            "INFO: Epoch 028: loss 2.937 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.286 | clip 0.2\n",
            "INFO: Epoch 028: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 029: loss 2.896 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.955 | clip 0.1\n",
            "INFO: Epoch 029: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 030: loss 2.852 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.367 | clip 0.1\n",
            "INFO: Epoch 030: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.2\n",
            "INFO: Epoch 031: loss 2.832 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.705 | clip 0.1\n",
            "INFO: Epoch 031: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.4\n",
            "INFO: Epoch 032: loss 2.79 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.755 | clip 0.1\n",
            "INFO: Epoch 032: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 033: loss 2.749 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.463 | clip 0\n",
            "INFO: Epoch 033: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 034: loss 2.725 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.5 | clip 0\n",
            "INFO: Epoch 034: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 035: loss 2.693 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.671 | clip 0.1\n",
            "INFO: Epoch 035: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.5\n",
            "INFO: Epoch 036: loss 2.683 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.674 | clip 0.1\n",
            "INFO: Epoch 036: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 037: loss 2.653 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.445 | clip 0\n",
            "INFO: Epoch 037: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 038: loss 2.626 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.491 | clip 0.1\n",
            "INFO: Epoch 038: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 039: loss 2.61 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.672 | clip 0.1\n",
            "INFO: Epoch 039: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 040: loss 2.578 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.297 | clip 0.1\n",
            "INFO: Epoch 040: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.4\n",
            "INFO: Epoch 041: loss 2.549 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.103 | clip 0.1\n",
            "INFO: Epoch 041: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 042: loss 2.535 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.169 | clip 0\n",
            "INFO: Epoch 042: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 043: loss 2.532 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.479 | clip 0.1\n",
            "INFO: Epoch 043: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 044: loss 2.51 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.289 | clip 0.1\n",
            "INFO: Epoch 044: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 045: loss 2.507 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.325 | clip 0\n",
            "INFO: Epoch 045: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.1\n",
            "INFO: Epoch 046: loss 2.485 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.245 | clip 0\n",
            "INFO: Epoch 046: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 047: loss 2.518 | lr 0.01 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.061 | clip 0.1\n",
            "INFO: Epoch 047: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.1\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E7 bs1000+lr0.001+layer2+dropout0.25"
      ],
      "metadata": {
        "id": "8UWgqo89cbsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_7/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.001 \\\n",
        "    --encoder-num-layers 2 \\\n",
        "    --decoder-num-layers 2"
      ],
      "metadata": {
        "id": "qvigur4TcpuB",
        "outputId": "8e40f56b-bb2c-45d0-e497-de98e61e6c6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_7/checkpoints --cuda --batch-size 1000 --lr 0.001 --encoder-num-layers 2 --decoder-num-layers 2\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.001, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_7/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1540000 parameters\n",
            "INFO: Epoch 000: loss 8.111 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.28 | clip 0.3\n",
            "INFO: Epoch 000: valid_loss 7.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 1.43e+03\n",
            "INFO: Epoch 001: loss 6.405 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.17 | clip 1\n",
            "INFO: Epoch 001: valid_loss 5.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 256\n",
            "INFO: Epoch 002: loss 5.516 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.467 | clip 1\n",
            "INFO: Epoch 002: valid_loss 5.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 198\n",
            "INFO: Epoch 003: loss 5.414 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.299 | clip 0.5\n",
            "INFO: Epoch 003: valid_loss 5.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 195\n",
            "INFO: Epoch 004: loss 5.39 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.654 | clip 0.6\n",
            "INFO: Epoch 004: valid_loss 5.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 190\n",
            "INFO: Epoch 005: loss 5.366 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.97 | clip 0.5\n",
            "INFO: Epoch 005: valid_loss 5.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 185\n",
            "INFO: Epoch 006: loss 5.338 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.062 | clip 0.6\n",
            "INFO: Epoch 006: valid_loss 5.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 179\n",
            "INFO: Epoch 007: loss 5.295 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.468 | clip 0.6\n",
            "INFO: Epoch 007: valid_loss 5.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 179\n",
            "INFO: Epoch 008: loss 5.303 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.529 | clip 0.9\n",
            "INFO: Epoch 008: valid_loss 5.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 172\n",
            "INFO: Epoch 009: loss 5.223 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.805 | clip 0.4\n",
            "INFO: Epoch 009: valid_loss 5.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 178\n",
            "INFO: Epoch 010: loss 5.314 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.65 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 5.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 167\n",
            "INFO: Epoch 011: loss 5.173 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.532 | clip 0.3\n",
            "INFO: Epoch 011: valid_loss 5.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 171\n",
            "INFO: Epoch 012: loss 5.294 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.85 | clip 1\n",
            "INFO: Epoch 012: valid_loss 5.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 160\n",
            "INFO: Epoch 013: loss 5.139 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.65 | clip 0.8\n",
            "INFO: Epoch 013: valid_loss 5.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 162\n",
            "INFO: Epoch 014: loss 5.259 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.83 | clip 1\n",
            "INFO: Epoch 014: valid_loss 5.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 153\n",
            "INFO: Epoch 015: loss 5.142 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.574 | clip 0.7\n",
            "INFO: Epoch 015: valid_loss 5.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 151\n",
            "INFO: Epoch 016: loss 5.104 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.672 | clip 0.8\n",
            "INFO: Epoch 016: valid_loss 4.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 147\n",
            "INFO: Epoch 017: loss 5.086 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.744 | clip 0.8\n",
            "INFO: Epoch 017: valid_loss 4.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 143\n",
            "INFO: Epoch 018: loss 5.057 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.203 | clip 0.9\n",
            "INFO: Epoch 018: valid_loss 4.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 138\n",
            "INFO: Epoch 019: loss 5.016 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.76 | clip 0.8\n",
            "INFO: Epoch 019: valid_loss 4.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 134\n",
            "INFO: Epoch 020: loss 4.979 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.975 | clip 0.8\n",
            "INFO: Epoch 020: valid_loss 4.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 128\n",
            "INFO: Epoch 021: loss 4.932 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.026 | clip 0.7\n",
            "INFO: Epoch 021: valid_loss 4.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 124\n",
            "INFO: Epoch 022: loss 4.893 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.952 | clip 0.5\n",
            "INFO: Epoch 022: valid_loss 4.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 120\n",
            "INFO: Epoch 023: loss 4.858 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.052 | clip 0.4\n",
            "INFO: Epoch 023: valid_loss 4.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 117\n",
            "INFO: Epoch 024: loss 4.823 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.099 | clip 0.3\n",
            "INFO: Epoch 024: valid_loss 4.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 114\n",
            "INFO: Epoch 025: loss 4.801 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.919 | clip 0.3\n",
            "INFO: Epoch 025: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112\n",
            "INFO: Epoch 026: loss 4.777 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.046 | clip 0.3\n",
            "INFO: Epoch 026: valid_loss 4.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 111\n",
            "INFO: Epoch 027: loss 4.765 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.759 | clip 0.7\n",
            "INFO: Epoch 027: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 110\n",
            "INFO: Epoch 028: loss 4.759 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.511 | clip 0.8\n",
            "INFO: Epoch 028: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106\n",
            "INFO: Epoch 029: loss 4.745 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.811 | clip 0.6\n",
            "INFO: Epoch 029: valid_loss 4.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 114\n",
            "INFO: Epoch 030: loss 4.723 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.416 | clip 0.4\n",
            "INFO: Epoch 030: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106\n",
            "INFO: Epoch 031: loss 4.71 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.282 | clip 0.8\n",
            "INFO: Epoch 031: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 110\n",
            "INFO: Epoch 032: loss 4.784 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.18 | clip 0.9\n",
            "INFO: Epoch 032: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106\n",
            "INFO: Epoch 033: loss 4.673 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.621 | clip 0.2\n",
            "INFO: Epoch 033: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 103\n",
            "INFO: Epoch 034: loss 4.689 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.926 | clip 0.8\n",
            "INFO: Epoch 034: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 035: loss 4.636 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.782 | clip 0.3\n",
            "INFO: Epoch 035: valid_loss 4.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 97.7\n",
            "INFO: Epoch 036: loss 4.619 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.41 | clip 0.5\n",
            "INFO: Epoch 036: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.3\n",
            "INFO: Epoch 037: loss 4.606 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.261 | clip 0.5\n",
            "INFO: Epoch 037: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 94.2\n",
            "INFO: Epoch 038: loss 4.583 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.171 | clip 0.5\n",
            "INFO: Epoch 038: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 92.6\n",
            "INFO: Epoch 039: loss 4.549 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.796 | clip 0.3\n",
            "INFO: Epoch 039: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.2\n",
            "INFO: Epoch 040: loss 4.526 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.79 | clip 0.3\n",
            "INFO: Epoch 040: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.6\n",
            "INFO: Epoch 041: loss 4.513 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.957 | clip 0.6\n",
            "INFO: Epoch 041: valid_loss 4.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 87\n",
            "INFO: Epoch 042: loss 4.505 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.293 | clip 0.8\n",
            "INFO: Epoch 042: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.5\n",
            "INFO: Epoch 043: loss 4.46 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.929 | clip 0.2\n",
            "INFO: Epoch 043: valid_loss 4.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.3\n",
            "INFO: Epoch 044: loss 4.438 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.323 | clip 0.1\n",
            "INFO: Epoch 044: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.1\n",
            "INFO: Epoch 045: loss 4.415 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.79 | clip 0.3\n",
            "INFO: Epoch 045: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.6\n",
            "INFO: Epoch 046: loss 4.456 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.831 | clip 0.8\n",
            "INFO: Epoch 046: valid_loss 4.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 78.8\n",
            "INFO: Epoch 047: loss 4.385 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.632 | clip 0.3\n",
            "INFO: Epoch 047: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.4\n",
            "INFO: Epoch 048: loss 4.353 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.735 | clip 0.2\n",
            "INFO: Epoch 048: valid_loss 4.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.4\n",
            "INFO: Epoch 049: loss 4.332 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.872 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 4.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72.8\n",
            "INFO: Epoch 050: loss 4.313 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.284 | clip 0.2\n",
            "INFO: Epoch 050: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.4\n",
            "INFO: Epoch 051: loss 4.302 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.682 | clip 0.4\n",
            "INFO: Epoch 051: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.8\n",
            "INFO: Epoch 052: loss 4.275 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.476 | clip 0.2\n",
            "INFO: Epoch 052: valid_loss 4.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.4\n",
            "INFO: Epoch 053: loss 4.255 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.125 | clip 0.1\n",
            "INFO: Epoch 053: valid_loss 4.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.7\n",
            "INFO: Epoch 054: loss 4.236 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.939 | clip 0.2\n",
            "INFO: Epoch 054: valid_loss 4.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.9\n",
            "INFO: Epoch 055: loss 4.212 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.126 | clip 0.2\n",
            "INFO: Epoch 055: valid_loss 4.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.8\n",
            "INFO: Epoch 056: loss 4.227 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.568 | clip 0.5\n",
            "INFO: Epoch 056: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.1\n",
            "INFO: Epoch 057: loss 4.173 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.703 | clip 0.1\n",
            "INFO: Epoch 057: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.6\n",
            "INFO: Epoch 058: loss 4.164 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.403 | clip 0.2\n",
            "INFO: Epoch 058: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.4\n",
            "INFO: Epoch 059: loss 4.145 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.586 | clip 0.7\n",
            "INFO: Epoch 059: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.7\n",
            "INFO: Epoch 060: loss 4.138 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.131 | clip 0.5\n",
            "INFO: Epoch 060: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.3\n",
            "INFO: Epoch 061: loss 4.1 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.749 | clip 0.1\n",
            "INFO: Epoch 061: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.6\n",
            "INFO: Epoch 062: loss 4.073 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.862 | clip 0.1\n",
            "INFO: Epoch 062: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.9\n",
            "INFO: Epoch 063: loss 4.062 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.265 | clip 0.1\n",
            "INFO: Epoch 063: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.7\n",
            "INFO: Epoch 064: loss 4.036 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.83 | clip 0.1\n",
            "INFO: Epoch 064: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.7\n",
            "INFO: Epoch 065: loss 4.021 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.389 | clip 0.3\n",
            "INFO: Epoch 065: valid_loss 4.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55\n",
            "INFO: Epoch 066: loss 4.016 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.516 | clip 0.6\n",
            "INFO: Epoch 066: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.9\n",
            "INFO: Epoch 067: loss 4.035 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.412 | clip 0.8\n",
            "INFO: Epoch 067: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.1\n",
            "INFO: Epoch 068: loss 3.971 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.825 | clip 0.1\n",
            "INFO: Epoch 068: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.4\n",
            "INFO: Epoch 069: loss 3.946 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.737 | clip 0.1\n",
            "INFO: Epoch 069: valid_loss 3.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.3\n",
            "INFO: Epoch 070: loss 3.938 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.639 | clip 0.4\n",
            "INFO: Epoch 070: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53\n",
            "INFO: Epoch 071: loss 4.004 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.258 | clip 0.9\n",
            "INFO: Epoch 071: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.1\n",
            "INFO: Epoch 072: loss 3.905 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.635 | clip 0.1\n",
            "INFO: Epoch 072: valid_loss 3.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49\n",
            "INFO: Epoch 073: loss 3.881 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.134 | clip 0.2\n",
            "INFO: Epoch 073: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.9\n",
            "INFO: Epoch 074: loss 3.935 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.992 | clip 0.9\n",
            "INFO: Epoch 074: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.5\n",
            "INFO: Epoch 075: loss 3.865 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.87 | clip 0.1\n",
            "INFO: Epoch 075: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.7\n",
            "INFO: Epoch 076: loss 3.833 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.188 | clip 0.1\n",
            "INFO: Epoch 076: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.3\n",
            "INFO: Epoch 077: loss 3.872 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.403 | clip 0.8\n",
            "INFO: Epoch 077: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.1\n",
            "INFO: Epoch 078: loss 3.808 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.888 | clip 0.1\n",
            "INFO: Epoch 078: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.8\n",
            "INFO: Epoch 079: loss 3.781 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.334 | clip 0.2\n",
            "INFO: Epoch 079: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.4\n",
            "INFO: Epoch 080: loss 3.835 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.814 | clip 0.9\n",
            "INFO: Epoch 080: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.9\n",
            "INFO: Epoch 081: loss 3.748 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.532 | clip 0.1\n",
            "INFO: Epoch 081: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.2\n",
            "INFO: Epoch 082: loss 3.724 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.035 | clip 0.1\n",
            "INFO: Epoch 082: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42\n",
            "INFO: Epoch 083: loss 3.749 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.81 | clip 0.8\n",
            "INFO: Epoch 083: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.1\n",
            "INFO: Epoch 084: loss 3.691 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.678 | clip 0\n",
            "INFO: Epoch 084: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.1\n",
            "INFO: Epoch 085: loss 3.67 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.429 | clip 0.4\n",
            "INFO: Epoch 085: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.7\n",
            "INFO: Epoch 086: loss 3.738 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.452 | clip 0.9\n",
            "INFO: Epoch 086: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.5\n",
            "INFO: Epoch 087: loss 3.651 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.894 | clip 0.1\n",
            "INFO: Epoch 087: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.4\n",
            "INFO: Epoch 088: loss 3.637 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.965 | clip 0.4\n",
            "INFO: Epoch 088: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.2\n",
            "INFO: Epoch 089: loss 3.607 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.25 | clip 0.2\n",
            "INFO: Epoch 089: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.7\n",
            "INFO: Epoch 090: loss 3.566 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.198 | clip 0\n",
            "INFO: Epoch 090: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.3\n",
            "INFO: Epoch 091: loss 3.552 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.961 | clip 0.1\n",
            "INFO: Epoch 091: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36\n",
            "INFO: Epoch 092: loss 3.578 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.181 | clip 0.8\n",
            "INFO: Epoch 092: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.3\n",
            "INFO: Epoch 093: loss 3.523 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.354 | clip 0.1\n",
            "INFO: Epoch 093: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 094: loss 3.499 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.688 | clip 0.1\n",
            "INFO: Epoch 094: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 095: loss 3.518 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.541 | clip 0.6\n",
            "INFO: Epoch 095: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8\n",
            "INFO: Epoch 096: loss 3.477 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.875 | clip 0.2\n",
            "INFO: Epoch 096: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.6\n",
            "INFO: Epoch 097: loss 3.474 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.765 | clip 0.8\n",
            "INFO: Epoch 097: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.7\n",
            "INFO: Epoch 098: loss 3.568 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.633 | clip 0.9\n",
            "INFO: Epoch 098: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.5\n",
            "INFO: Epoch 099: loss 3.458 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.555 | clip 0.2\n",
            "INFO: Epoch 099: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.8\n",
            "INFO: Epoch 100: loss 3.467 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.939 | clip 0.7\n",
            "INFO: Epoch 100: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.1\n",
            "INFO: Epoch 101: loss 3.398 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.314 | clip 0\n",
            "INFO: Epoch 101: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.1\n",
            "INFO: Epoch 102: loss 3.385 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.037 | clip 0.2\n",
            "INFO: Epoch 102: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.3\n",
            "INFO: Epoch 103: loss 3.367 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.564 | clip 0\n",
            "INFO: Epoch 103: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.4\n",
            "INFO: Epoch 104: loss 3.349 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.864 | clip 0.1\n",
            "INFO: Epoch 104: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30\n",
            "INFO: Epoch 105: loss 3.343 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.111 | clip 0.2\n",
            "INFO: Epoch 105: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.3\n",
            "INFO: Epoch 106: loss 3.308 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.172 | clip 0.1\n",
            "INFO: Epoch 106: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 107: loss 3.296 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.279 | clip 0\n",
            "INFO: Epoch 107: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 108: loss 3.302 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.282 | clip 0.2\n",
            "INFO: Epoch 108: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.3\n",
            "INFO: Epoch 109: loss 3.281 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.624 | clip 0.1\n",
            "INFO: Epoch 109: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.2\n",
            "INFO: Epoch 110: loss 3.265 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.895 | clip 0.1\n",
            "INFO: Epoch 110: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 111: loss 3.268 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.366 | clip 0.2\n",
            "INFO: Epoch 111: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.2\n",
            "INFO: Epoch 112: loss 3.235 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.896 | clip 0.1\n",
            "INFO: Epoch 112: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 113: loss 3.223 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.608 | clip 0\n",
            "INFO: Epoch 113: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.8\n",
            "INFO: Epoch 114: loss 3.199 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.621 | clip 0.1\n",
            "INFO: Epoch 114: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 115: loss 3.181 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.356 | clip 0.1\n",
            "INFO: Epoch 115: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 116: loss 3.193 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.214 | clip 0.3\n",
            "INFO: Epoch 116: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 117: loss 3.183 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.273 | clip 0.3\n",
            "INFO: Epoch 117: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.9\n",
            "INFO: Epoch 118: loss 3.175 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.664 | clip 0.3\n",
            "INFO: Epoch 118: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 119: loss 3.146 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.075 | clip 0.2\n",
            "INFO: Epoch 119: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 120: loss 3.146 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.821 | clip 0.3\n",
            "INFO: Epoch 120: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 121: loss 3.122 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.737 | clip 0.1\n",
            "INFO: Epoch 121: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 122: loss 3.105 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.809 | clip 0.1\n",
            "INFO: Epoch 122: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 123: loss 3.113 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.601 | clip 0.5\n",
            "INFO: Epoch 123: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 124: loss 3.074 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.835 | clip 0.1\n",
            "INFO: Epoch 124: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 125: loss 3.061 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.351 | clip 0.1\n",
            "INFO: Epoch 125: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 126: loss 3.05 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.623 | clip 0.1\n",
            "INFO: Epoch 126: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.6\n",
            "INFO: Epoch 127: loss 3.06 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.284 | clip 0.2\n",
            "INFO: Epoch 127: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 128: loss 3.032 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.064 | clip 0.1\n",
            "INFO: Epoch 128: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 129: loss 3.009 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.216 | clip 0.1\n",
            "INFO: Epoch 129: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 130: loss 2.993 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.777 | clip 0.1\n",
            "INFO: Epoch 130: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 131: loss 3.017 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.86 | clip 0.6\n",
            "INFO: Epoch 131: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 132: loss 2.98 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.008 | clip 0.1\n",
            "INFO: Epoch 132: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 133: loss 2.957 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.345 | clip 0.1\n",
            "INFO: Epoch 133: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.7\n",
            "INFO: Epoch 134: loss 2.964 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.208 | clip 0.4\n",
            "INFO: Epoch 134: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.7\n",
            "INFO: Epoch 135: loss 2.968 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.592 | clip 0.4\n",
            "INFO: Epoch 135: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 136: loss 2.956 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.633 | clip 0.3\n",
            "INFO: Epoch 136: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 137: loss 2.943 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.509 | clip 0.3\n",
            "INFO: Epoch 137: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 138: loss 2.909 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.731 | clip 0.1\n",
            "INFO: Epoch 138: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 139: loss 2.893 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.782 | clip 0.1\n",
            "INFO: Epoch 139: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 140: loss 2.893 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.48 | clip 0.4\n",
            "INFO: Epoch 140: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4\n",
            "INFO: Epoch 141: loss 2.898 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.61 | clip 0.4\n",
            "INFO: Epoch 141: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8\n",
            "INFO: Epoch 142: loss 2.872 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.984 | clip 0.1\n",
            "INFO: Epoch 142: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4\n",
            "INFO: Epoch 143: loss 2.847 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.883 | clip 0.2\n",
            "INFO: Epoch 143: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 144: loss 2.835 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.87 | clip 0.1\n",
            "INFO: Epoch 144: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 145: loss 2.817 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.797 | clip 0.1\n",
            "INFO: Epoch 145: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.3\n",
            "INFO: Epoch 146: loss 2.811 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.085 | clip 0.3\n",
            "INFO: Epoch 146: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 147: loss 2.849 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.451 | clip 0.7\n",
            "INFO: Epoch 147: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 148: loss 2.816 | lr 0.001 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.421 | clip 0.3\n",
            "INFO: Epoch 148: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E8 bs1000+lr0.0024+layer2"
      ],
      "metadata": {
        "id": "zSkoXoU3ch9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_8/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.0024 \\\n",
        "    --encoder-num-layers 2 \\\n",
        "    --decoder-num-layers 2"
      ],
      "metadata": {
        "id": "zpNLONPu7T6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646f13ac-806f-4419-8df4-b55b76952c80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_8/checkpoints --cuda --batch-size 2000 --lr 0.0024 --encoder-num-layers 2 --decoder-num-layers 2\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 2000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0024, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_8/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1540000 parameters\n",
            "INFO: Loaded checkpoint assignments/03/experiment_8/checkpoints/checkpoint_last.pt\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/atmt_2024/train.py\", line 225, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/atmt_2024/train.py\", line 124, in main\n",
            "    output, _ = model(sample['src_tokens'], sample['src_lengths'], sample['tgt_inputs'])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/atmt_2024/seq2seq/models/model.py\", line 24, in forward\n",
            "    decoder_out = self.decoder(tgt_inputs, encoder_out)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/atmt_2024/seq2seq/models/lstm.py\", line 289, in forward\n",
            "    input_feed, step_attn_weights = self.attention(tgt_hidden_states[-1], src_out, src_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/atmt_2024/seq2seq/models/lstm.py\", line 165, in forward\n",
            "    attn_scores = self.score(tgt_input, encoder_out)\n",
            "  File \"/content/drive/MyDrive/atmt_2024/seq2seq/models/lstm.py\", line 180, in score\n",
            "    projected_encoder_out = self.src_projection(encoder_out).transpose(2, 1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 14.75 GiB total capacity; 14.50 GiB already allocated; 65.06 MiB free; 14.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E9 bs1000+lr0.0024"
      ],
      "metadata": {
        "id": "WLxF9hUgCD3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_9/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.0024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr3j4xMDCIJz",
        "outputId": "8eaf5264-3afa-42e4-bdae-0c9e9083c151"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_9/checkpoints --cuda --batch-size 1000 --lr 0.0024\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0024, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_9/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 7.373 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.209 | clip 0.6\n",
            "INFO: Epoch 000: valid_loss 5.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 279\n",
            "INFO: Epoch 001: loss 5.5 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.765 | clip 0.9\n",
            "INFO: Epoch 001: valid_loss 5.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 194\n",
            "INFO: Epoch 002: loss 5.366 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.942 | clip 1\n",
            "INFO: Epoch 002: valid_loss 5.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 171\n",
            "INFO: Epoch 003: loss 5.221 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.515 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 152\n",
            "INFO: Epoch 004: loss 5.08 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.202 | clip 0.9\n",
            "INFO: Epoch 004: valid_loss 4.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 140\n",
            "INFO: Epoch 005: loss 5.02 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.613 | clip 1\n",
            "INFO: Epoch 005: valid_loss 4.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 121\n",
            "INFO: Epoch 006: loss 4.898 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.537 | clip 0.8\n",
            "INFO: Epoch 006: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116\n",
            "INFO: Epoch 007: loss 4.831 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.326 | clip 0.8\n",
            "INFO: Epoch 007: valid_loss 4.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 109\n",
            "INFO: Epoch 008: loss 4.792 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.537 | clip 0.8\n",
            "INFO: Epoch 008: valid_loss 4.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104\n",
            "INFO: Epoch 009: loss 4.703 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.679 | clip 0.5\n",
            "INFO: Epoch 009: valid_loss 4.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 010: loss 4.68 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.697 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98\n",
            "INFO: Epoch 011: loss 4.642 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.996 | clip 0.8\n",
            "INFO: Epoch 011: valid_loss 4.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.5\n",
            "INFO: Epoch 012: loss 4.591 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.586 | clip 0.8\n",
            "INFO: Epoch 012: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 87.9\n",
            "INFO: Epoch 013: loss 4.525 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.645 | clip 0.4\n",
            "INFO: Epoch 013: valid_loss 4.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.2\n",
            "INFO: Epoch 014: loss 4.475 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.396 | clip 0.4\n",
            "INFO: Epoch 014: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.3\n",
            "INFO: Epoch 015: loss 4.422 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.259 | clip 0.3\n",
            "INFO: Epoch 015: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.8\n",
            "INFO: Epoch 016: loss 4.37 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.54 | clip 0.5\n",
            "INFO: Epoch 016: valid_loss 4.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 78.8\n",
            "INFO: Epoch 017: loss 4.388 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.103 | clip 0.8\n",
            "INFO: Epoch 017: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70.1\n",
            "INFO: Epoch 018: loss 4.272 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.143 | clip 0.5\n",
            "INFO: Epoch 018: valid_loss 4.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.8\n",
            "INFO: Epoch 019: loss 4.211 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.474 | clip 0.5\n",
            "INFO: Epoch 019: valid_loss 4.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.7\n",
            "INFO: Epoch 020: loss 4.142 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.268 | clip 0.5\n",
            "INFO: Epoch 020: valid_loss 4.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60\n",
            "INFO: Epoch 021: loss 4.083 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.866 | clip 0.3\n",
            "INFO: Epoch 021: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.6\n",
            "INFO: Epoch 022: loss 4.018 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.663 | clip 0.4\n",
            "INFO: Epoch 022: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53\n",
            "INFO: Epoch 023: loss 3.962 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.954 | clip 0.5\n",
            "INFO: Epoch 023: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.1\n",
            "INFO: Epoch 024: loss 3.903 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.322 | clip 0.5\n",
            "INFO: Epoch 024: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.1\n",
            "INFO: Epoch 025: loss 3.829 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.655 | clip 0.2\n",
            "INFO: Epoch 025: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.5\n",
            "INFO: Epoch 026: loss 3.765 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.819 | clip 0.1\n",
            "INFO: Epoch 026: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.7\n",
            "INFO: Epoch 027: loss 3.699 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.367 | clip 0.1\n",
            "INFO: Epoch 027: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.1\n",
            "INFO: Epoch 028: loss 3.666 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.303 | clip 0.2\n",
            "INFO: Epoch 028: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.7\n",
            "INFO: Epoch 029: loss 3.645 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.123 | clip 0.3\n",
            "INFO: Epoch 029: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.6\n",
            "INFO: Epoch 030: loss 3.604 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.877 | clip 0.4\n",
            "INFO: Epoch 030: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.8\n",
            "INFO: Epoch 031: loss 3.553 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.642 | clip 0.4\n",
            "INFO: Epoch 031: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.3\n",
            "INFO: Epoch 032: loss 3.523 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.63 | clip 0.2\n",
            "INFO: Epoch 032: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.3\n",
            "INFO: Epoch 033: loss 3.494 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.483 | clip 0.7\n",
            "INFO: Epoch 033: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8\n",
            "INFO: Epoch 034: loss 3.436 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.389 | clip 0.2\n",
            "INFO: Epoch 034: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.8\n",
            "INFO: Epoch 035: loss 3.382 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.967 | clip 0.1\n",
            "INFO: Epoch 035: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.5\n",
            "INFO: Epoch 036: loss 3.342 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.43 | clip 0\n",
            "INFO: Epoch 036: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.9\n",
            "INFO: Epoch 037: loss 3.31 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.026 | clip 0.2\n",
            "INFO: Epoch 037: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.7\n",
            "INFO: Epoch 038: loss 3.347 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.444 | clip 0.7\n",
            "INFO: Epoch 038: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.1\n",
            "INFO: Epoch 039: loss 3.255 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.656 | clip 0.1\n",
            "INFO: Epoch 039: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 040: loss 3.207 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.081 | clip 0.1\n",
            "INFO: Epoch 040: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 041: loss 3.182 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.203 | clip 0\n",
            "INFO: Epoch 041: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28\n",
            "INFO: Epoch 042: loss 3.161 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.448 | clip 0\n",
            "INFO: Epoch 042: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.4\n",
            "INFO: Epoch 043: loss 3.148 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.126 | clip 0.1\n",
            "INFO: Epoch 043: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 044: loss 3.171 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.332 | clip 0.8\n",
            "INFO: Epoch 044: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.2\n",
            "INFO: Epoch 045: loss 3.109 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.112 | clip 0.1\n",
            "INFO: Epoch 045: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 046: loss 3.059 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.489 | clip 0.1\n",
            "INFO: Epoch 046: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.5\n",
            "INFO: Epoch 047: loss 3.041 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.092 | clip 0.2\n",
            "INFO: Epoch 047: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 048: loss 3.066 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.265 | clip 0.6\n",
            "INFO: Epoch 048: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 049: loss 3 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.19 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 050: loss 2.942 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.104 | clip 0\n",
            "INFO: Epoch 050: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 051: loss 2.904 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.088 | clip 0\n",
            "INFO: Epoch 051: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 052: loss 2.898 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.088 | clip 0.3\n",
            "INFO: Epoch 052: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 053: loss 2.921 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.054 | clip 0.8\n",
            "INFO: Epoch 053: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.5\n",
            "INFO: Epoch 054: loss 2.836 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.317 | clip 0.1\n",
            "INFO: Epoch 054: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9\n",
            "INFO: Epoch 055: loss 2.811 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.74 | clip 0.1\n",
            "INFO: Epoch 055: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 056: loss 2.822 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.441 | clip 0.5\n",
            "INFO: Epoch 056: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 057: loss 2.763 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.911 | clip 0.1\n",
            "INFO: Epoch 057: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 058: loss 2.709 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.086 | clip 0\n",
            "INFO: Epoch 058: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 059: loss 2.698 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.756 | clip 0\n",
            "INFO: Epoch 059: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 060: loss 2.717 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.48 | clip 0.6\n",
            "INFO: Epoch 060: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 061: loss 2.645 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.264 | clip 0\n",
            "INFO: Epoch 061: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 062: loss 2.614 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.223 | clip 0.1\n",
            "INFO: Epoch 062: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 063: loss 2.591 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.577 | clip 0.1\n",
            "INFO: Epoch 063: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 064: loss 2.59 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.99 | clip 0.1\n",
            "INFO: Epoch 064: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 065: loss 2.54 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.142 | clip 0.1\n",
            "INFO: Epoch 065: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 066: loss 2.516 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.452 | clip 0.1\n",
            "INFO: Epoch 066: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 067: loss 2.51 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.075 | clip 0.1\n",
            "INFO: Epoch 067: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 068: loss 2.473 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.317 | clip 0.1\n",
            "INFO: Epoch 068: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 069: loss 2.444 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.202 | clip 0.1\n",
            "INFO: Epoch 069: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 070: loss 2.416 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.316 | clip 0.1\n",
            "INFO: Epoch 070: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 071: loss 2.403 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.529 | clip 0.1\n",
            "INFO: Epoch 071: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 072: loss 2.357 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.332 | clip 0.1\n",
            "INFO: Epoch 072: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 073: loss 2.354 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.57 | clip 0.1\n",
            "INFO: Epoch 073: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 074: loss 2.321 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.506 | clip 0.1\n",
            "INFO: Epoch 074: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 075: loss 2.306 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.615 | clip 0.1\n",
            "INFO: Epoch 075: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 076: loss 2.29 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.886 | clip 0.1\n",
            "INFO: Epoch 076: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 077: loss 2.294 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.038 | clip 0.1\n",
            "INFO: Epoch 077: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 078: loss 2.251 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.533 | clip 0.1\n",
            "INFO: Epoch 078: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 079: loss 2.246 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.784 | clip 0.1\n",
            "INFO: Epoch 079: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 080: loss 2.226 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.914 | clip 0.1\n",
            "INFO: Epoch 080: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 081: loss 2.185 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.376 | clip 0.1\n",
            "INFO: Epoch 081: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 082: loss 2.165 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.669 | clip 0.1\n",
            "INFO: Epoch 082: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 083: loss 2.14 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.429 | clip 0\n",
            "INFO: Epoch 083: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 084: loss 2.125 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.586 | clip 0.1\n",
            "INFO: Epoch 084: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 085: loss 2.128 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.919 | clip 0.1\n",
            "INFO: Epoch 085: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 086: loss 2.097 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.922 | clip 0.2\n",
            "INFO: Epoch 086: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 087: loss 2.061 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.329 | clip 0.1\n",
            "INFO: Epoch 087: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 088: loss 2.049 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.762 | clip 0.1\n",
            "INFO: Epoch 088: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 089: loss 2.039 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.921 | clip 0.1\n",
            "INFO: Epoch 089: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 090: loss 2.016 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.697 | clip 0.1\n",
            "INFO: Epoch 090: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 091: loss 2.028 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.392 | clip 0.3\n",
            "INFO: Epoch 091: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 092: loss 2.007 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.681 | clip 0.2\n",
            "INFO: Epoch 092: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 093: loss 1.951 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.442 | clip 0.1\n",
            "INFO: Epoch 093: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 094: loss 1.946 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.804 | clip 0.1\n",
            "INFO: Epoch 094: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 095: loss 1.973 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.521 | clip 0.1\n",
            "INFO: Epoch 095: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 096: loss 1.934 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.159 | clip 0.2\n",
            "INFO: Epoch 096: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 097: loss 1.911 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.972 | clip 0.1\n",
            "INFO: Epoch 097: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 098: loss 1.883 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.794 | clip 0.1\n",
            "INFO: Epoch 098: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 099: loss 1.867 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.716 | clip 0.1\n",
            "INFO: Epoch 099: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 100: loss 1.874 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.709 | clip 0.4\n",
            "INFO: Epoch 100: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 101: loss 1.91 | lr 0.0024 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.227 | clip 0.5\n",
            "INFO: Epoch 101: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E10 bs500+lr0.001"
      ],
      "metadata": {
        "id": "MV9B-Y3aEvWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_10/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 500 \\\n",
        "    --lr 0.001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d12E9e3KEykX",
        "outputId": "f2ff0d7a-6bbf-4496-976b-3df2d2d69180"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_10/checkpoints --cuda --batch-size 500 --lr 0.001\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 500, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.001, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_10/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 7.426 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 8.089 | clip 0.65\n",
            "INFO: Epoch 000: valid_loss 5.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 310\n",
            "INFO: Epoch 001: loss 5.497 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 7.81 | clip 0.9\n",
            "INFO: Epoch 001: valid_loss 5.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 186\n",
            "INFO: Epoch 002: loss 5.317 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.941 | clip 0.75\n",
            "INFO: Epoch 002: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 168\n",
            "INFO: Epoch 003: loss 5.154 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 6.222 | clip 0.85\n",
            "INFO: Epoch 003: valid_loss 5.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 157\n",
            "INFO: Epoch 004: loss 5.063 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 7.355 | clip 1\n",
            "INFO: Epoch 004: valid_loss 4.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 132\n",
            "INFO: Epoch 005: loss 4.929 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 6.189 | clip 0.8\n",
            "INFO: Epoch 005: valid_loss 4.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 124\n",
            "INFO: Epoch 006: loss 4.877 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 7.476 | clip 0.85\n",
            "INFO: Epoch 006: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112\n",
            "INFO: Epoch 007: loss 4.829 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 7.878 | clip 0.85\n",
            "INFO: Epoch 007: valid_loss 4.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104\n",
            "INFO: Epoch 008: loss 4.701 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.158 | clip 0.65\n",
            "INFO: Epoch 008: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 102\n",
            "INFO: Epoch 009: loss 4.675 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.947 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95\n",
            "INFO: Epoch 010: loss 4.596 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.535 | clip 0.7\n",
            "INFO: Epoch 010: valid_loss 4.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.5\n",
            "INFO: Epoch 011: loss 4.542 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.616 | clip 0.65\n",
            "INFO: Epoch 011: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.7\n",
            "INFO: Epoch 012: loss 4.472 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.392 | clip 0.65\n",
            "INFO: Epoch 012: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.6\n",
            "INFO: Epoch 013: loss 4.423 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.946 | clip 0.75\n",
            "INFO: Epoch 013: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.1\n",
            "INFO: Epoch 014: loss 4.396 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 6.774 | clip 0.75\n",
            "INFO: Epoch 014: valid_loss 4.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.2\n",
            "INFO: Epoch 015: loss 4.328 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 6.349 | clip 0.75\n",
            "INFO: Epoch 015: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.6\n",
            "INFO: Epoch 016: loss 4.253 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.711 | clip 0.7\n",
            "INFO: Epoch 016: valid_loss 4.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.6\n",
            "INFO: Epoch 017: loss 4.192 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 5.109 | clip 0.7\n",
            "INFO: Epoch 017: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.6\n",
            "INFO: Epoch 018: loss 4.14 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.936 | clip 0.6\n",
            "INFO: Epoch 018: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.7\n",
            "INFO: Epoch 019: loss 4.078 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.478 | clip 0.4\n",
            "INFO: Epoch 019: valid_loss 4.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.2\n",
            "INFO: Epoch 020: loss 4.026 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.648 | clip 0.45\n",
            "INFO: Epoch 020: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.7\n",
            "INFO: Epoch 021: loss 3.98 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.316 | clip 0.45\n",
            "INFO: Epoch 021: valid_loss 3.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.3\n",
            "INFO: Epoch 022: loss 3.932 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.219 | clip 0.45\n",
            "INFO: Epoch 022: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.6\n",
            "INFO: Epoch 023: loss 3.891 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.251 | clip 0.5\n",
            "INFO: Epoch 023: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.5\n",
            "INFO: Epoch 024: loss 3.851 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.432 | clip 0.45\n",
            "INFO: Epoch 024: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.3\n",
            "INFO: Epoch 025: loss 3.817 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.767 | clip 0.7\n",
            "INFO: Epoch 025: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.2\n",
            "INFO: Epoch 026: loss 3.772 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.447 | clip 0.7\n",
            "INFO: Epoch 026: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.3\n",
            "INFO: Epoch 027: loss 3.714 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.809 | clip 0.3\n",
            "INFO: Epoch 027: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.8\n",
            "INFO: Epoch 028: loss 3.668 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.425 | clip 0.2\n",
            "INFO: Epoch 028: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.4\n",
            "INFO: Epoch 029: loss 3.624 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.328 | clip 0.2\n",
            "INFO: Epoch 029: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.2\n",
            "INFO: Epoch 030: loss 3.583 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.215 | clip 0.25\n",
            "INFO: Epoch 030: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.8\n",
            "INFO: Epoch 031: loss 3.536 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.001 | clip 0.25\n",
            "INFO: Epoch 031: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.9\n",
            "INFO: Epoch 032: loss 3.505 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.112 | clip 0.25\n",
            "INFO: Epoch 032: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.9\n",
            "INFO: Epoch 033: loss 3.462 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.994 | clip 0.25\n",
            "INFO: Epoch 033: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 034: loss 3.434 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.268 | clip 0.3\n",
            "INFO: Epoch 034: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9\n",
            "INFO: Epoch 035: loss 3.392 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.036 | clip 0.25\n",
            "INFO: Epoch 035: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.2\n",
            "INFO: Epoch 036: loss 3.359 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.1 | clip 0.3\n",
            "INFO: Epoch 036: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.2\n",
            "INFO: Epoch 037: loss 3.326 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.158 | clip 0.3\n",
            "INFO: Epoch 037: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.6\n",
            "INFO: Epoch 038: loss 3.295 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.134 | clip 0.3\n",
            "INFO: Epoch 038: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.6\n",
            "INFO: Epoch 039: loss 3.261 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.103 | clip 0.25\n",
            "INFO: Epoch 039: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.1\n",
            "INFO: Epoch 040: loss 3.231 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.156 | clip 0.3\n",
            "INFO: Epoch 040: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.3\n",
            "INFO: Epoch 041: loss 3.191 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.048 | clip 0.15\n",
            "INFO: Epoch 041: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 042: loss 3.167 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.192 | clip 0.25\n",
            "INFO: Epoch 042: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 043: loss 3.138 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.027 | clip 0.2\n",
            "INFO: Epoch 043: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.8\n",
            "INFO: Epoch 044: loss 3.12 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.466 | clip 0.25\n",
            "INFO: Epoch 044: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 045: loss 3.072 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.827 | clip 0.1\n",
            "INFO: Epoch 045: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 046: loss 3.075 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.779 | clip 0.3\n",
            "INFO: Epoch 046: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.6\n",
            "INFO: Epoch 047: loss 3.016 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.78 | clip 0.15\n",
            "INFO: Epoch 047: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 048: loss 3.023 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.769 | clip 0.35\n",
            "INFO: Epoch 048: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 049: loss 2.968 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.045 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 050: loss 2.982 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.125 | clip 0.45\n",
            "INFO: Epoch 050: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 051: loss 2.921 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.116 | clip 0.2\n",
            "INFO: Epoch 051: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.3\n",
            "INFO: Epoch 052: loss 2.955 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.615 | clip 0.6\n",
            "INFO: Epoch 052: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 053: loss 2.886 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.434 | clip 0.3\n",
            "INFO: Epoch 053: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 054: loss 2.908 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.572 | clip 0.65\n",
            "INFO: Epoch 054: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 055: loss 2.844 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.764 | clip 0.3\n",
            "INFO: Epoch 055: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 056: loss 2.827 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.433 | clip 0.3\n",
            "INFO: Epoch 056: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 057: loss 2.784 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.281 | clip 0.25\n",
            "INFO: Epoch 057: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1\n",
            "INFO: Epoch 058: loss 2.771 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.111 | clip 0.05\n",
            "INFO: Epoch 058: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 059: loss 2.733 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.32 | clip 0.25\n",
            "INFO: Epoch 059: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 060: loss 2.723 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.907 | clip 0.05\n",
            "INFO: Epoch 060: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 061: loss 2.685 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.114 | clip 0.25\n",
            "INFO: Epoch 061: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 062: loss 2.67 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.866 | clip 0.05\n",
            "INFO: Epoch 062: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 063: loss 2.644 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.162 | clip 0.2\n",
            "INFO: Epoch 063: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 064: loss 2.637 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.009 | clip 0.05\n",
            "INFO: Epoch 064: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 065: loss 2.594 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.955 | clip 0.2\n",
            "INFO: Epoch 065: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 066: loss 2.583 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.871 | clip 0.1\n",
            "INFO: Epoch 066: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 067: loss 2.549 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.817 | clip 0.1\n",
            "INFO: Epoch 067: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 068: loss 2.532 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.96 | clip 0.1\n",
            "INFO: Epoch 068: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 069: loss 2.502 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.768 | clip 0.05\n",
            "INFO: Epoch 069: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 070: loss 2.488 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.061 | clip 0.1\n",
            "INFO: Epoch 070: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 071: loss 2.471 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.794 | clip 0.05\n",
            "INFO: Epoch 071: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 072: loss 2.449 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.076 | clip 0.1\n",
            "INFO: Epoch 072: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 073: loss 2.442 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.165 | clip 0.15\n",
            "INFO: Epoch 073: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 074: loss 2.407 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.937 | clip 0.05\n",
            "INFO: Epoch 074: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 075: loss 2.402 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.272 | clip 0.15\n",
            "INFO: Epoch 075: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 076: loss 2.369 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.925 | clip 0.05\n",
            "INFO: Epoch 076: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 077: loss 2.362 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.278 | clip 0.15\n",
            "INFO: Epoch 077: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 078: loss 2.335 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 2.93 | clip 0.05\n",
            "INFO: Epoch 078: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 079: loss 2.329 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.336 | clip 0.15\n",
            "INFO: Epoch 079: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 080: loss 2.302 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.061 | clip 0.05\n",
            "INFO: Epoch 080: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 081: loss 2.29 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.23 | clip 0.15\n",
            "INFO: Epoch 081: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 082: loss 2.263 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.057 | clip 0.05\n",
            "INFO: Epoch 082: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 083: loss 2.262 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.398 | clip 0.15\n",
            "INFO: Epoch 083: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 084: loss 2.23 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.188 | clip 0.15\n",
            "INFO: Epoch 084: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 085: loss 2.235 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.592 | clip 0.25\n",
            "INFO: Epoch 085: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 086: loss 2.2 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.338 | clip 0.1\n",
            "INFO: Epoch 086: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 087: loss 2.209 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.692 | clip 0.2\n",
            "INFO: Epoch 087: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 088: loss 2.17 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.305 | clip 0.15\n",
            "INFO: Epoch 088: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 089: loss 2.186 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.059 | clip 0.45\n",
            "INFO: Epoch 089: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 090: loss 2.143 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.363 | clip 0.15\n",
            "INFO: Epoch 090: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 091: loss 2.153 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.77 | clip 0.25\n",
            "INFO: Epoch 091: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 092: loss 2.115 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.445 | clip 0.15\n",
            "INFO: Epoch 092: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 093: loss 2.123 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.948 | clip 0.35\n",
            "INFO: Epoch 093: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 094: loss 2.087 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.357 | clip 0.15\n",
            "INFO: Epoch 094: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 095: loss 2.089 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.789 | clip 0.3\n",
            "INFO: Epoch 095: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 096: loss 2.053 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.273 | clip 0.25\n",
            "INFO: Epoch 096: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 097: loss 2.054 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.541 | clip 0.25\n",
            "INFO: Epoch 097: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 098: loss 2.029 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.352 | clip 0.25\n",
            "INFO: Epoch 098: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 099: loss 2.028 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.709 | clip 0.35\n",
            "INFO: Epoch 099: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 100: loss 1.997 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.517 | clip 0.15\n",
            "INFO: Epoch 100: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 101: loss 2.012 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.829 | clip 0.35\n",
            "INFO: Epoch 101: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 102: loss 1.983 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.562 | clip 0.2\n",
            "INFO: Epoch 102: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 103: loss 1.989 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.904 | clip 0.45\n",
            "INFO: Epoch 103: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 104: loss 1.956 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.609 | clip 0.2\n",
            "INFO: Epoch 104: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 105: loss 1.969 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.034 | clip 0.4\n",
            "INFO: Epoch 105: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 106: loss 1.936 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.541 | clip 0.2\n",
            "INFO: Epoch 106: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 107: loss 1.948 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.048 | clip 0.4\n",
            "INFO: Epoch 107: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 108: loss 1.909 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.715 | clip 0.3\n",
            "INFO: Epoch 108: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 109: loss 1.923 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 4.025 | clip 0.4\n",
            "INFO: Epoch 109: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 110: loss 1.889 | lr 0.001 | num_tokens 9.1 | batch_size 500 | grad_norm 3.758 | clip 0.3\n",
            "INFO: Epoch 110: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E11 bs1000+lr0.0005"
      ],
      "metadata": {
        "id": "lPvtVN-mIS-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_11/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.0005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDPmJRL7IUYp",
        "outputId": "130611d0-4151-48ca-d3e1-6b57a8a0e85c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_11/checkpoints --cuda --batch-size 1000 --lr 0.0005\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_11/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.236 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.134 | clip 0\n",
            "INFO: Epoch 000: valid_loss 8.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.39e+03\n",
            "INFO: Epoch 001: loss 7.883 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.654 | clip 0.7\n",
            "INFO: Epoch 001: valid_loss 7.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 1.39e+03\n",
            "INFO: Epoch 002: loss 6.741 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.6 | clip 1\n",
            "INFO: Epoch 002: valid_loss 6.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 446\n",
            "INFO: Epoch 003: loss 5.892 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.64 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 258\n",
            "INFO: Epoch 004: loss 5.523 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.06 | clip 1\n",
            "INFO: Epoch 004: valid_loss 5.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 207\n",
            "INFO: Epoch 005: loss 5.393 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.096 | clip 0.6\n",
            "INFO: Epoch 005: valid_loss 5.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 190\n",
            "INFO: Epoch 006: loss 5.332 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.994 | clip 0.5\n",
            "INFO: Epoch 006: valid_loss 5.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 179\n",
            "INFO: Epoch 007: loss 5.29 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.937 | clip 0.7\n",
            "INFO: Epoch 007: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169\n",
            "INFO: Epoch 008: loss 5.229 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.822 | clip 0.8\n",
            "INFO: Epoch 008: valid_loss 5.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 161\n",
            "INFO: Epoch 009: loss 5.173 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.711 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 5.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 152\n",
            "INFO: Epoch 010: loss 5.112 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.494 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 144\n",
            "INFO: Epoch 011: loss 5.061 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.448 | clip 0.7\n",
            "INFO: Epoch 011: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 012: loss 5.015 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.585 | clip 0.7\n",
            "INFO: Epoch 012: valid_loss 4.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 132\n",
            "INFO: Epoch 013: loss 4.97 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.995 | clip 0.6\n",
            "INFO: Epoch 013: valid_loss 4.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 127\n",
            "INFO: Epoch 014: loss 4.936 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.104 | clip 0.6\n",
            "INFO: Epoch 014: valid_loss 4.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 123\n",
            "INFO: Epoch 015: loss 4.899 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.586 | clip 0.6\n",
            "INFO: Epoch 015: valid_loss 4.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 119\n",
            "INFO: Epoch 016: loss 4.868 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.88 | clip 0.6\n",
            "INFO: Epoch 016: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116\n",
            "INFO: Epoch 017: loss 4.832 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.047 | clip 0.5\n",
            "INFO: Epoch 017: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 113\n",
            "INFO: Epoch 018: loss 4.804 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.274 | clip 0.6\n",
            "INFO: Epoch 018: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 109\n",
            "INFO: Epoch 019: loss 4.774 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.677 | clip 0.4\n",
            "INFO: Epoch 019: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 107\n",
            "INFO: Epoch 020: loss 4.744 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.378 | clip 0.3\n",
            "INFO: Epoch 020: valid_loss 4.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104\n",
            "INFO: Epoch 021: loss 4.716 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.009 | clip 0.3\n",
            "INFO: Epoch 021: valid_loss 4.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 102\n",
            "INFO: Epoch 022: loss 4.689 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.81 | clip 0.3\n",
            "INFO: Epoch 022: valid_loss 4.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 99.5\n",
            "INFO: Epoch 023: loss 4.67 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.17 | clip 0.3\n",
            "INFO: Epoch 023: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.6\n",
            "INFO: Epoch 024: loss 4.658 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.503 | clip 0.6\n",
            "INFO: Epoch 024: valid_loss 4.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95.6\n",
            "INFO: Epoch 025: loss 4.628 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.476 | clip 0.4\n",
            "INFO: Epoch 025: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 94\n",
            "INFO: Epoch 026: loss 4.6 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.503 | clip 0.2\n",
            "INFO: Epoch 026: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 92.8\n",
            "INFO: Epoch 027: loss 4.585 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.343 | clip 0.5\n",
            "INFO: Epoch 027: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.2\n",
            "INFO: Epoch 028: loss 4.598 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.127 | clip 0.8\n",
            "INFO: Epoch 028: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.9\n",
            "INFO: Epoch 029: loss 4.559 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.159 | clip 0.6\n",
            "INFO: Epoch 029: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.2\n",
            "INFO: Epoch 030: loss 4.529 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.716 | clip 0.3\n",
            "INFO: Epoch 030: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.6\n",
            "INFO: Epoch 031: loss 4.509 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.474 | clip 0.3\n",
            "INFO: Epoch 031: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85\n",
            "INFO: Epoch 032: loss 4.487 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.607 | clip 0.3\n",
            "INFO: Epoch 032: valid_loss 4.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 83.6\n",
            "INFO: Epoch 033: loss 4.47 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.214 | clip 0.4\n",
            "INFO: Epoch 033: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82\n",
            "INFO: Epoch 034: loss 4.454 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.833 | clip 0.6\n",
            "INFO: Epoch 034: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.9\n",
            "INFO: Epoch 035: loss 4.424 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.7 | clip 0.3\n",
            "INFO: Epoch 035: valid_loss 4.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 78.5\n",
            "INFO: Epoch 036: loss 4.399 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.256 | clip 0.2\n",
            "INFO: Epoch 036: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.7\n",
            "INFO: Epoch 037: loss 4.376 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.624 | clip 0.2\n",
            "INFO: Epoch 037: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.5\n",
            "INFO: Epoch 038: loss 4.385 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.972 | clip 0.8\n",
            "INFO: Epoch 038: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.6\n",
            "INFO: Epoch 039: loss 4.34 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.056 | clip 0.4\n",
            "INFO: Epoch 039: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.9\n",
            "INFO: Epoch 040: loss 4.313 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.671 | clip 0.3\n",
            "INFO: Epoch 040: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70.2\n",
            "INFO: Epoch 041: loss 4.291 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.249 | clip 0.2\n",
            "INFO: Epoch 041: valid_loss 4.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.6\n",
            "INFO: Epoch 042: loss 4.265 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.086 | clip 0.2\n",
            "INFO: Epoch 042: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.1\n",
            "INFO: Epoch 043: loss 4.245 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.737 | clip 0.3\n",
            "INFO: Epoch 043: valid_loss 4.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.9\n",
            "INFO: Epoch 044: loss 4.231 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.325 | clip 0.7\n",
            "INFO: Epoch 044: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.9\n",
            "INFO: Epoch 045: loss 4.197 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.759 | clip 0.2\n",
            "INFO: Epoch 045: valid_loss 4.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.1\n",
            "INFO: Epoch 046: loss 4.174 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.34 | clip 0.1\n",
            "INFO: Epoch 046: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.8\n",
            "INFO: Epoch 047: loss 4.15 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.478 | clip 0.2\n",
            "INFO: Epoch 047: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.3\n",
            "INFO: Epoch 048: loss 4.151 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.944 | clip 0.8\n",
            "INFO: Epoch 048: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.2\n",
            "INFO: Epoch 049: loss 4.135 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.626 | clip 0.8\n",
            "INFO: Epoch 049: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.4\n",
            "INFO: Epoch 050: loss 4.092 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.806 | clip 0.1\n",
            "INFO: Epoch 050: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.1\n",
            "INFO: Epoch 051: loss 4.071 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.184 | clip 0.2\n",
            "INFO: Epoch 051: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.8\n",
            "INFO: Epoch 052: loss 4.071 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.382 | clip 0.7\n",
            "INFO: Epoch 052: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.9\n",
            "INFO: Epoch 053: loss 4.047 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.462 | clip 0.5\n",
            "INFO: Epoch 053: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54\n",
            "INFO: Epoch 054: loss 4.015 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.781 | clip 0.1\n",
            "INFO: Epoch 054: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.3\n",
            "INFO: Epoch 055: loss 3.995 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.257 | clip 0.3\n",
            "INFO: Epoch 055: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.4\n",
            "INFO: Epoch 056: loss 3.985 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.139 | clip 0.6\n",
            "INFO: Epoch 056: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.4\n",
            "INFO: Epoch 057: loss 4 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.733 | clip 0.9\n",
            "INFO: Epoch 057: valid_loss 3.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.4\n",
            "INFO: Epoch 058: loss 3.947 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.022 | clip 0.2\n",
            "INFO: Epoch 058: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.7\n",
            "INFO: Epoch 059: loss 3.936 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.752 | clip 0.5\n",
            "INFO: Epoch 059: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51\n",
            "INFO: Epoch 060: loss 3.965 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.36 | clip 0.8\n",
            "INFO: Epoch 060: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.9\n",
            "INFO: Epoch 061: loss 3.9 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.347 | clip 0.3\n",
            "INFO: Epoch 061: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.3\n",
            "INFO: Epoch 062: loss 3.903 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.365 | clip 0.6\n",
            "INFO: Epoch 062: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.3\n",
            "INFO: Epoch 063: loss 3.873 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.641 | clip 0.3\n",
            "INFO: Epoch 063: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.5\n",
            "INFO: Epoch 064: loss 3.849 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.261 | clip 0.2\n",
            "INFO: Epoch 064: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.6\n",
            "INFO: Epoch 065: loss 3.832 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.391 | clip 0.2\n",
            "INFO: Epoch 065: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.2\n",
            "INFO: Epoch 066: loss 3.813 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.712 | clip 0.2\n",
            "INFO: Epoch 066: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.5\n",
            "INFO: Epoch 067: loss 3.805 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.574 | clip 0.4\n",
            "INFO: Epoch 067: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.7\n",
            "INFO: Epoch 068: loss 3.783 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.673 | clip 0.1\n",
            "INFO: Epoch 068: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.3\n",
            "INFO: Epoch 069: loss 3.767 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.755 | clip 0.1\n",
            "INFO: Epoch 069: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.7\n",
            "INFO: Epoch 070: loss 3.751 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.29 | clip 0.2\n",
            "INFO: Epoch 070: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.8\n",
            "INFO: Epoch 071: loss 3.765 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.066 | clip 0.7\n",
            "INFO: Epoch 071: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.7\n",
            "INFO: Epoch 072: loss 3.73 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.151 | clip 0.2\n",
            "INFO: Epoch 072: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.4\n",
            "INFO: Epoch 073: loss 3.706 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.229 | clip 0\n",
            "INFO: Epoch 073: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.6\n",
            "INFO: Epoch 074: loss 3.694 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.986 | clip 0.2\n",
            "INFO: Epoch 074: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.6\n",
            "INFO: Epoch 075: loss 3.697 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.021 | clip 0.7\n",
            "INFO: Epoch 075: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.7\n",
            "INFO: Epoch 076: loss 3.677 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.373 | clip 0.3\n",
            "INFO: Epoch 076: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.9\n",
            "INFO: Epoch 077: loss 3.654 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.517 | clip 0\n",
            "INFO: Epoch 077: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39\n",
            "INFO: Epoch 078: loss 3.644 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.433 | clip 0.4\n",
            "INFO: Epoch 078: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.5\n",
            "INFO: Epoch 079: loss 3.666 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.601 | clip 0.8\n",
            "INFO: Epoch 079: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.7\n",
            "INFO: Epoch 080: loss 3.621 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.297 | clip 0.1\n",
            "INFO: Epoch 080: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38\n",
            "INFO: Epoch 081: loss 3.601 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.292 | clip 0\n",
            "INFO: Epoch 081: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.6\n",
            "INFO: Epoch 082: loss 3.599 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.815 | clip 0.5\n",
            "INFO: Epoch 082: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.2\n",
            "INFO: Epoch 083: loss 3.584 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.516 | clip 0.4\n",
            "INFO: Epoch 083: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.9\n",
            "INFO: Epoch 084: loss 3.563 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.619 | clip 0.1\n",
            "INFO: Epoch 084: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.8\n",
            "INFO: Epoch 085: loss 3.557 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.484 | clip 0.4\n",
            "INFO: Epoch 085: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.5\n",
            "INFO: Epoch 086: loss 3.56 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.598 | clip 0.6\n",
            "INFO: Epoch 086: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36\n",
            "INFO: Epoch 087: loss 3.547 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.472 | clip 0.7\n",
            "INFO: Epoch 087: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.8\n",
            "INFO: Epoch 088: loss 3.518 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.322 | clip 0\n",
            "INFO: Epoch 088: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.2\n",
            "INFO: Epoch 089: loss 3.51 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.415 | clip 0.3\n",
            "INFO: Epoch 089: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.8\n",
            "INFO: Epoch 090: loss 3.539 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.668 | clip 0.9\n",
            "INFO: Epoch 090: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.5\n",
            "INFO: Epoch 091: loss 3.485 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.573 | clip 0\n",
            "INFO: Epoch 091: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 092: loss 3.479 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.868 | clip 0.6\n",
            "INFO: Epoch 092: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.6\n",
            "INFO: Epoch 093: loss 3.514 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.73 | clip 0.9\n",
            "INFO: Epoch 093: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.3\n",
            "INFO: Epoch 094: loss 3.466 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.042 | clip 0.6\n",
            "INFO: Epoch 094: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34\n",
            "INFO: Epoch 095: loss 3.475 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.585 | clip 0.5\n",
            "INFO: Epoch 095: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.7\n",
            "INFO: Epoch 096: loss 3.434 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.175 | clip 0.1\n",
            "INFO: Epoch 096: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9\n",
            "INFO: Epoch 097: loss 3.42 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.369 | clip 0\n",
            "INFO: Epoch 097: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.5\n",
            "INFO: Epoch 098: loss 3.406 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.526 | clip 0.1\n",
            "INFO: Epoch 098: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.2\n",
            "INFO: Epoch 099: loss 3.391 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 1.908 | clip 0\n",
            "INFO: Epoch 099: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.8\n",
            "INFO: Epoch 100: loss 3.379 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.252 | clip 0\n",
            "INFO: Epoch 100: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.6\n",
            "INFO: Epoch 101: loss 3.376 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.254 | clip 0.3\n",
            "INFO: Epoch 101: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.2\n",
            "INFO: Epoch 102: loss 3.364 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.168 | clip 0.1\n",
            "INFO: Epoch 102: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.1\n",
            "INFO: Epoch 103: loss 3.345 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.363 | clip 0\n",
            "INFO: Epoch 103: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.7\n",
            "INFO: Epoch 104: loss 3.337 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.307 | clip 0.3\n",
            "INFO: Epoch 104: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 105: loss 3.367 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.798 | clip 0.8\n",
            "INFO: Epoch 105: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.7\n",
            "INFO: Epoch 106: loss 3.316 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.49 | clip 0.1\n",
            "INFO: Epoch 106: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.9\n",
            "INFO: Epoch 107: loss 3.315 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.845 | clip 0.4\n",
            "INFO: Epoch 107: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.5\n",
            "INFO: Epoch 108: loss 3.359 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.01 | clip 0.9\n",
            "INFO: Epoch 108: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.2\n",
            "INFO: Epoch 109: loss 3.301 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.689 | clip 0.4\n",
            "INFO: Epoch 109: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 110: loss 3.317 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.982 | clip 0.6\n",
            "INFO: Epoch 110: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.5\n",
            "INFO: Epoch 111: loss 3.274 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.445 | clip 0.1\n",
            "INFO: Epoch 111: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 112: loss 3.26 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.198 | clip 0\n",
            "INFO: Epoch 112: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.5\n",
            "INFO: Epoch 113: loss 3.248 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.355 | clip 0.1\n",
            "INFO: Epoch 113: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.3\n",
            "INFO: Epoch 114: loss 3.235 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.224 | clip 0\n",
            "INFO: Epoch 114: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28\n",
            "INFO: Epoch 115: loss 3.228 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.607 | clip 0.1\n",
            "INFO: Epoch 115: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 116: loss 3.214 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.177 | clip 0\n",
            "INFO: Epoch 116: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.5\n",
            "INFO: Epoch 117: loss 3.211 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.61 | clip 0.1\n",
            "INFO: Epoch 117: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 118: loss 3.201 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.787 | clip 0\n",
            "INFO: Epoch 118: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 119: loss 3.199 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.292 | clip 0.2\n",
            "INFO: Epoch 119: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27\n",
            "INFO: Epoch 120: loss 3.178 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.274 | clip 0\n",
            "INFO: Epoch 120: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 121: loss 3.172 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.004 | clip 0.1\n",
            "INFO: Epoch 121: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.4\n",
            "INFO: Epoch 122: loss 3.162 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.259 | clip 0.2\n",
            "INFO: Epoch 122: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 123: loss 3.154 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.1 | clip 0.2\n",
            "INFO: Epoch 123: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 124: loss 3.138 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.8 | clip 0.2\n",
            "INFO: Epoch 124: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 125: loss 3.134 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.69 | clip 0.5\n",
            "INFO: Epoch 125: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26\n",
            "INFO: Epoch 126: loss 3.148 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.8 | clip 0.9\n",
            "INFO: Epoch 126: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 127: loss 3.116 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.217 | clip 0.3\n",
            "INFO: Epoch 127: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 128: loss 3.11 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.116 | clip 0.5\n",
            "INFO: Epoch 128: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 129: loss 3.171 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.856 | clip 0.9\n",
            "INFO: Epoch 129: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.8\n",
            "INFO: Epoch 130: loss 3.114 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.63 | clip 0.8\n",
            "INFO: Epoch 130: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 131: loss 3.113 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.41 | clip 0.8\n",
            "INFO: Epoch 131: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 132: loss 3.071 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.887 | clip 0.1\n",
            "INFO: Epoch 132: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 133: loss 3.075 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.452 | clip 0.4\n",
            "INFO: Epoch 133: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 134: loss 3.047 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.282 | clip 0.1\n",
            "INFO: Epoch 134: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 135: loss 3.039 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 1.978 | clip 0\n",
            "INFO: Epoch 135: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 136: loss 3.029 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.505 | clip 0.1\n",
            "INFO: Epoch 136: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 137: loss 3.024 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.317 | clip 0.2\n",
            "INFO: Epoch 137: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 138: loss 3.011 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.421 | clip 0.1\n",
            "INFO: Epoch 138: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 139: loss 3.004 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.332 | clip 0.3\n",
            "INFO: Epoch 139: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 140: loss 3.044 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.77 | clip 0.8\n",
            "INFO: Epoch 140: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 141: loss 2.988 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.223 | clip 0.1\n",
            "INFO: Epoch 141: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 142: loss 2.988 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.262 | clip 0.4\n",
            "INFO: Epoch 142: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.8\n",
            "INFO: Epoch 143: loss 2.98 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.518 | clip 0.2\n",
            "INFO: Epoch 143: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 144: loss 2.956 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.509 | clip 0.1\n",
            "INFO: Epoch 144: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 145: loss 2.954 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.63 | clip 0.4\n",
            "INFO: Epoch 145: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.8\n",
            "INFO: Epoch 146: loss 2.993 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.699 | clip 0.9\n",
            "INFO: Epoch 146: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 147: loss 2.947 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.945 | clip 0.6\n",
            "INFO: Epoch 147: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 148: loss 2.956 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.681 | clip 0.7\n",
            "INFO: Epoch 148: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 149: loss 2.915 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.06 | clip 0\n",
            "INFO: Epoch 149: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5\n",
            "INFO: Epoch 150: loss 2.905 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.675 | clip 0.1\n",
            "INFO: Epoch 150: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5\n",
            "INFO: Epoch 151: loss 2.901 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.925 | clip 0.1\n",
            "INFO: Epoch 151: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 152: loss 2.88 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.323 | clip 0.1\n",
            "INFO: Epoch 152: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 153: loss 2.879 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.91 | clip 0.1\n",
            "INFO: Epoch 153: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 154: loss 2.884 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.268 | clip 0.3\n",
            "INFO: Epoch 154: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 155: loss 2.865 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.064 | clip 0.3\n",
            "INFO: Epoch 155: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 156: loss 2.86 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.228 | clip 0.5\n",
            "INFO: Epoch 156: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 157: loss 2.886 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.655 | clip 0.8\n",
            "INFO: Epoch 157: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8\n",
            "INFO: Epoch 158: loss 2.849 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.669 | clip 0.2\n",
            "INFO: Epoch 158: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 159: loss 2.837 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.05 | clip 0.3\n",
            "INFO: Epoch 159: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 160: loss 2.831 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.716 | clip 0.2\n",
            "INFO: Epoch 160: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 161: loss 2.812 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.756 | clip 0.1\n",
            "INFO: Epoch 161: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 162: loss 2.804 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.479 | clip 0.3\n",
            "INFO: Epoch 162: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 163: loss 2.814 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.382 | clip 0.5\n",
            "INFO: Epoch 163: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 164: loss 2.786 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.276 | clip 0.3\n",
            "INFO: Epoch 164: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 165: loss 2.789 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.44 | clip 0.5\n",
            "INFO: Epoch 165: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 166: loss 2.8 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.253 | clip 0.7\n",
            "INFO: Epoch 166: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7\n",
            "INFO: Epoch 167: loss 2.767 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.284 | clip 0.2\n",
            "INFO: Epoch 167: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 168: loss 2.756 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.762 | clip 0.4\n",
            "INFO: Epoch 168: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 169: loss 2.759 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.094 | clip 0.3\n",
            "INFO: Epoch 169: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 170: loss 2.74 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.043 | clip 0.2\n",
            "INFO: Epoch 170: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 171: loss 2.738 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.767 | clip 0.4\n",
            "INFO: Epoch 171: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 172: loss 2.745 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.978 | clip 0.7\n",
            "INFO: Epoch 172: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 173: loss 2.721 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.362 | clip 0.2\n",
            "INFO: Epoch 173: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 174: loss 2.711 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.697 | clip 0.4\n",
            "INFO: Epoch 174: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 175: loss 2.715 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.086 | clip 0.4\n",
            "INFO: Epoch 175: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 176: loss 2.694 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.166 | clip 0.2\n",
            "INFO: Epoch 176: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 177: loss 2.69 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.965 | clip 0.5\n",
            "INFO: Epoch 177: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 178: loss 2.699 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.931 | clip 0.6\n",
            "INFO: Epoch 178: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 179: loss 2.676 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.386 | clip 0.2\n",
            "INFO: Epoch 179: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 180: loss 2.666 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.021 | clip 0.3\n",
            "INFO: Epoch 180: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 181: loss 2.662 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.611 | clip 0.1\n",
            "INFO: Epoch 181: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 182: loss 2.645 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.77 | clip 0.2\n",
            "INFO: Epoch 182: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 183: loss 2.641 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.551 | clip 0.3\n",
            "INFO: Epoch 183: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 184: loss 2.659 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.025 | clip 0.7\n",
            "INFO: Epoch 184: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 185: loss 2.626 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.432 | clip 0.2\n",
            "INFO: Epoch 185: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 186: loss 2.62 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.802 | clip 0.3\n",
            "INFO: Epoch 186: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 187: loss 2.619 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.004 | clip 0.3\n",
            "INFO: Epoch 187: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 188: loss 2.601 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.034 | clip 0.2\n",
            "INFO: Epoch 188: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 189: loss 2.597 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.985 | clip 0.4\n",
            "INFO: Epoch 189: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 190: loss 2.602 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.39 | clip 0.4\n",
            "INFO: Epoch 190: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 191: loss 2.58 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.132 | clip 0.2\n",
            "INFO: Epoch 191: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 192: loss 2.574 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.578 | clip 0.3\n",
            "INFO: Epoch 192: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 193: loss 2.578 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.233 | clip 0.4\n",
            "INFO: Epoch 193: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 194: loss 2.556 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.224 | clip 0.2\n",
            "INFO: Epoch 194: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 195: loss 2.551 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.692 | clip 0.3\n",
            "INFO: Epoch 195: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 196: loss 2.558 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.328 | clip 0.3\n",
            "INFO: Epoch 196: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 197: loss 2.535 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.98 | clip 0.1\n",
            "INFO: Epoch 197: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 198: loss 2.527 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.488 | clip 0.3\n",
            "INFO: Epoch 198: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 199: loss 2.537 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.335 | clip 0.4\n",
            "INFO: Epoch 199: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 200: loss 2.518 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.343 | clip 0.2\n",
            "INFO: Epoch 200: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 201: loss 2.51 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.798 | clip 0.4\n",
            "INFO: Epoch 201: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 202: loss 2.52 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.665 | clip 0.5\n",
            "INFO: Epoch 202: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 203: loss 2.497 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.494 | clip 0.3\n",
            "INFO: Epoch 203: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 204: loss 2.49 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.013 | clip 0.4\n",
            "INFO: Epoch 204: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 205: loss 2.493 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.258 | clip 0.4\n",
            "INFO: Epoch 205: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 206: loss 2.475 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.251 | clip 0.2\n",
            "INFO: Epoch 206: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 207: loss 2.467 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.652 | clip 0.3\n",
            "INFO: Epoch 207: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 208: loss 2.475 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.355 | clip 0.4\n",
            "INFO: Epoch 208: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 209: loss 2.456 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.149 | clip 0.2\n",
            "INFO: Epoch 209: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 210: loss 2.448 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.78 | clip 0.3\n",
            "INFO: Epoch 210: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 211: loss 2.448 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.905 | clip 0.2\n",
            "INFO: Epoch 211: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 212: loss 2.433 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.948 | clip 0.2\n",
            "INFO: Epoch 212: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 213: loss 2.427 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.505 | clip 0.3\n",
            "INFO: Epoch 213: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 214: loss 2.44 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.061 | clip 0.7\n",
            "INFO: Epoch 214: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 215: loss 2.415 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.418 | clip 0.2\n",
            "INFO: Epoch 215: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 216: loss 2.41 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.259 | clip 0.4\n",
            "INFO: Epoch 216: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 217: loss 2.414 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.124 | clip 0.3\n",
            "INFO: Epoch 217: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 218: loss 2.395 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.115 | clip 0.2\n",
            "INFO: Epoch 218: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 219: loss 2.39 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.93 | clip 0.4\n",
            "INFO: Epoch 219: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 220: loss 2.395 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.252 | clip 0.4\n",
            "INFO: Epoch 220: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 221: loss 2.376 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.125 | clip 0.2\n",
            "INFO: Epoch 221: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 222: loss 2.368 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.75 | clip 0.3\n",
            "INFO: Epoch 222: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 223: loss 2.365 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.75 | clip 0.3\n",
            "INFO: Epoch 223: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 224: loss 2.354 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.202 | clip 0.2\n",
            "INFO: Epoch 224: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 225: loss 2.357 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.53 | clip 0.4\n",
            "INFO: Epoch 225: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 226: loss 2.349 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.059 | clip 0.3\n",
            "INFO: Epoch 226: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 227: loss 2.337 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.121 | clip 0.2\n",
            "INFO: Epoch 227: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 228: loss 2.326 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.619 | clip 0.3\n",
            "INFO: Epoch 228: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 229: loss 2.342 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.39 | clip 0.4\n",
            "INFO: Epoch 229: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 230: loss 2.32 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.464 | clip 0.2\n",
            "INFO: Epoch 230: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 231: loss 2.322 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.433 | clip 0.4\n",
            "INFO: Epoch 231: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 232: loss 2.318 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.411 | clip 0.3\n",
            "INFO: Epoch 232: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 233: loss 2.304 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.245 | clip 0.2\n",
            "INFO: Epoch 233: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 234: loss 2.294 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.143 | clip 0.4\n",
            "INFO: Epoch 234: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 235: loss 2.3 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.85 | clip 0.3\n",
            "INFO: Epoch 235: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 236: loss 2.281 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.207 | clip 0.2\n",
            "INFO: Epoch 236: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 237: loss 2.273 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.07 | clip 0.3\n",
            "INFO: Epoch 237: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 238: loss 2.276 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.043 | clip 0.3\n",
            "INFO: Epoch 238: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 239: loss 2.264 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.116 | clip 0.2\n",
            "INFO: Epoch 239: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 240: loss 2.259 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.009 | clip 0.4\n",
            "INFO: Epoch 240: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 241: loss 2.261 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.356 | clip 0.4\n",
            "INFO: Epoch 241: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 242: loss 2.251 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.592 | clip 0.2\n",
            "INFO: Epoch 242: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 243: loss 2.245 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.07 | clip 0.4\n",
            "INFO: Epoch 243: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 244: loss 2.246 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.192 | clip 0.4\n",
            "INFO: Epoch 244: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 245: loss 2.232 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.445 | clip 0.2\n",
            "INFO: Epoch 245: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 246: loss 2.224 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.257 | clip 0.3\n",
            "INFO: Epoch 246: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 247: loss 2.221 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.56 | clip 0.2\n",
            "INFO: Epoch 247: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 248: loss 2.206 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.045 | clip 0.2\n",
            "INFO: Epoch 248: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 249: loss 2.208 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.95 | clip 0.3\n",
            "INFO: Epoch 249: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 250: loss 2.208 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.296 | clip 0.3\n",
            "INFO: Epoch 250: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 251: loss 2.194 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.305 | clip 0.3\n",
            "INFO: Epoch 251: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 252: loss 2.194 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.833 | clip 0.3\n",
            "INFO: Epoch 252: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 253: loss 2.185 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.941 | clip 0.3\n",
            "INFO: Epoch 253: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 254: loss 2.177 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.198 | clip 0.2\n",
            "INFO: Epoch 254: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 255: loss 2.17 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.776 | clip 0.3\n",
            "INFO: Epoch 255: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 256: loss 2.177 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.332 | clip 0.3\n",
            "INFO: Epoch 256: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 257: loss 2.16 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.292 | clip 0.2\n",
            "INFO: Epoch 257: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 258: loss 2.153 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.48 | clip 0.4\n",
            "INFO: Epoch 258: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 259: loss 2.153 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.103 | clip 0.2\n",
            "INFO: Epoch 259: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 260: loss 2.141 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.184 | clip 0.2\n",
            "INFO: Epoch 260: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 261: loss 2.133 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.946 | clip 0.4\n",
            "INFO: Epoch 261: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 262: loss 2.145 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.399 | clip 0.4\n",
            "INFO: Epoch 262: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 263: loss 2.128 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.583 | clip 0.2\n",
            "INFO: Epoch 263: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 264: loss 2.128 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.965 | clip 0.4\n",
            "INFO: Epoch 264: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 265: loss 2.12 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.533 | clip 0.2\n",
            "INFO: Epoch 265: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 266: loss 2.106 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.986 | clip 0.1\n",
            "INFO: Epoch 266: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 267: loss 2.109 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.984 | clip 0.3\n",
            "INFO: Epoch 267: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 268: loss 2.109 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.75 | clip 0.2\n",
            "INFO: Epoch 268: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 269: loss 2.091 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.123 | clip 0.2\n",
            "INFO: Epoch 269: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 270: loss 2.096 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.261 | clip 0.3\n",
            "INFO: Epoch 270: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 271: loss 2.09 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.84 | clip 0.3\n",
            "INFO: Epoch 271: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 272: loss 2.076 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.787 | clip 0.2\n",
            "INFO: Epoch 272: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 273: loss 2.084 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.207 | clip 0.3\n",
            "INFO: Epoch 273: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 274: loss 2.073 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.475 | clip 0.2\n",
            "INFO: Epoch 274: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 275: loss 2.066 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.115 | clip 0.1\n",
            "INFO: Epoch 275: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 276: loss 2.06 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.88 | clip 0.3\n",
            "INFO: Epoch 276: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 277: loss 2.052 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.34 | clip 0.2\n",
            "INFO: Epoch 277: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 278: loss 2.044 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.315 | clip 0.2\n",
            "INFO: Epoch 278: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 279: loss 2.051 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.247 | clip 0.3\n",
            "INFO: Epoch 279: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 280: loss 2.042 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.673 | clip 0.2\n",
            "INFO: Epoch 280: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 281: loss 2.036 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.477 | clip 0.3\n",
            "INFO: Epoch 281: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 282: loss 2.033 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.057 | clip 0.3\n",
            "INFO: Epoch 282: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 283: loss 2.025 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.569 | clip 0.2\n",
            "INFO: Epoch 283: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 284: loss 2.022 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.669 | clip 0.3\n",
            "INFO: Epoch 284: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 285: loss 2.022 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.359 | clip 0.3\n",
            "INFO: Epoch 285: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 286: loss 2.013 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.563 | clip 0.2\n",
            "INFO: Epoch 286: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 287: loss 2.005 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.373 | clip 0.2\n",
            "INFO: Epoch 287: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 288: loss 2.004 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.443 | clip 0.3\n",
            "INFO: Epoch 288: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 289: loss 1.992 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.324 | clip 0.2\n",
            "INFO: Epoch 289: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 290: loss 1.988 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.573 | clip 0.2\n",
            "INFO: Epoch 290: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 291: loss 1.991 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.366 | clip 0.3\n",
            "INFO: Epoch 291: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 292: loss 1.984 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.488 | clip 0.1\n",
            "INFO: Epoch 292: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 293: loss 1.973 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.34 | clip 0.2\n",
            "INFO: Epoch 293: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 294: loss 1.971 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.088 | clip 0.3\n",
            "INFO: Epoch 294: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 295: loss 1.967 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.42 | clip 0.2\n",
            "INFO: Epoch 295: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 296: loss 1.956 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.461 | clip 0.2\n",
            "INFO: Epoch 296: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 297: loss 1.955 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.307 | clip 0.3\n",
            "INFO: Epoch 297: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 298: loss 1.952 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.499 | clip 0.1\n",
            "INFO: Epoch 298: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 299: loss 1.941 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.375 | clip 0.2\n",
            "INFO: Epoch 299: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 300: loss 1.942 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.384 | clip 0.3\n",
            "INFO: Epoch 300: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 301: loss 1.941 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.806 | clip 0.3\n",
            "INFO: Epoch 301: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 302: loss 1.943 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.859 | clip 0.2\n",
            "INFO: Epoch 302: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 303: loss 1.935 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.796 | clip 0.4\n",
            "INFO: Epoch 303: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 304: loss 1.934 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.92 | clip 0.4\n",
            "INFO: Epoch 304: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 305: loss 1.923 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.623 | clip 0.2\n",
            "INFO: Epoch 305: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 306: loss 1.927 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.347 | clip 0.4\n",
            "INFO: Epoch 306: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 307: loss 1.923 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.9 | clip 0.4\n",
            "INFO: Epoch 307: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 308: loss 1.908 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.932 | clip 0.4\n",
            "INFO: Epoch 308: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 309: loss 1.924 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.695 | clip 0.7\n",
            "INFO: Epoch 309: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 310: loss 1.9 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.615 | clip 0.3\n",
            "INFO: Epoch 310: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 311: loss 1.895 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.878 | clip 0.3\n",
            "INFO: Epoch 311: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 312: loss 1.905 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.691 | clip 0.4\n",
            "INFO: Epoch 312: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 313: loss 1.88 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.438 | clip 0.3\n",
            "INFO: Epoch 313: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 314: loss 1.874 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.194 | clip 0.4\n",
            "INFO: Epoch 314: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 315: loss 1.891 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.723 | clip 0.5\n",
            "INFO: Epoch 315: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 316: loss 1.875 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.839 | clip 0.3\n",
            "INFO: Epoch 316: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 317: loss 1.874 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.782 | clip 0.3\n",
            "INFO: Epoch 317: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 318: loss 1.876 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.599 | clip 0.2\n",
            "INFO: Epoch 318: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 319: loss 1.858 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.196 | clip 0.2\n",
            "INFO: Epoch 319: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 320: loss 1.858 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.507 | clip 0.3\n",
            "INFO: Epoch 320: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 321: loss 1.854 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.889 | clip 0.4\n",
            "INFO: Epoch 321: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 322: loss 1.847 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.554 | clip 0.2\n",
            "INFO: Epoch 322: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 323: loss 1.842 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.4 | clip 0.4\n",
            "INFO: Epoch 323: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 324: loss 1.846 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.945 | clip 0.3\n",
            "INFO: Epoch 324: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 325: loss 1.836 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.005 | clip 0.3\n",
            "INFO: Epoch 325: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 326: loss 1.843 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.922 | clip 0.4\n",
            "INFO: Epoch 326: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 327: loss 1.833 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.761 | clip 0.3\n",
            "INFO: Epoch 327: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 328: loss 1.825 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.713 | clip 0.4\n",
            "INFO: Epoch 328: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 329: loss 1.83 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.889 | clip 0.5\n",
            "INFO: Epoch 329: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 330: loss 1.819 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.763 | clip 0.2\n",
            "INFO: Epoch 330: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 331: loss 1.816 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.585 | clip 0.2\n",
            "INFO: Epoch 331: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 332: loss 1.812 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.168 | clip 0.3\n",
            "INFO: Epoch 332: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 333: loss 1.803 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.455 | clip 0.2\n",
            "INFO: Epoch 333: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 334: loss 1.799 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.873 | clip 0.3\n",
            "INFO: Epoch 334: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 335: loss 1.806 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.725 | clip 0.3\n",
            "INFO: Epoch 335: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 336: loss 1.793 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.673 | clip 0.2\n",
            "INFO: Epoch 336: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 337: loss 1.791 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.57 | clip 0.4\n",
            "INFO: Epoch 337: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 338: loss 1.795 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.412 | clip 0.5\n",
            "INFO: Epoch 338: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 339: loss 1.782 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.856 | clip 0.3\n",
            "INFO: Epoch 339: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 340: loss 1.778 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.716 | clip 0.4\n",
            "INFO: Epoch 340: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 341: loss 1.781 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.306 | clip 0.5\n",
            "INFO: Epoch 341: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 342: loss 1.771 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.934 | clip 0.2\n",
            "INFO: Epoch 342: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 343: loss 1.771 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5 | clip 0.4\n",
            "INFO: Epoch 343: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 344: loss 1.774 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.869 | clip 0.3\n",
            "INFO: Epoch 344: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 345: loss 1.758 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.617 | clip 0.2\n",
            "INFO: Epoch 345: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 346: loss 1.753 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.025 | clip 0.3\n",
            "INFO: Epoch 346: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 347: loss 1.753 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.634 | clip 0.2\n",
            "INFO: Epoch 347: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 348: loss 1.745 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.605 | clip 0.2\n",
            "INFO: Epoch 348: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 349: loss 1.749 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.475 | clip 0.3\n",
            "INFO: Epoch 349: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 350: loss 1.74 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.635 | clip 0.2\n",
            "INFO: Epoch 350: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 351: loss 1.734 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.617 | clip 0.2\n",
            "INFO: Epoch 351: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 352: loss 1.738 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.045 | clip 0.4\n",
            "INFO: Epoch 352: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 353: loss 1.729 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.582 | clip 0.3\n",
            "INFO: Epoch 353: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 354: loss 1.728 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.771 | clip 0.4\n",
            "INFO: Epoch 354: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 355: loss 1.734 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.034 | clip 0.7\n",
            "INFO: Epoch 355: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E12 bs1000+lr0.0008"
      ],
      "metadata": {
        "id": "b649ngWoNitV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_12/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.0008"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebs5EbE2Nknk",
        "outputId": "edf3b28a-1865-41a1-877d-f455aaf2c5db"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_12/checkpoints --cuda --batch-size 1000 --lr 0.0008\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0008, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_12/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.187 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.754 | clip 0.1\n",
            "INFO: Epoch 000: valid_loss 7.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 2.6e+03\n",
            "INFO: Epoch 001: loss 7.091 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.45 | clip 1\n",
            "INFO: Epoch 001: valid_loss 6.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 450\n",
            "INFO: Epoch 002: loss 5.826 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.01 | clip 1\n",
            "INFO: Epoch 002: valid_loss 5.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 230\n",
            "INFO: Epoch 003: loss 5.441 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.94 | clip 0.8\n",
            "INFO: Epoch 003: valid_loss 5.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 192\n",
            "INFO: Epoch 004: loss 5.338 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.896 | clip 0.8\n",
            "INFO: Epoch 004: valid_loss 5.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 176\n",
            "INFO: Epoch 005: loss 5.263 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.477 | clip 0.7\n",
            "INFO: Epoch 005: valid_loss 5.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 164\n",
            "INFO: Epoch 006: loss 5.181 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.397 | clip 0.8\n",
            "INFO: Epoch 006: valid_loss 5.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 151\n",
            "INFO: Epoch 007: loss 5.111 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.05 | clip 0.8\n",
            "INFO: Epoch 007: valid_loss 4.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 140\n",
            "INFO: Epoch 008: loss 5.02 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.332 | clip 0.6\n",
            "INFO: Epoch 008: valid_loss 4.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 135\n",
            "INFO: Epoch 009: loss 5.008 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.859 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 4.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 127\n",
            "INFO: Epoch 010: loss 4.917 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.643 | clip 0.4\n",
            "INFO: Epoch 010: valid_loss 4.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 122\n",
            "INFO: Epoch 011: loss 4.895 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.826 | clip 0.7\n",
            "INFO: Epoch 011: valid_loss 4.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116\n",
            "INFO: Epoch 012: loss 4.828 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.534 | clip 0.4\n",
            "INFO: Epoch 012: valid_loss 4.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 114\n",
            "INFO: Epoch 013: loss 4.819 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.718 | clip 0.7\n",
            "INFO: Epoch 013: valid_loss 4.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 108\n",
            "INFO: Epoch 014: loss 4.757 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.16 | clip 0.4\n",
            "INFO: Epoch 014: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 105\n",
            "INFO: Epoch 015: loss 4.733 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.316 | clip 0.6\n",
            "INFO: Epoch 015: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 102\n",
            "INFO: Epoch 016: loss 4.704 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.377 | clip 0.7\n",
            "INFO: Epoch 016: valid_loss 4.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 99.2\n",
            "INFO: Epoch 017: loss 4.662 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.875 | clip 0.3\n",
            "INFO: Epoch 017: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.6\n",
            "INFO: Epoch 018: loss 4.628 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.661 | clip 0.2\n",
            "INFO: Epoch 018: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 94.2\n",
            "INFO: Epoch 019: loss 4.614 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.708 | clip 0.6\n",
            "INFO: Epoch 019: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 020: loss 4.681 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.751 | clip 0.9\n",
            "INFO: Epoch 020: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93\n",
            "INFO: Epoch 021: loss 4.562 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.412 | clip 0.2\n",
            "INFO: Epoch 021: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.1\n",
            "INFO: Epoch 022: loss 4.556 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.441 | clip 0.7\n",
            "INFO: Epoch 022: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.8\n",
            "INFO: Epoch 023: loss 4.503 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.906 | clip 0.4\n",
            "INFO: Epoch 023: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.1\n",
            "INFO: Epoch 024: loss 4.487 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.803 | clip 0.6\n",
            "INFO: Epoch 024: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82\n",
            "INFO: Epoch 025: loss 4.442 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.427 | clip 0.2\n",
            "INFO: Epoch 025: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.6\n",
            "INFO: Epoch 026: loss 4.413 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.881 | clip 0.3\n",
            "INFO: Epoch 026: valid_loss 4.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.7\n",
            "INFO: Epoch 027: loss 4.393 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.305 | clip 0.6\n",
            "INFO: Epoch 027: valid_loss 4.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.8\n",
            "INFO: Epoch 028: loss 4.357 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.553 | clip 0.5\n",
            "INFO: Epoch 028: valid_loss 4.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72.6\n",
            "INFO: Epoch 029: loss 4.315 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.799 | clip 0.2\n",
            "INFO: Epoch 029: valid_loss 4.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70.7\n",
            "INFO: Epoch 030: loss 4.28 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.281 | clip 0.2\n",
            "INFO: Epoch 030: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68\n",
            "INFO: Epoch 031: loss 4.246 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.222 | clip 0.2\n",
            "INFO: Epoch 031: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.1\n",
            "INFO: Epoch 032: loss 4.238 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.545 | clip 0.7\n",
            "INFO: Epoch 032: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.2\n",
            "INFO: Epoch 033: loss 4.202 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.428 | clip 0.8\n",
            "INFO: Epoch 033: valid_loss 4.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.6\n",
            "INFO: Epoch 034: loss 4.151 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.192 | clip 0.1\n",
            "INFO: Epoch 034: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.7\n",
            "INFO: Epoch 035: loss 4.116 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.178 | clip 0.2\n",
            "INFO: Epoch 035: valid_loss 4.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.5\n",
            "INFO: Epoch 036: loss 4.113 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.284 | clip 0.7\n",
            "INFO: Epoch 036: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.1\n",
            "INFO: Epoch 037: loss 4.101 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.1 | clip 0.8\n",
            "INFO: Epoch 037: valid_loss 4.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.3\n",
            "INFO: Epoch 038: loss 4.035 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.803 | clip 0.1\n",
            "INFO: Epoch 038: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.4\n",
            "INFO: Epoch 039: loss 4.009 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.122 | clip 0.2\n",
            "INFO: Epoch 039: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.9\n",
            "INFO: Epoch 040: loss 4.017 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.847 | clip 0.8\n",
            "INFO: Epoch 040: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.1\n",
            "INFO: Epoch 041: loss 3.95 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.659 | clip 0.2\n",
            "INFO: Epoch 041: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.2\n",
            "INFO: Epoch 042: loss 3.919 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.568 | clip 0.2\n",
            "INFO: Epoch 042: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.4\n",
            "INFO: Epoch 043: loss 3.906 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.034 | clip 0.5\n",
            "INFO: Epoch 043: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48\n",
            "INFO: Epoch 044: loss 3.871 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.445 | clip 0.2\n",
            "INFO: Epoch 044: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.6\n",
            "INFO: Epoch 045: loss 3.838 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.605 | clip 0.1\n",
            "INFO: Epoch 045: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.1\n",
            "INFO: Epoch 046: loss 3.811 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.683 | clip 0.1\n",
            "INFO: Epoch 046: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.8\n",
            "INFO: Epoch 047: loss 3.793 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.229 | clip 0.3\n",
            "INFO: Epoch 047: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.5\n",
            "INFO: Epoch 048: loss 3.814 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.299 | clip 0.8\n",
            "INFO: Epoch 048: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.5\n",
            "INFO: Epoch 049: loss 3.746 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.255 | clip 0.1\n",
            "INFO: Epoch 049: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.4\n",
            "INFO: Epoch 050: loss 3.72 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.237 | clip 0\n",
            "INFO: Epoch 050: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.6\n",
            "INFO: Epoch 051: loss 3.701 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.746 | clip 0.2\n",
            "INFO: Epoch 051: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.1\n",
            "INFO: Epoch 052: loss 3.69 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.694 | clip 0.5\n",
            "INFO: Epoch 052: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40\n",
            "INFO: Epoch 053: loss 3.662 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.998 | clip 0.1\n",
            "INFO: Epoch 053: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.6\n",
            "INFO: Epoch 054: loss 3.635 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.592 | clip 0.1\n",
            "INFO: Epoch 054: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.1\n",
            "INFO: Epoch 055: loss 3.614 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.989 | clip 0.1\n",
            "INFO: Epoch 055: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.3\n",
            "INFO: Epoch 056: loss 3.614 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.341 | clip 0.7\n",
            "INFO: Epoch 056: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.1\n",
            "INFO: Epoch 057: loss 3.665 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.089 | clip 0.9\n",
            "INFO: Epoch 057: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.3\n",
            "INFO: Epoch 058: loss 3.578 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.414 | clip 0.3\n",
            "INFO: Epoch 058: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.5\n",
            "INFO: Epoch 059: loss 3.581 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.792 | clip 0.6\n",
            "INFO: Epoch 059: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.6\n",
            "INFO: Epoch 060: loss 3.55 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.912 | clip 0.4\n",
            "INFO: Epoch 060: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.2\n",
            "INFO: Epoch 061: loss 3.51 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.025 | clip 0\n",
            "INFO: Epoch 061: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.5\n",
            "INFO: Epoch 062: loss 3.49 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.435 | clip 0\n",
            "INFO: Epoch 062: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.9\n",
            "INFO: Epoch 063: loss 3.492 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.763 | clip 0.4\n",
            "INFO: Epoch 063: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.7\n",
            "INFO: Epoch 064: loss 3.483 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.33 | clip 0.7\n",
            "INFO: Epoch 064: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 065: loss 3.441 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.472 | clip 0\n",
            "INFO: Epoch 065: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8\n",
            "INFO: Epoch 066: loss 3.425 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.224 | clip 0.4\n",
            "INFO: Epoch 066: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.3\n",
            "INFO: Epoch 067: loss 3.441 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5 | clip 0.6\n",
            "INFO: Epoch 067: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33\n",
            "INFO: Epoch 068: loss 3.431 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.536 | clip 0.7\n",
            "INFO: Epoch 068: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9\n",
            "INFO: Epoch 069: loss 3.385 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.221 | clip 0\n",
            "INFO: Epoch 069: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.9\n",
            "INFO: Epoch 070: loss 3.37 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.086 | clip 0.2\n",
            "INFO: Epoch 070: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.1\n",
            "INFO: Epoch 071: loss 3.399 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.822 | clip 0.8\n",
            "INFO: Epoch 071: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32\n",
            "INFO: Epoch 072: loss 3.33 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.181 | clip 0\n",
            "INFO: Epoch 072: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.6\n",
            "INFO: Epoch 073: loss 3.322 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.897 | clip 0.1\n",
            "INFO: Epoch 073: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31\n",
            "INFO: Epoch 074: loss 3.347 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.511 | clip 0.8\n",
            "INFO: Epoch 074: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.8\n",
            "INFO: Epoch 075: loss 3.291 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.179 | clip 0\n",
            "INFO: Epoch 075: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.5\n",
            "INFO: Epoch 076: loss 3.28 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.931 | clip 0.3\n",
            "INFO: Epoch 076: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.9\n",
            "INFO: Epoch 077: loss 3.309 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.651 | clip 0.8\n",
            "INFO: Epoch 077: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 078: loss 3.254 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.458 | clip 0.1\n",
            "INFO: Epoch 078: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 079: loss 3.252 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.709 | clip 0.4\n",
            "INFO: Epoch 079: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.6\n",
            "INFO: Epoch 080: loss 3.318 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.969 | clip 0.9\n",
            "INFO: Epoch 080: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 081: loss 3.234 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.385 | clip 0.3\n",
            "INFO: Epoch 081: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 082: loss 3.249 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.124 | clip 0.8\n",
            "INFO: Epoch 082: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.6\n",
            "INFO: Epoch 083: loss 3.19 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.176 | clip 0\n",
            "INFO: Epoch 083: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 084: loss 3.186 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.584 | clip 0.1\n",
            "INFO: Epoch 084: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 085: loss 3.163 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.219 | clip 0.1\n",
            "INFO: Epoch 085: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 086: loss 3.144 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.266 | clip 0\n",
            "INFO: Epoch 086: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 087: loss 3.134 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.707 | clip 0.1\n",
            "INFO: Epoch 087: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26\n",
            "INFO: Epoch 088: loss 3.117 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.227 | clip 0\n",
            "INFO: Epoch 088: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 089: loss 3.112 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.426 | clip 0.1\n",
            "INFO: Epoch 089: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.5\n",
            "INFO: Epoch 090: loss 3.092 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.342 | clip 0.1\n",
            "INFO: Epoch 090: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 091: loss 3.088 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.869 | clip 0.1\n",
            "INFO: Epoch 091: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 092: loss 3.066 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.533 | clip 0.1\n",
            "INFO: Epoch 092: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 093: loss 3.057 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.732 | clip 0.1\n",
            "INFO: Epoch 093: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.6\n",
            "INFO: Epoch 094: loss 3.026 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.194 | clip 0.1\n",
            "INFO: Epoch 094: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.1\n",
            "INFO: Epoch 095: loss 3.026 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.66 | clip 0.1\n",
            "INFO: Epoch 095: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 096: loss 3.011 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.874 | clip 0.2\n",
            "INFO: Epoch 096: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.6\n",
            "INFO: Epoch 097: loss 3.012 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.417 | clip 0.2\n",
            "INFO: Epoch 097: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 098: loss 2.98 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.607 | clip 0.1\n",
            "INFO: Epoch 098: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 099: loss 2.972 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.799 | clip 0.1\n",
            "INFO: Epoch 099: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 100: loss 2.979 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.515 | clip 0.6\n",
            "INFO: Epoch 100: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 101: loss 2.963 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.812 | clip 0.3\n",
            "INFO: Epoch 101: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 102: loss 2.934 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.116 | clip 0.3\n",
            "INFO: Epoch 102: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 103: loss 2.926 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.048 | clip 0.5\n",
            "INFO: Epoch 103: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.8\n",
            "INFO: Epoch 104: loss 2.986 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.649 | clip 0.8\n",
            "INFO: Epoch 104: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 105: loss 2.903 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.8 | clip 0.4\n",
            "INFO: Epoch 105: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9\n",
            "INFO: Epoch 106: loss 2.898 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.295 | clip 0.4\n",
            "INFO: Epoch 106: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6\n",
            "INFO: Epoch 107: loss 2.874 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.93 | clip 0.1\n",
            "INFO: Epoch 107: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 108: loss 2.848 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.282 | clip 0.1\n",
            "INFO: Epoch 108: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 109: loss 2.836 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.837 | clip 0.1\n",
            "INFO: Epoch 109: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 110: loss 2.865 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.089 | clip 0.7\n",
            "INFO: Epoch 110: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 111: loss 2.818 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.907 | clip 0.2\n",
            "INFO: Epoch 111: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4\n",
            "INFO: Epoch 112: loss 2.812 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.812 | clip 0.5\n",
            "INFO: Epoch 112: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 113: loss 2.872 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.422 | clip 0.9\n",
            "INFO: Epoch 113: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 114: loss 2.795 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.538 | clip 0.3\n",
            "INFO: Epoch 114: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 115: loss 2.807 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.774 | clip 0.5\n",
            "INFO: Epoch 115: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 116: loss 2.752 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.121 | clip 0.1\n",
            "INFO: Epoch 116: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 117: loss 2.737 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.389 | clip 0.1\n",
            "INFO: Epoch 117: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 118: loss 2.739 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.148 | clip 0.2\n",
            "INFO: Epoch 118: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 119: loss 2.711 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.397 | clip 0.1\n",
            "INFO: Epoch 119: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 120: loss 2.701 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.743 | clip 0.1\n",
            "INFO: Epoch 120: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 121: loss 2.709 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.148 | clip 0.3\n",
            "INFO: Epoch 121: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 122: loss 2.679 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.507 | clip 0.1\n",
            "INFO: Epoch 122: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 123: loss 2.664 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.873 | clip 0.1\n",
            "INFO: Epoch 123: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 124: loss 2.684 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.41 | clip 0.5\n",
            "INFO: Epoch 124: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 125: loss 2.653 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.98 | clip 0.1\n",
            "INFO: Epoch 125: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 126: loss 2.638 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.254 | clip 0.2\n",
            "INFO: Epoch 126: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 127: loss 2.656 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.768 | clip 0.3\n",
            "INFO: Epoch 127: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 128: loss 2.607 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.193 | clip 0\n",
            "INFO: Epoch 128: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 129: loss 2.588 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.383 | clip 0.1\n",
            "INFO: Epoch 129: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 130: loss 2.593 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.18 | clip 0.2\n",
            "INFO: Epoch 130: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 131: loss 2.573 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.847 | clip 0.1\n",
            "INFO: Epoch 131: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 132: loss 2.561 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.974 | clip 0.1\n",
            "INFO: Epoch 132: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 133: loss 2.564 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.621 | clip 0.3\n",
            "INFO: Epoch 133: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 134: loss 2.544 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.043 | clip 0.2\n",
            "INFO: Epoch 134: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 135: loss 2.531 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.89 | clip 0.2\n",
            "INFO: Epoch 135: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 136: loss 2.515 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.876 | clip 0.2\n",
            "INFO: Epoch 136: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 137: loss 2.524 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.686 | clip 0.3\n",
            "INFO: Epoch 137: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 138: loss 2.501 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.919 | clip 0.2\n",
            "INFO: Epoch 138: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 139: loss 2.478 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.706 | clip 0.1\n",
            "INFO: Epoch 139: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 140: loss 2.464 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.443 | clip 0.1\n",
            "INFO: Epoch 140: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 141: loss 2.461 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.978 | clip 0.1\n",
            "INFO: Epoch 141: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 142: loss 2.449 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.211 | clip 0.1\n",
            "INFO: Epoch 142: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 143: loss 2.441 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.791 | clip 0.1\n",
            "INFO: Epoch 143: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 144: loss 2.434 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.077 | clip 0.1\n",
            "INFO: Epoch 144: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 145: loss 2.434 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.15 | clip 0.3\n",
            "INFO: Epoch 145: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 146: loss 2.418 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.332 | clip 0.3\n",
            "INFO: Epoch 146: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 147: loss 2.403 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.313 | clip 0.3\n",
            "INFO: Epoch 147: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 148: loss 2.395 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.533 | clip 0.5\n",
            "INFO: Epoch 148: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 149: loss 2.447 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.098 | clip 1\n",
            "INFO: Epoch 149: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 150: loss 2.383 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.536 | clip 0.1\n",
            "INFO: Epoch 150: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 151: loss 2.372 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.726 | clip 0.3\n",
            "INFO: Epoch 151: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 152: loss 2.365 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.903 | clip 0.3\n",
            "INFO: Epoch 152: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 153: loss 2.341 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.044 | clip 0.3\n",
            "INFO: Epoch 153: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 154: loss 2.331 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.201 | clip 0.2\n",
            "INFO: Epoch 154: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 155: loss 2.365 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.457 | clip 0.8\n",
            "INFO: Epoch 155: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 156: loss 2.308 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.734 | clip 0.1\n",
            "INFO: Epoch 156: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 157: loss 2.299 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.048 | clip 0.1\n",
            "INFO: Epoch 157: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 158: loss 2.326 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.376 | clip 0.4\n",
            "INFO: Epoch 158: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 159: loss 2.286 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.37 | clip 0.2\n",
            "INFO: Epoch 159: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 160: loss 2.283 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.051 | clip 0.5\n",
            "INFO: Epoch 160: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 161: loss 2.307 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.964 | clip 0.8\n",
            "INFO: Epoch 161: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 162: loss 2.27 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.599 | clip 0.2\n",
            "INFO: Epoch 162: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 163: loss 2.261 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.856 | clip 0.3\n",
            "INFO: Epoch 163: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 164: loss 2.238 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.254 | clip 0.1\n",
            "INFO: Epoch 164: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 165: loss 2.228 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.653 | clip 0.1\n",
            "INFO: Epoch 165: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 166: loss 2.222 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.494 | clip 0.3\n",
            "INFO: Epoch 166: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 167: loss 2.241 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.473 | clip 0.5\n",
            "INFO: Epoch 167: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 168: loss 2.215 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.505 | clip 0.2\n",
            "INFO: Epoch 168: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 169: loss 2.197 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.722 | clip 0.4\n",
            "INFO: Epoch 169: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 170: loss 2.206 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.165 | clip 0.4\n",
            "INFO: Epoch 170: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 171: loss 2.188 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.589 | clip 0.2\n",
            "INFO: Epoch 171: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 172: loss 2.181 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.104 | clip 0.3\n",
            "INFO: Epoch 172: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 173: loss 2.186 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.093 | clip 0.3\n",
            "INFO: Epoch 173: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 174: loss 2.156 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.264 | clip 0.2\n",
            "INFO: Epoch 174: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 175: loss 2.153 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.838 | clip 0.4\n",
            "INFO: Epoch 175: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 176: loss 2.148 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.918 | clip 0.3\n",
            "INFO: Epoch 176: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 177: loss 2.14 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.587 | clip 0.2\n",
            "INFO: Epoch 177: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 178: loss 2.137 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.533 | clip 0.3\n",
            "INFO: Epoch 178: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 179: loss 2.131 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.803 | clip 0.2\n",
            "INFO: Epoch 179: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 180: loss 2.106 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.972 | clip 0.2\n",
            "INFO: Epoch 180: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 181: loss 2.099 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.334 | clip 0.3\n",
            "INFO: Epoch 181: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 182: loss 2.102 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.882 | clip 0.2\n",
            "INFO: Epoch 182: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 183: loss 2.084 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.324 | clip 0.2\n",
            "INFO: Epoch 183: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 184: loss 2.087 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.145 | clip 0.4\n",
            "INFO: Epoch 184: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 185: loss 2.083 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.015 | clip 0.3\n",
            "INFO: Epoch 185: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 186: loss 2.062 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.221 | clip 0.2\n",
            "INFO: Epoch 186: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 187: loss 2.052 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.357 | clip 0.3\n",
            "INFO: Epoch 187: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 188: loss 2.059 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.035 | clip 0.3\n",
            "INFO: Epoch 188: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 189: loss 2.041 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.753 | clip 0.2\n",
            "INFO: Epoch 189: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 190: loss 2.035 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.187 | clip 0.4\n",
            "INFO: Epoch 190: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 191: loss 2.044 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.294 | clip 0.4\n",
            "INFO: Epoch 191: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 192: loss 2.023 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.529 | clip 0.2\n",
            "INFO: Epoch 192: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 193: loss 2.012 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.809 | clip 0.4\n",
            "INFO: Epoch 193: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 194: loss 2.01 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.67 | clip 0.2\n",
            "INFO: Epoch 194: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 195: loss 1.991 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.134 | clip 0.2\n",
            "INFO: Epoch 195: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 196: loss 1.991 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.677 | clip 0.3\n",
            "INFO: Epoch 196: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 197: loss 2.003 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.353 | clip 0.6\n",
            "INFO: Epoch 197: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 198: loss 1.982 | lr 0.0008 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.605 | clip 0.2\n",
            "INFO: Epoch 198: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E13 bs1000+lr0.0005+layer2"
      ],
      "metadata": {
        "id": "B8D52RpeRBAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_13/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.0005 \\\n",
        "    --encoder-num-layers 2 \\\n",
        "    --decoder-num-layers 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVrSMfOoRNlX",
        "outputId": "ca6d1843-2f87-4722-f413-96967ca5dea4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_13/checkpoints --cuda --batch-size 1000 --lr 0.0005 --encoder-num-layers 2 --decoder-num-layers 2\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_13/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1540000 parameters\n",
            "INFO: Epoch 000: loss 8.247 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.095 | clip 0\n",
            "INFO: Epoch 000: valid_loss 8.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.41e+03\n",
            "INFO: Epoch 001: loss 7.805 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.305 | clip 0.7\n",
            "INFO: Epoch 001: valid_loss 6.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 1.05e+03\n",
            "INFO: Epoch 002: loss 6.488 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.59 | clip 1\n",
            "INFO: Epoch 002: valid_loss 5.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 339\n",
            "INFO: Epoch 003: loss 5.727 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.955 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 229\n",
            "INFO: Epoch 004: loss 5.475 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.17 | clip 0.9\n",
            "INFO: Epoch 004: valid_loss 5.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 201\n",
            "INFO: Epoch 005: loss 5.406 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.062 | clip 0.4\n",
            "INFO: Epoch 005: valid_loss 5.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 194\n",
            "INFO: Epoch 006: loss 5.386 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.095 | clip 0.5\n",
            "INFO: Epoch 006: valid_loss 5.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 191\n",
            "INFO: Epoch 007: loss 5.37 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.977 | clip 0.5\n",
            "INFO: Epoch 007: valid_loss 5.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 187\n",
            "INFO: Epoch 008: loss 5.347 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.035 | clip 0.5\n",
            "INFO: Epoch 008: valid_loss 5.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 183\n",
            "INFO: Epoch 009: loss 5.32 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.64 | clip 0.5\n",
            "INFO: Epoch 009: valid_loss 5.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 178\n",
            "INFO: Epoch 010: loss 5.295 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.999 | clip 0.7\n",
            "INFO: Epoch 010: valid_loss 5.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 174\n",
            "INFO: Epoch 011: loss 5.261 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.423 | clip 0.7\n",
            "INFO: Epoch 011: valid_loss 5.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 170\n",
            "INFO: Epoch 012: loss 5.255 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.291 | clip 0.8\n",
            "INFO: Epoch 012: valid_loss 5.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 166\n",
            "INFO: Epoch 013: loss 5.201 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.16 | clip 0.6\n",
            "INFO: Epoch 013: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169\n",
            "INFO: Epoch 014: loss 5.272 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.24 | clip 0.9\n",
            "INFO: Epoch 014: valid_loss 5.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 164\n",
            "INFO: Epoch 015: loss 5.162 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.211 | clip 0.5\n",
            "INFO: Epoch 015: valid_loss 5.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 167\n",
            "INFO: Epoch 016: loss 5.285 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.87 | clip 0.9\n",
            "INFO: Epoch 016: valid_loss 5.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 157\n",
            "INFO: Epoch 017: loss 5.156 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.184 | clip 0.7\n",
            "INFO: Epoch 017: valid_loss 5.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 154\n",
            "INFO: Epoch 018: loss 5.142 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.603 | clip 0.8\n",
            "INFO: Epoch 018: valid_loss 5.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 153\n",
            "INFO: Epoch 019: loss 5.11 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.434 | clip 0.7\n",
            "INFO: Epoch 019: valid_loss 5.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 150\n",
            "INFO: Epoch 020: loss 5.123 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.395 | clip 0.8\n",
            "INFO: Epoch 020: valid_loss 5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 148\n",
            "INFO: Epoch 021: loss 5.064 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.45 | clip 0.5\n",
            "INFO: Epoch 021: valid_loss 4.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 148\n",
            "INFO: Epoch 022: loss 5.109 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.97 | clip 0.9\n",
            "INFO: Epoch 022: valid_loss 4.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 146\n",
            "INFO: Epoch 023: loss 5.028 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.108 | clip 0.3\n",
            "INFO: Epoch 023: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 024: loss 5.023 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.68 | clip 0.9\n",
            "INFO: Epoch 024: valid_loss 4.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 133\n",
            "INFO: Epoch 025: loss 4.989 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.517 | clip 0.8\n",
            "INFO: Epoch 025: valid_loss 4.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 132\n",
            "INFO: Epoch 026: loss 4.95 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.627 | clip 0.3\n",
            "INFO: Epoch 026: valid_loss 4.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 128\n",
            "INFO: Epoch 027: loss 4.926 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.07 | clip 0.5\n",
            "INFO: Epoch 027: valid_loss 4.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 125\n",
            "INFO: Epoch 028: loss 4.926 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.261 | clip 0.8\n",
            "INFO: Epoch 028: valid_loss 4.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 125\n",
            "INFO: Epoch 029: loss 4.878 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.305 | clip 0.3\n",
            "INFO: Epoch 029: valid_loss 4.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 121\n",
            "INFO: Epoch 030: loss 4.883 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.543 | clip 0.8\n",
            "INFO: Epoch 030: valid_loss 4.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 120\n",
            "INFO: Epoch 031: loss 4.842 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.117 | clip 0.3\n",
            "INFO: Epoch 031: valid_loss 4.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 117\n",
            "INFO: Epoch 032: loss 4.838 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.008 | clip 0.7\n",
            "INFO: Epoch 032: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116\n",
            "INFO: Epoch 033: loss 4.811 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.883 | clip 0.2\n",
            "INFO: Epoch 033: valid_loss 4.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 114\n",
            "INFO: Epoch 034: loss 4.795 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.035 | clip 0.3\n",
            "INFO: Epoch 034: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112\n",
            "INFO: Epoch 035: loss 4.789 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.087 | clip 0.5\n",
            "INFO: Epoch 035: valid_loss 4.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 111\n",
            "INFO: Epoch 036: loss 4.771 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.33 | clip 0.5\n",
            "INFO: Epoch 036: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 110\n",
            "INFO: Epoch 037: loss 4.756 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.756 | clip 0.2\n",
            "INFO: Epoch 037: valid_loss 4.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 108\n",
            "INFO: Epoch 038: loss 4.737 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.281 | clip 0.2\n",
            "INFO: Epoch 038: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 107\n",
            "INFO: Epoch 039: loss 4.732 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.86 | clip 0.5\n",
            "INFO: Epoch 039: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 107\n",
            "INFO: Epoch 040: loss 4.728 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.304 | clip 0.6\n",
            "INFO: Epoch 040: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 105\n",
            "INFO: Epoch 041: loss 4.724 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.614 | clip 0.7\n",
            "INFO: Epoch 041: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106\n",
            "INFO: Epoch 042: loss 4.716 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.295 | clip 0.6\n",
            "INFO: Epoch 042: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 107\n",
            "INFO: Epoch 043: loss 4.692 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.036 | clip 0.6\n",
            "INFO: Epoch 043: valid_loss 4.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 103\n",
            "INFO: Epoch 044: loss 4.703 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.452 | clip 0.7\n",
            "INFO: Epoch 044: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 103\n",
            "INFO: Epoch 045: loss 4.701 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.26 | clip 0.9\n",
            "INFO: Epoch 045: valid_loss 4.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104\n",
            "INFO: Epoch 046: loss 4.667 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.797 | clip 0.2\n",
            "INFO: Epoch 046: valid_loss 4.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 047: loss 4.658 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.702 | clip 0.5\n",
            "INFO: Epoch 047: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 103\n",
            "INFO: Epoch 048: loss 4.696 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.665 | clip 0.9\n",
            "INFO: Epoch 048: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 102\n",
            "INFO: Epoch 049: loss 4.639 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.67 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.7\n",
            "INFO: Epoch 050: loss 4.637 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.164 | clip 0.7\n",
            "INFO: Epoch 050: valid_loss 4.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 97.1\n",
            "INFO: Epoch 051: loss 4.629 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.156 | clip 0.7\n",
            "INFO: Epoch 051: valid_loss 4.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 97.4\n",
            "INFO: Epoch 052: loss 4.6 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.113 | clip 0.1\n",
            "INFO: Epoch 052: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95.1\n",
            "INFO: Epoch 053: loss 4.584 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.912 | clip 0.4\n",
            "INFO: Epoch 053: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.5\n",
            "INFO: Epoch 054: loss 4.63 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.703 | clip 0.9\n",
            "INFO: Epoch 054: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.7\n",
            "INFO: Epoch 055: loss 4.562 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.092 | clip 0.1\n",
            "INFO: Epoch 055: valid_loss 4.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.9\n",
            "INFO: Epoch 056: loss 4.563 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.205 | clip 0.7\n",
            "INFO: Epoch 056: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.3\n",
            "INFO: Epoch 057: loss 4.54 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.497 | clip 0.5\n",
            "INFO: Epoch 057: valid_loss 4.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.4\n",
            "INFO: Epoch 058: loss 4.516 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.012 | clip 0.1\n",
            "INFO: Epoch 058: valid_loss 4.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 87.7\n",
            "INFO: Epoch 059: loss 4.495 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.965 | clip 0.2\n",
            "INFO: Epoch 059: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.5\n",
            "INFO: Epoch 060: loss 4.496 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.233 | clip 0.6\n",
            "INFO: Epoch 060: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.6\n",
            "INFO: Epoch 061: loss 4.476 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.369 | clip 0.5\n",
            "INFO: Epoch 061: valid_loss 4.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 83.6\n",
            "INFO: Epoch 062: loss 4.452 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.931 | clip 0.1\n",
            "INFO: Epoch 062: valid_loss 4.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.8\n",
            "INFO: Epoch 063: loss 4.431 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.908 | clip 0.1\n",
            "INFO: Epoch 063: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.9\n",
            "INFO: Epoch 064: loss 4.416 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.393 | clip 0.3\n",
            "INFO: Epoch 064: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.2\n",
            "INFO: Epoch 065: loss 4.447 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.696 | clip 0.8\n",
            "INFO: Epoch 065: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.7\n",
            "INFO: Epoch 066: loss 4.392 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.01 | clip 0.1\n",
            "INFO: Epoch 066: valid_loss 4.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.4\n",
            "INFO: Epoch 067: loss 4.37 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.104 | clip 0.2\n",
            "INFO: Epoch 067: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.9\n",
            "INFO: Epoch 068: loss 4.377 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.82 | clip 0.7\n",
            "INFO: Epoch 068: valid_loss 4.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75\n",
            "INFO: Epoch 069: loss 4.345 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.269 | clip 0.1\n",
            "INFO: Epoch 069: valid_loss 4.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.3\n",
            "INFO: Epoch 070: loss 4.329 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.782 | clip 0.1\n",
            "INFO: Epoch 070: valid_loss 4.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.2\n",
            "INFO: Epoch 071: loss 4.311 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.085 | clip 0.1\n",
            "INFO: Epoch 071: valid_loss 4.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72.8\n",
            "INFO: Epoch 072: loss 4.328 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.72 | clip 0.7\n",
            "INFO: Epoch 072: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.3\n",
            "INFO: Epoch 073: loss 4.294 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.629 | clip 0.3\n",
            "INFO: Epoch 073: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70.4\n",
            "INFO: Epoch 074: loss 4.269 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.47 | clip 0.1\n",
            "INFO: Epoch 074: valid_loss 4.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.5\n",
            "INFO: Epoch 075: loss 4.256 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.644 | clip 0.1\n",
            "INFO: Epoch 075: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.2\n",
            "INFO: Epoch 076: loss 4.243 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.291 | clip 0.3\n",
            "INFO: Epoch 076: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.1\n",
            "INFO: Epoch 077: loss 4.254 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.604 | clip 0.8\n",
            "INFO: Epoch 077: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.4\n",
            "INFO: Epoch 078: loss 4.221 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.068 | clip 0.1\n",
            "INFO: Epoch 078: valid_loss 4.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.5\n",
            "INFO: Epoch 079: loss 4.207 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.095 | clip 0.1\n",
            "INFO: Epoch 079: valid_loss 4.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.9\n",
            "INFO: Epoch 080: loss 4.208 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.525 | clip 0.6\n",
            "INFO: Epoch 080: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64\n",
            "INFO: Epoch 081: loss 4.187 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.73 | clip 0.4\n",
            "INFO: Epoch 081: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.5\n",
            "INFO: Epoch 082: loss 4.168 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.982 | clip 0.1\n",
            "INFO: Epoch 082: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.5\n",
            "INFO: Epoch 083: loss 4.148 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.91 | clip 0.1\n",
            "INFO: Epoch 083: valid_loss 4.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.9\n",
            "INFO: Epoch 084: loss 4.166 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.252 | clip 0.8\n",
            "INFO: Epoch 084: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.3\n",
            "INFO: Epoch 085: loss 4.125 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.814 | clip 0.1\n",
            "INFO: Epoch 085: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.1\n",
            "INFO: Epoch 086: loss 4.109 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.654 | clip 0.1\n",
            "INFO: Epoch 086: valid_loss 4.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.5\n",
            "INFO: Epoch 087: loss 4.098 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.995 | clip 0.1\n",
            "INFO: Epoch 087: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.8\n",
            "INFO: Epoch 088: loss 4.095 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.224 | clip 0.6\n",
            "INFO: Epoch 088: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.8\n",
            "INFO: Epoch 089: loss 4.078 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.153 | clip 0.1\n",
            "INFO: Epoch 089: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.3\n",
            "INFO: Epoch 090: loss 4.06 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.539 | clip 0.1\n",
            "INFO: Epoch 090: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57\n",
            "INFO: Epoch 091: loss 4.053 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.491 | clip 0.2\n",
            "INFO: Epoch 091: valid_loss 4.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.4\n",
            "INFO: Epoch 092: loss 4.058 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.303 | clip 0.7\n",
            "INFO: Epoch 092: valid_loss 4.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.3\n",
            "INFO: Epoch 093: loss 4.026 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.636 | clip 0.1\n",
            "INFO: Epoch 093: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.9\n",
            "INFO: Epoch 094: loss 4.016 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.237 | clip 0.3\n",
            "INFO: Epoch 094: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.4\n",
            "INFO: Epoch 095: loss 4 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.206 | clip 0.4\n",
            "INFO: Epoch 095: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.7\n",
            "INFO: Epoch 096: loss 4.03 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.328 | clip 0.9\n",
            "INFO: Epoch 096: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.9\n",
            "INFO: Epoch 097: loss 3.973 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.455 | clip 0.1\n",
            "INFO: Epoch 097: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.6\n",
            "INFO: Epoch 098: loss 3.961 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.678 | clip 0.1\n",
            "INFO: Epoch 098: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.3\n",
            "INFO: Epoch 099: loss 3.965 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.95 | clip 0.4\n",
            "INFO: Epoch 099: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.7\n",
            "INFO: Epoch 100: loss 3.95 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.186 | clip 0.5\n",
            "INFO: Epoch 100: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.6\n",
            "INFO: Epoch 101: loss 3.934 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.177 | clip 0.2\n",
            "INFO: Epoch 101: valid_loss 3.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.3\n",
            "INFO: Epoch 102: loss 3.92 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.542 | clip 0.3\n",
            "INFO: Epoch 102: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50\n",
            "INFO: Epoch 103: loss 3.906 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.129 | clip 0.2\n",
            "INFO: Epoch 103: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.4\n",
            "INFO: Epoch 104: loss 3.936 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.268 | clip 0.9\n",
            "INFO: Epoch 104: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.6\n",
            "INFO: Epoch 105: loss 3.884 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.766 | clip 0.1\n",
            "INFO: Epoch 105: valid_loss 3.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.1\n",
            "INFO: Epoch 106: loss 3.874 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.271 | clip 0.2\n",
            "INFO: Epoch 106: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.6\n",
            "INFO: Epoch 107: loss 3.895 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.031 | clip 0.8\n",
            "INFO: Epoch 107: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.9\n",
            "INFO: Epoch 108: loss 3.856 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.784 | clip 0.1\n",
            "INFO: Epoch 108: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.8\n",
            "INFO: Epoch 109: loss 3.846 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.479 | clip 0.3\n",
            "INFO: Epoch 109: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.8\n",
            "INFO: Epoch 110: loss 3.881 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.756 | clip 0.9\n",
            "INFO: Epoch 110: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47\n",
            "INFO: Epoch 111: loss 3.823 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.665 | clip 0.1\n",
            "INFO: Epoch 111: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.6\n",
            "INFO: Epoch 112: loss 3.807 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.983 | clip 0.1\n",
            "INFO: Epoch 112: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.8\n",
            "INFO: Epoch 113: loss 3.828 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.16 | clip 0.9\n",
            "INFO: Epoch 113: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.2\n",
            "INFO: Epoch 114: loss 3.79 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.91 | clip 0.1\n",
            "INFO: Epoch 114: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.7\n",
            "INFO: Epoch 115: loss 3.776 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.038 | clip 0.1\n",
            "INFO: Epoch 115: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.3\n",
            "INFO: Epoch 116: loss 3.794 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.209 | clip 0.7\n",
            "INFO: Epoch 116: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.5\n",
            "INFO: Epoch 117: loss 3.756 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.892 | clip 0.1\n",
            "INFO: Epoch 117: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.6\n",
            "INFO: Epoch 118: loss 3.744 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.71 | clip 0.1\n",
            "INFO: Epoch 118: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.3\n",
            "INFO: Epoch 119: loss 3.758 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.494 | clip 0.8\n",
            "INFO: Epoch 119: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.4\n",
            "INFO: Epoch 120: loss 3.725 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.624 | clip 0.1\n",
            "INFO: Epoch 120: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.5\n",
            "INFO: Epoch 121: loss 3.717 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.875 | clip 0.4\n",
            "INFO: Epoch 121: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.5\n",
            "INFO: Epoch 122: loss 3.782 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.876 | clip 0.9\n",
            "INFO: Epoch 122: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.5\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E14 bs500+lr0.0005"
      ],
      "metadata": {
        "id": "x-fFoImIStEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_14/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 500\\\n",
        "    --lr 0.0005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7waZKzxS01_",
        "outputId": "0fe34646-88db-4a7d-9deb-19c49bc0afb9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_14/checkpoints --cuda --batch-size 500 --lr 0.0005\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 500, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_14/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.083 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.632 | clip 0.3\n",
            "INFO: Epoch 000: valid_loss 7.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 1.41e+03\n",
            "INFO: Epoch 001: loss 6.368 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 11.67 | clip 1\n",
            "INFO: Epoch 001: valid_loss 5.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 265\n",
            "INFO: Epoch 002: loss 5.483 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 7.196 | clip 0.85\n",
            "INFO: Epoch 002: valid_loss 5.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 193\n",
            "INFO: Epoch 003: loss 5.343 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.299 | clip 0.6\n",
            "INFO: Epoch 003: valid_loss 5.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 176\n",
            "INFO: Epoch 004: loss 5.243 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.865 | clip 0.75\n",
            "INFO: Epoch 004: valid_loss 5.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 162\n",
            "INFO: Epoch 005: loss 5.14 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 6.481 | clip 0.9\n",
            "INFO: Epoch 005: valid_loss 5.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 150\n",
            "INFO: Epoch 006: loss 5.066 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 7.212 | clip 0.95\n",
            "INFO: Epoch 006: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 138\n",
            "INFO: Epoch 007: loss 4.999 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 7.225 | clip 0.85\n",
            "INFO: Epoch 007: valid_loss 4.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 126\n",
            "INFO: Epoch 008: loss 4.925 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 6.494 | clip 0.7\n",
            "INFO: Epoch 008: valid_loss 4.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 119\n",
            "INFO: Epoch 009: loss 4.851 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.43 | clip 0.75\n",
            "INFO: Epoch 009: valid_loss 4.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 114\n",
            "INFO: Epoch 010: loss 4.813 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.721 | clip 0.7\n",
            "INFO: Epoch 010: valid_loss 4.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 109\n",
            "INFO: Epoch 011: loss 4.761 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.46 | clip 0.75\n",
            "INFO: Epoch 011: valid_loss 4.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104\n",
            "INFO: Epoch 012: loss 4.711 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.973 | clip 0.7\n",
            "INFO: Epoch 012: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 100\n",
            "INFO: Epoch 013: loss 4.671 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.872 | clip 0.6\n",
            "INFO: Epoch 013: valid_loss 4.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 97\n",
            "INFO: Epoch 014: loss 4.63 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.756 | clip 0.5\n",
            "INFO: Epoch 014: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 94\n",
            "INFO: Epoch 015: loss 4.593 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.87 | clip 0.45\n",
            "INFO: Epoch 015: valid_loss 4.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.6\n",
            "INFO: Epoch 016: loss 4.563 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.99 | clip 0.55\n",
            "INFO: Epoch 016: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.8\n",
            "INFO: Epoch 017: loss 4.538 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.531 | clip 0.75\n",
            "INFO: Epoch 017: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.1\n",
            "INFO: Epoch 018: loss 4.519 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 6.403 | clip 0.75\n",
            "INFO: Epoch 018: valid_loss 4.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.9\n",
            "INFO: Epoch 019: loss 4.465 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.782 | clip 0.7\n",
            "INFO: Epoch 019: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.6\n",
            "INFO: Epoch 020: loss 4.42 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.072 | clip 0.7\n",
            "INFO: Epoch 020: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.1\n",
            "INFO: Epoch 021: loss 4.385 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.9 | clip 0.65\n",
            "INFO: Epoch 021: valid_loss 4.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.6\n",
            "INFO: Epoch 022: loss 4.347 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.787 | clip 0.65\n",
            "INFO: Epoch 022: valid_loss 4.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72.2\n",
            "INFO: Epoch 023: loss 4.311 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.796 | clip 0.65\n",
            "INFO: Epoch 023: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.8\n",
            "INFO: Epoch 024: loss 4.275 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.692 | clip 0.6\n",
            "INFO: Epoch 024: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.5\n",
            "INFO: Epoch 025: loss 4.239 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.625 | clip 0.6\n",
            "INFO: Epoch 025: valid_loss 4.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.5\n",
            "INFO: Epoch 026: loss 4.209 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.651 | clip 0.6\n",
            "INFO: Epoch 026: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.2\n",
            "INFO: Epoch 027: loss 4.171 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.427 | clip 0.55\n",
            "INFO: Epoch 027: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.3\n",
            "INFO: Epoch 028: loss 4.136 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.249 | clip 0.55\n",
            "INFO: Epoch 028: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59\n",
            "INFO: Epoch 029: loss 4.098 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.035 | clip 0.4\n",
            "INFO: Epoch 029: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.4\n",
            "INFO: Epoch 030: loss 4.066 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.933 | clip 0.4\n",
            "INFO: Epoch 030: valid_loss 4.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.2\n",
            "INFO: Epoch 031: loss 4.031 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.759 | clip 0.35\n",
            "INFO: Epoch 031: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.1\n",
            "INFO: Epoch 032: loss 4.001 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.729 | clip 0.3\n",
            "INFO: Epoch 032: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.2\n",
            "INFO: Epoch 033: loss 3.961 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.512 | clip 0.3\n",
            "INFO: Epoch 033: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51\n",
            "INFO: Epoch 034: loss 3.933 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.559 | clip 0.3\n",
            "INFO: Epoch 034: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.3\n",
            "INFO: Epoch 035: loss 3.899 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.493 | clip 0.3\n",
            "INFO: Epoch 035: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.3\n",
            "INFO: Epoch 036: loss 3.877 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.593 | clip 0.3\n",
            "INFO: Epoch 036: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.6\n",
            "INFO: Epoch 037: loss 3.843 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.388 | clip 0.25\n",
            "INFO: Epoch 037: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.9\n",
            "INFO: Epoch 038: loss 3.819 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.448 | clip 0.3\n",
            "INFO: Epoch 038: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.6\n",
            "INFO: Epoch 039: loss 3.792 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.392 | clip 0.3\n",
            "INFO: Epoch 039: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.9\n",
            "INFO: Epoch 040: loss 3.766 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.431 | clip 0.3\n",
            "INFO: Epoch 040: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.8\n",
            "INFO: Epoch 041: loss 3.736 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.291 | clip 0.25\n",
            "INFO: Epoch 041: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42\n",
            "INFO: Epoch 042: loss 3.712 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.199 | clip 0.25\n",
            "INFO: Epoch 042: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.1\n",
            "INFO: Epoch 043: loss 3.69 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.164 | clip 0.25\n",
            "INFO: Epoch 043: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.3\n",
            "INFO: Epoch 044: loss 3.665 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.101 | clip 0.3\n",
            "INFO: Epoch 044: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.4\n",
            "INFO: Epoch 045: loss 3.637 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.104 | clip 0.2\n",
            "INFO: Epoch 045: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.8\n",
            "INFO: Epoch 046: loss 3.616 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.241 | clip 0.25\n",
            "INFO: Epoch 046: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.7\n",
            "INFO: Epoch 047: loss 3.588 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.015 | clip 0.15\n",
            "INFO: Epoch 047: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.1\n",
            "INFO: Epoch 048: loss 3.564 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.132 | clip 0.15\n",
            "INFO: Epoch 048: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.4\n",
            "INFO: Epoch 049: loss 3.543 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.098 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.6\n",
            "INFO: Epoch 050: loss 3.514 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 2.983 | clip 0.15\n",
            "INFO: Epoch 050: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.9\n",
            "INFO: Epoch 051: loss 3.491 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.066 | clip 0.15\n",
            "INFO: Epoch 051: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 052: loss 3.471 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.123 | clip 0.15\n",
            "INFO: Epoch 052: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.6\n",
            "INFO: Epoch 053: loss 3.446 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.052 | clip 0.15\n",
            "INFO: Epoch 053: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9\n",
            "INFO: Epoch 054: loss 3.421 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.057 | clip 0.1\n",
            "INFO: Epoch 054: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.4\n",
            "INFO: Epoch 055: loss 3.403 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.203 | clip 0.2\n",
            "INFO: Epoch 055: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.7\n",
            "INFO: Epoch 056: loss 3.379 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.319 | clip 0.2\n",
            "INFO: Epoch 056: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.2\n",
            "INFO: Epoch 057: loss 3.356 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.209 | clip 0.2\n",
            "INFO: Epoch 057: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.8\n",
            "INFO: Epoch 058: loss 3.334 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.005 | clip 0.15\n",
            "INFO: Epoch 058: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.2\n",
            "INFO: Epoch 059: loss 3.318 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.254 | clip 0.25\n",
            "INFO: Epoch 059: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.7\n",
            "INFO: Epoch 060: loss 3.302 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.195 | clip 0.2\n",
            "INFO: Epoch 060: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.1\n",
            "INFO: Epoch 061: loss 3.28 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.175 | clip 0.2\n",
            "INFO: Epoch 061: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 062: loss 3.257 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.288 | clip 0.2\n",
            "INFO: Epoch 062: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.3\n",
            "INFO: Epoch 063: loss 3.24 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.14 | clip 0.1\n",
            "INFO: Epoch 063: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 064: loss 3.225 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.086 | clip 0.2\n",
            "INFO: Epoch 064: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.4\n",
            "INFO: Epoch 065: loss 3.199 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.22 | clip 0.2\n",
            "INFO: Epoch 065: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 066: loss 3.187 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.108 | clip 0.15\n",
            "INFO: Epoch 066: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 067: loss 3.163 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.304 | clip 0.2\n",
            "INFO: Epoch 067: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.4\n",
            "INFO: Epoch 068: loss 3.153 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.288 | clip 0.2\n",
            "INFO: Epoch 068: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 069: loss 3.129 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.193 | clip 0.1\n",
            "INFO: Epoch 069: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 070: loss 3.118 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.214 | clip 0.2\n",
            "INFO: Epoch 070: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 071: loss 3.098 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.556 | clip 0.1\n",
            "INFO: Epoch 071: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 072: loss 3.086 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.28 | clip 0.15\n",
            "INFO: Epoch 072: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.6\n",
            "INFO: Epoch 073: loss 3.066 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.33 | clip 0.15\n",
            "INFO: Epoch 073: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.3\n",
            "INFO: Epoch 074: loss 3.05 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.269 | clip 0.2\n",
            "INFO: Epoch 074: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 075: loss 3.031 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.208 | clip 0.2\n",
            "INFO: Epoch 075: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 076: loss 3.019 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.203 | clip 0.15\n",
            "INFO: Epoch 076: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.4\n",
            "INFO: Epoch 077: loss 2.996 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.47 | clip 0.15\n",
            "INFO: Epoch 077: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 078: loss 2.986 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.14 | clip 0.15\n",
            "INFO: Epoch 078: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.9\n",
            "INFO: Epoch 079: loss 2.967 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.616 | clip 0.15\n",
            "INFO: Epoch 079: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 080: loss 2.955 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.292 | clip 0.15\n",
            "INFO: Epoch 080: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.3\n",
            "INFO: Epoch 081: loss 2.934 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.297 | clip 0.15\n",
            "INFO: Epoch 081: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 082: loss 2.931 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.465 | clip 0.2\n",
            "INFO: Epoch 082: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 083: loss 2.909 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.324 | clip 0.15\n",
            "INFO: Epoch 083: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6\n",
            "INFO: Epoch 084: loss 2.895 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.42 | clip 0.2\n",
            "INFO: Epoch 084: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 085: loss 2.881 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.249 | clip 0.2\n",
            "INFO: Epoch 085: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 086: loss 2.868 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.39 | clip 0.2\n",
            "INFO: Epoch 086: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 087: loss 2.854 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.426 | clip 0.2\n",
            "INFO: Epoch 087: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 088: loss 2.839 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.447 | clip 0.2\n",
            "INFO: Epoch 088: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 089: loss 2.83 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.593 | clip 0.15\n",
            "INFO: Epoch 089: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 090: loss 2.819 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.474 | clip 0.15\n",
            "INFO: Epoch 090: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1\n",
            "INFO: Epoch 091: loss 2.798 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.467 | clip 0.1\n",
            "INFO: Epoch 091: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1\n",
            "INFO: Epoch 092: loss 2.797 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.782 | clip 0.3\n",
            "INFO: Epoch 092: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 093: loss 2.773 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.49 | clip 0.2\n",
            "INFO: Epoch 093: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 094: loss 2.77 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.733 | clip 0.35\n",
            "INFO: Epoch 094: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.3\n",
            "INFO: Epoch 095: loss 2.749 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.823 | clip 0.2\n",
            "INFO: Epoch 095: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 096: loss 2.742 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.767 | clip 0.3\n",
            "INFO: Epoch 096: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 097: loss 2.726 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.832 | clip 0.25\n",
            "INFO: Epoch 097: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 098: loss 2.734 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.513 | clip 0.45\n",
            "INFO: Epoch 098: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 099: loss 2.699 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.769 | clip 0.25\n",
            "INFO: Epoch 099: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 100: loss 2.708 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.561 | clip 0.55\n",
            "INFO: Epoch 100: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 101: loss 2.677 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.631 | clip 0.25\n",
            "INFO: Epoch 101: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19\n",
            "INFO: Epoch 102: loss 2.686 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.453 | clip 0.45\n",
            "INFO: Epoch 102: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 103: loss 2.656 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.691 | clip 0.4\n",
            "INFO: Epoch 103: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 104: loss 2.675 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.034 | clip 0.75\n",
            "INFO: Epoch 104: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 105: loss 2.637 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.977 | clip 0.35\n",
            "INFO: Epoch 105: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 106: loss 2.661 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.22 | clip 0.7\n",
            "INFO: Epoch 106: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 107: loss 2.622 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.464 | clip 0.35\n",
            "INFO: Epoch 107: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 108: loss 2.638 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.949 | clip 0.7\n",
            "INFO: Epoch 108: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 109: loss 2.603 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.525 | clip 0.4\n",
            "INFO: Epoch 109: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 110: loss 2.615 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.561 | clip 0.5\n",
            "INFO: Epoch 110: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 111: loss 2.581 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.511 | clip 0.35\n",
            "INFO: Epoch 111: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 112: loss 2.586 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.416 | clip 0.55\n",
            "INFO: Epoch 112: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 113: loss 2.566 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.734 | clip 0.45\n",
            "INFO: Epoch 113: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 114: loss 2.562 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.066 | clip 0.45\n",
            "INFO: Epoch 114: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 115: loss 2.537 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.376 | clip 0.3\n",
            "INFO: Epoch 115: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 116: loss 2.536 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.848 | clip 0.3\n",
            "INFO: Epoch 116: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 117: loss 2.51 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.07 | clip 0.3\n",
            "INFO: Epoch 117: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 118: loss 2.508 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.719 | clip 0.35\n",
            "INFO: Epoch 118: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 119: loss 2.493 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.414 | clip 0.3\n",
            "INFO: Epoch 119: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 120: loss 2.489 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.677 | clip 0.4\n",
            "INFO: Epoch 120: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 121: loss 2.473 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.446 | clip 0.35\n",
            "INFO: Epoch 121: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 122: loss 2.468 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.637 | clip 0.35\n",
            "INFO: Epoch 122: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 123: loss 2.449 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.24 | clip 0.3\n",
            "INFO: Epoch 123: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 124: loss 2.449 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.644 | clip 0.25\n",
            "INFO: Epoch 124: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 125: loss 2.429 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.945 | clip 0.3\n",
            "INFO: Epoch 125: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 126: loss 2.421 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.619 | clip 0.4\n",
            "INFO: Epoch 126: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 127: loss 2.405 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.115 | clip 0.3\n",
            "INFO: Epoch 127: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 128: loss 2.402 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.72 | clip 0.35\n",
            "INFO: Epoch 128: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 129: loss 2.385 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.953 | clip 0.3\n",
            "INFO: Epoch 129: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 130: loss 2.381 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.639 | clip 0.35\n",
            "INFO: Epoch 130: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 131: loss 2.367 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.421 | clip 0.3\n",
            "INFO: Epoch 131: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 132: loss 2.363 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.774 | clip 0.3\n",
            "INFO: Epoch 132: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 133: loss 2.348 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.786 | clip 0.35\n",
            "INFO: Epoch 133: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 134: loss 2.341 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.724 | clip 0.35\n",
            "INFO: Epoch 134: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 135: loss 2.328 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.083 | clip 0.35\n",
            "INFO: Epoch 135: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 136: loss 2.327 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.916 | clip 0.35\n",
            "INFO: Epoch 136: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 137: loss 2.311 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.135 | clip 0.3\n",
            "INFO: Epoch 137: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 138: loss 2.308 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.88 | clip 0.35\n",
            "INFO: Epoch 138: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 139: loss 2.289 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.75 | clip 0.3\n",
            "INFO: Epoch 139: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 140: loss 2.286 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.795 | clip 0.25\n",
            "INFO: Epoch 140: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 141: loss 2.27 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.736 | clip 0.2\n",
            "INFO: Epoch 141: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 142: loss 2.263 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.742 | clip 0.35\n",
            "INFO: Epoch 142: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 143: loss 2.253 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.941 | clip 0.25\n",
            "INFO: Epoch 143: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 144: loss 2.249 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.062 | clip 0.35\n",
            "INFO: Epoch 144: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 145: loss 2.234 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.803 | clip 0.3\n",
            "INFO: Epoch 145: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 146: loss 2.225 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.776 | clip 0.4\n",
            "INFO: Epoch 146: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 147: loss 2.219 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.288 | clip 0.4\n",
            "INFO: Epoch 147: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 148: loss 2.216 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.954 | clip 0.25\n",
            "INFO: Epoch 148: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 149: loss 2.196 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.723 | clip 0.25\n",
            "INFO: Epoch 149: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 150: loss 2.198 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.123 | clip 0.35\n",
            "INFO: Epoch 150: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 151: loss 2.182 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.704 | clip 0.3\n",
            "INFO: Epoch 151: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 152: loss 2.179 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.969 | clip 0.35\n",
            "INFO: Epoch 152: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 153: loss 2.168 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.738 | clip 0.35\n",
            "INFO: Epoch 153: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 154: loss 2.162 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.261 | clip 0.35\n",
            "INFO: Epoch 154: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 155: loss 2.154 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.779 | clip 0.35\n",
            "INFO: Epoch 155: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 156: loss 2.14 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.054 | clip 0.4\n",
            "INFO: Epoch 156: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 157: loss 2.141 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.221 | clip 0.5\n",
            "INFO: Epoch 157: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 158: loss 2.128 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.943 | clip 0.4\n",
            "INFO: Epoch 158: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 159: loss 2.124 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.991 | clip 0.4\n",
            "INFO: Epoch 159: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 160: loss 2.111 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.263 | clip 0.4\n",
            "INFO: Epoch 160: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 161: loss 2.107 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 3.908 | clip 0.35\n",
            "INFO: Epoch 161: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 162: loss 2.093 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.227 | clip 0.4\n",
            "INFO: Epoch 162: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 163: loss 2.095 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.104 | clip 0.5\n",
            "INFO: Epoch 163: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 164: loss 2.081 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.438 | clip 0.4\n",
            "INFO: Epoch 164: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 165: loss 2.079 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.067 | clip 0.45\n",
            "INFO: Epoch 165: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 166: loss 2.063 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.335 | clip 0.35\n",
            "INFO: Epoch 166: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 167: loss 2.069 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.288 | clip 0.4\n",
            "INFO: Epoch 167: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 168: loss 2.059 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.824 | clip 0.45\n",
            "INFO: Epoch 168: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 169: loss 2.06 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.431 | clip 0.5\n",
            "INFO: Epoch 169: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 170: loss 2.043 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.614 | clip 0.45\n",
            "INFO: Epoch 170: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 171: loss 2.044 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.496 | clip 0.5\n",
            "INFO: Epoch 171: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 172: loss 2.03 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.139 | clip 0.4\n",
            "INFO: Epoch 172: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 173: loss 2.026 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.114 | clip 0.55\n",
            "INFO: Epoch 173: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 174: loss 2.012 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.512 | clip 0.35\n",
            "INFO: Epoch 174: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 175: loss 2.014 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.506 | clip 0.5\n",
            "INFO: Epoch 175: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 176: loss 1.994 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.196 | clip 0.45\n",
            "INFO: Epoch 176: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 177: loss 1.997 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.169 | clip 0.45\n",
            "INFO: Epoch 177: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 178: loss 1.981 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.417 | clip 0.35\n",
            "INFO: Epoch 178: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 179: loss 1.986 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.108 | clip 0.45\n",
            "INFO: Epoch 179: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 180: loss 1.97 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.667 | clip 0.45\n",
            "INFO: Epoch 180: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 181: loss 1.972 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.182 | clip 0.45\n",
            "INFO: Epoch 181: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 182: loss 1.949 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.246 | clip 0.4\n",
            "INFO: Epoch 182: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 183: loss 1.956 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.197 | clip 0.4\n",
            "INFO: Epoch 183: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 184: loss 1.943 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.457 | clip 0.45\n",
            "INFO: Epoch 184: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 185: loss 1.947 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.37 | clip 0.45\n",
            "INFO: Epoch 185: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 186: loss 1.934 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.443 | clip 0.5\n",
            "INFO: Epoch 186: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 187: loss 1.929 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.278 | clip 0.45\n",
            "INFO: Epoch 187: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 188: loss 1.92 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.214 | clip 0.4\n",
            "INFO: Epoch 188: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 189: loss 1.918 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.267 | clip 0.45\n",
            "INFO: Epoch 189: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 190: loss 1.904 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.179 | clip 0.45\n",
            "INFO: Epoch 190: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 191: loss 1.904 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.122 | clip 0.55\n",
            "INFO: Epoch 191: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 192: loss 1.884 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.513 | clip 0.45\n",
            "INFO: Epoch 192: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 193: loss 1.89 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 4.257 | clip 0.45\n",
            "INFO: Epoch 193: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 194: loss 1.881 | lr 0.0005 | num_tokens 9.1 | batch_size 500 | grad_norm 5.195 | clip 0.5\n",
            "INFO: Epoch 194: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E15 bs500+lr0.0003"
      ],
      "metadata": {
        "id": "0vP6i_oKbW5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_15/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 500\\\n",
        "    --lr 0.0003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM6JDm2KbdGA",
        "outputId": "3676bf10-bc13-41b9-9376-75965c483eb8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_15/checkpoints --cuda --batch-size 500 --lr 0.0003\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 500, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_15/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.216 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 2.385 | clip 0.05\n",
            "INFO: Epoch 000: valid_loss 8.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.12e+03\n",
            "INFO: Epoch 001: loss 7.434 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 9.545 | clip 0.9\n",
            "INFO: Epoch 001: valid_loss 6.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 665\n",
            "INFO: Epoch 002: loss 6.088 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 11.19 | clip 1\n",
            "INFO: Epoch 002: valid_loss 5.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 277\n",
            "INFO: Epoch 003: loss 5.543 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 7.895 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 206\n",
            "INFO: Epoch 004: loss 5.384 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.784 | clip 0.65\n",
            "INFO: Epoch 004: valid_loss 5.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 188\n",
            "INFO: Epoch 005: loss 5.323 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.434 | clip 0.6\n",
            "INFO: Epoch 005: valid_loss 5.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 177\n",
            "INFO: Epoch 006: loss 5.263 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.129 | clip 0.75\n",
            "INFO: Epoch 006: valid_loss 5.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 168\n",
            "INFO: Epoch 007: loss 5.203 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 7.001 | clip 0.85\n",
            "INFO: Epoch 007: valid_loss 5.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 155\n",
            "INFO: Epoch 008: loss 5.133 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.987 | clip 0.9\n",
            "INFO: Epoch 008: valid_loss 4.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 145\n",
            "INFO: Epoch 009: loss 5.065 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.472 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 010: loss 5.014 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.348 | clip 0.7\n",
            "INFO: Epoch 010: valid_loss 4.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 131\n",
            "INFO: Epoch 011: loss 4.967 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.187 | clip 0.7\n",
            "INFO: Epoch 011: valid_loss 4.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 126\n",
            "INFO: Epoch 012: loss 4.921 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.942 | clip 0.7\n",
            "INFO: Epoch 012: valid_loss 4.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 121\n",
            "INFO: Epoch 013: loss 4.883 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.603 | clip 0.75\n",
            "INFO: Epoch 013: valid_loss 4.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 117\n",
            "INFO: Epoch 014: loss 4.845 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.417 | clip 0.75\n",
            "INFO: Epoch 014: valid_loss 4.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 114\n",
            "INFO: Epoch 015: loss 4.807 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.209 | clip 0.7\n",
            "INFO: Epoch 015: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 110\n",
            "INFO: Epoch 016: loss 4.77 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.834 | clip 0.55\n",
            "INFO: Epoch 016: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 107\n",
            "INFO: Epoch 017: loss 4.739 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.74 | clip 0.55\n",
            "INFO: Epoch 017: valid_loss 4.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 103\n",
            "INFO: Epoch 018: loss 4.707 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.639 | clip 0.45\n",
            "INFO: Epoch 018: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 019: loss 4.677 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.569 | clip 0.45\n",
            "INFO: Epoch 019: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.9\n",
            "INFO: Epoch 020: loss 4.654 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.707 | clip 0.45\n",
            "INFO: Epoch 020: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.8\n",
            "INFO: Epoch 021: loss 4.63 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.736 | clip 0.45\n",
            "INFO: Epoch 021: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 94.8\n",
            "INFO: Epoch 022: loss 4.605 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.83 | clip 0.5\n",
            "INFO: Epoch 022: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 92.7\n",
            "INFO: Epoch 023: loss 4.58 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.805 | clip 0.5\n",
            "INFO: Epoch 023: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90\n",
            "INFO: Epoch 024: loss 4.552 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.592 | clip 0.55\n",
            "INFO: Epoch 024: valid_loss 4.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 87.5\n",
            "INFO: Epoch 025: loss 4.521 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.365 | clip 0.45\n",
            "INFO: Epoch 025: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.3\n",
            "INFO: Epoch 026: loss 4.496 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.282 | clip 0.35\n",
            "INFO: Epoch 026: valid_loss 4.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 83.2\n",
            "INFO: Epoch 027: loss 4.466 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.201 | clip 0.3\n",
            "INFO: Epoch 027: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.4\n",
            "INFO: Epoch 028: loss 4.443 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.331 | clip 0.4\n",
            "INFO: Epoch 028: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.5\n",
            "INFO: Epoch 029: loss 4.418 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.338 | clip 0.45\n",
            "INFO: Epoch 029: valid_loss 4.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.9\n",
            "INFO: Epoch 030: loss 4.397 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.591 | clip 0.55\n",
            "INFO: Epoch 030: valid_loss 4.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.9\n",
            "INFO: Epoch 031: loss 4.373 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.717 | clip 0.55\n",
            "INFO: Epoch 031: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74\n",
            "INFO: Epoch 032: loss 4.35 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.698 | clip 0.6\n",
            "INFO: Epoch 032: valid_loss 4.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72\n",
            "INFO: Epoch 033: loss 4.318 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.561 | clip 0.5\n",
            "INFO: Epoch 033: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70\n",
            "INFO: Epoch 034: loss 4.293 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.34 | clip 0.45\n",
            "INFO: Epoch 034: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.1\n",
            "INFO: Epoch 035: loss 4.264 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.17 | clip 0.35\n",
            "INFO: Epoch 035: valid_loss 4.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.5\n",
            "INFO: Epoch 036: loss 4.239 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.027 | clip 0.3\n",
            "INFO: Epoch 036: valid_loss 4.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.9\n",
            "INFO: Epoch 037: loss 4.212 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.056 | clip 0.35\n",
            "INFO: Epoch 037: valid_loss 4.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.4\n",
            "INFO: Epoch 038: loss 4.188 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.966 | clip 0.35\n",
            "INFO: Epoch 038: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.1\n",
            "INFO: Epoch 039: loss 4.168 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.092 | clip 0.35\n",
            "INFO: Epoch 039: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.9\n",
            "INFO: Epoch 040: loss 4.145 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.036 | clip 0.35\n",
            "INFO: Epoch 040: valid_loss 4.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.6\n",
            "INFO: Epoch 041: loss 4.123 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.106 | clip 0.35\n",
            "INFO: Epoch 041: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.6\n",
            "INFO: Epoch 042: loss 4.104 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.121 | clip 0.35\n",
            "INFO: Epoch 042: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.3\n",
            "INFO: Epoch 043: loss 4.082 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.052 | clip 0.4\n",
            "INFO: Epoch 043: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.9\n",
            "INFO: Epoch 044: loss 4.059 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.833 | clip 0.35\n",
            "INFO: Epoch 044: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.7\n",
            "INFO: Epoch 045: loss 4.034 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.764 | clip 0.3\n",
            "INFO: Epoch 045: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.6\n",
            "INFO: Epoch 046: loss 4.016 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.741 | clip 0.35\n",
            "INFO: Epoch 046: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.7\n",
            "INFO: Epoch 047: loss 3.993 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.804 | clip 0.3\n",
            "INFO: Epoch 047: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.8\n",
            "INFO: Epoch 048: loss 3.979 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.842 | clip 0.35\n",
            "INFO: Epoch 048: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.2\n",
            "INFO: Epoch 049: loss 3.958 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.891 | clip 0.25\n",
            "INFO: Epoch 049: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.1\n",
            "INFO: Epoch 050: loss 3.942 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.761 | clip 0.3\n",
            "INFO: Epoch 050: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.5\n",
            "INFO: Epoch 051: loss 3.925 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.039 | clip 0.35\n",
            "INFO: Epoch 051: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.6\n",
            "INFO: Epoch 052: loss 3.913 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.844 | clip 0.4\n",
            "INFO: Epoch 052: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.1\n",
            "INFO: Epoch 053: loss 3.894 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.144 | clip 0.45\n",
            "INFO: Epoch 053: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.3\n",
            "INFO: Epoch 054: loss 3.876 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.864 | clip 0.45\n",
            "INFO: Epoch 054: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.7\n",
            "INFO: Epoch 055: loss 3.862 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.203 | clip 0.45\n",
            "INFO: Epoch 055: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46\n",
            "INFO: Epoch 056: loss 3.845 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.188 | clip 0.5\n",
            "INFO: Epoch 056: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.6\n",
            "INFO: Epoch 057: loss 3.831 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.023 | clip 0.5\n",
            "INFO: Epoch 057: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.9\n",
            "INFO: Epoch 058: loss 3.817 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.852 | clip 0.5\n",
            "INFO: Epoch 058: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.3\n",
            "INFO: Epoch 059: loss 3.803 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.935 | clip 0.45\n",
            "INFO: Epoch 059: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.8\n",
            "INFO: Epoch 060: loss 3.793 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.17 | clip 0.5\n",
            "INFO: Epoch 060: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.3\n",
            "INFO: Epoch 061: loss 3.776 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.89 | clip 0.45\n",
            "INFO: Epoch 061: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.7\n",
            "INFO: Epoch 062: loss 3.761 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.992 | clip 0.45\n",
            "INFO: Epoch 062: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.3\n",
            "INFO: Epoch 063: loss 3.753 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.025 | clip 0.35\n",
            "INFO: Epoch 063: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.9\n",
            "INFO: Epoch 064: loss 3.739 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.9 | clip 0.45\n",
            "INFO: Epoch 064: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.4\n",
            "INFO: Epoch 065: loss 3.723 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.127 | clip 0.45\n",
            "INFO: Epoch 065: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41\n",
            "INFO: Epoch 066: loss 3.713 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.874 | clip 0.4\n",
            "INFO: Epoch 066: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.7\n",
            "INFO: Epoch 067: loss 3.704 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.215 | clip 0.35\n",
            "INFO: Epoch 067: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.3\n",
            "INFO: Epoch 068: loss 3.691 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.961 | clip 0.35\n",
            "INFO: Epoch 068: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40\n",
            "INFO: Epoch 069: loss 3.682 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.038 | clip 0.35\n",
            "INFO: Epoch 069: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.5\n",
            "INFO: Epoch 070: loss 3.667 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.042 | clip 0.35\n",
            "INFO: Epoch 070: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.5\n",
            "INFO: Epoch 071: loss 3.661 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.387 | clip 0.45\n",
            "INFO: Epoch 071: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.8\n",
            "INFO: Epoch 072: loss 3.645 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.869 | clip 0.3\n",
            "INFO: Epoch 072: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.8\n",
            "INFO: Epoch 073: loss 3.639 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.181 | clip 0.45\n",
            "INFO: Epoch 073: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.1\n",
            "INFO: Epoch 074: loss 3.621 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.875 | clip 0.3\n",
            "INFO: Epoch 074: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.6\n",
            "INFO: Epoch 075: loss 3.624 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.434 | clip 0.45\n",
            "INFO: Epoch 075: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.5\n",
            "INFO: Epoch 076: loss 3.599 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.667 | clip 0.3\n",
            "INFO: Epoch 076: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38\n",
            "INFO: Epoch 077: loss 3.605 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.583 | clip 0.5\n",
            "INFO: Epoch 077: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.8\n",
            "INFO: Epoch 078: loss 3.579 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.727 | clip 0.25\n",
            "INFO: Epoch 078: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38\n",
            "INFO: Epoch 079: loss 3.596 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.594 | clip 0.7\n",
            "INFO: Epoch 079: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.4\n",
            "INFO: Epoch 080: loss 3.563 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.78 | clip 0.3\n",
            "INFO: Epoch 080: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.4\n",
            "INFO: Epoch 081: loss 3.576 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.481 | clip 0.7\n",
            "INFO: Epoch 081: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.9\n",
            "INFO: Epoch 082: loss 3.543 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.819 | clip 0.3\n",
            "INFO: Epoch 082: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.4\n",
            "INFO: Epoch 083: loss 3.55 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.605 | clip 0.55\n",
            "INFO: Epoch 083: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.5\n",
            "INFO: Epoch 084: loss 3.521 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.086 | clip 0.25\n",
            "INFO: Epoch 084: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.7\n",
            "INFO: Epoch 085: loss 3.525 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.186 | clip 0.45\n",
            "INFO: Epoch 085: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35\n",
            "INFO: Epoch 086: loss 3.506 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.253 | clip 0.35\n",
            "INFO: Epoch 086: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.2\n",
            "INFO: Epoch 087: loss 3.508 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.239 | clip 0.45\n",
            "INFO: Epoch 087: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.5\n",
            "INFO: Epoch 088: loss 3.488 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.314 | clip 0.3\n",
            "INFO: Epoch 088: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.6\n",
            "INFO: Epoch 089: loss 3.482 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.03 | clip 0.4\n",
            "INFO: Epoch 089: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34\n",
            "INFO: Epoch 090: loss 3.464 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.784 | clip 0.3\n",
            "INFO: Epoch 090: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 091: loss 3.468 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.234 | clip 0.45\n",
            "INFO: Epoch 091: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.4\n",
            "INFO: Epoch 092: loss 3.447 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.295 | clip 0.35\n",
            "INFO: Epoch 092: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.7\n",
            "INFO: Epoch 093: loss 3.448 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.259 | clip 0.45\n",
            "INFO: Epoch 093: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33\n",
            "INFO: Epoch 094: loss 3.423 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.712 | clip 0.25\n",
            "INFO: Epoch 094: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 095: loss 3.43 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.326 | clip 0.4\n",
            "INFO: Epoch 095: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.5\n",
            "INFO: Epoch 096: loss 3.406 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.246 | clip 0.35\n",
            "INFO: Epoch 096: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.8\n",
            "INFO: Epoch 097: loss 3.411 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.164 | clip 0.45\n",
            "INFO: Epoch 097: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32\n",
            "INFO: Epoch 098: loss 3.387 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.698 | clip 0.25\n",
            "INFO: Epoch 098: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.1\n",
            "INFO: Epoch 099: loss 3.389 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.156 | clip 0.45\n",
            "INFO: Epoch 099: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.6\n",
            "INFO: Epoch 100: loss 3.365 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.2 | clip 0.25\n",
            "INFO: Epoch 100: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.8\n",
            "INFO: Epoch 101: loss 3.374 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.537 | clip 0.5\n",
            "INFO: Epoch 101: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.1\n",
            "INFO: Epoch 102: loss 3.347 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.912 | clip 0.25\n",
            "INFO: Epoch 102: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.2\n",
            "INFO: Epoch 103: loss 3.346 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.048 | clip 0.4\n",
            "INFO: Epoch 103: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.5\n",
            "INFO: Epoch 104: loss 3.324 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.706 | clip 0.25\n",
            "INFO: Epoch 104: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.5\n",
            "INFO: Epoch 105: loss 3.321 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.748 | clip 0.35\n",
            "INFO: Epoch 105: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.1\n",
            "INFO: Epoch 106: loss 3.306 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.187 | clip 0.3\n",
            "INFO: Epoch 106: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30\n",
            "INFO: Epoch 107: loss 3.298 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.802 | clip 0.35\n",
            "INFO: Epoch 107: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.6\n",
            "INFO: Epoch 108: loss 3.281 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.912 | clip 0.25\n",
            "INFO: Epoch 108: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.6\n",
            "INFO: Epoch 109: loss 3.283 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.073 | clip 0.4\n",
            "INFO: Epoch 109: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29\n",
            "INFO: Epoch 110: loss 3.261 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.121 | clip 0.25\n",
            "INFO: Epoch 110: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.1\n",
            "INFO: Epoch 111: loss 3.261 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.152 | clip 0.45\n",
            "INFO: Epoch 111: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.5\n",
            "INFO: Epoch 112: loss 3.238 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.852 | clip 0.35\n",
            "INFO: Epoch 112: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.5\n",
            "INFO: Epoch 113: loss 3.241 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.215 | clip 0.45\n",
            "INFO: Epoch 113: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28\n",
            "INFO: Epoch 114: loss 3.22 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.899 | clip 0.3\n",
            "INFO: Epoch 114: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.1\n",
            "INFO: Epoch 115: loss 3.22 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.207 | clip 0.45\n",
            "INFO: Epoch 115: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.6\n",
            "INFO: Epoch 116: loss 3.198 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.037 | clip 0.25\n",
            "INFO: Epoch 116: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.7\n",
            "INFO: Epoch 117: loss 3.201 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.207 | clip 0.45\n",
            "INFO: Epoch 117: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 118: loss 3.176 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.783 | clip 0.3\n",
            "INFO: Epoch 118: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27\n",
            "INFO: Epoch 119: loss 3.174 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.899 | clip 0.4\n",
            "INFO: Epoch 119: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 120: loss 3.16 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.116 | clip 0.3\n",
            "INFO: Epoch 120: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 121: loss 3.157 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.832 | clip 0.35\n",
            "INFO: Epoch 121: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 122: loss 3.131 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.804 | clip 0.25\n",
            "INFO: Epoch 122: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 123: loss 3.135 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.899 | clip 0.35\n",
            "INFO: Epoch 123: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 124: loss 3.113 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.096 | clip 0.25\n",
            "INFO: Epoch 124: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.8\n",
            "INFO: Epoch 125: loss 3.116 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.099 | clip 0.45\n",
            "INFO: Epoch 125: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 126: loss 3.094 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.829 | clip 0.3\n",
            "INFO: Epoch 126: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 127: loss 3.09 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.775 | clip 0.35\n",
            "INFO: Epoch 127: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 128: loss 3.072 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.9 | clip 0.35\n",
            "INFO: Epoch 128: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 129: loss 3.07 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.949 | clip 0.4\n",
            "INFO: Epoch 129: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 130: loss 3.054 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.879 | clip 0.25\n",
            "INFO: Epoch 130: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.4\n",
            "INFO: Epoch 131: loss 3.054 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.892 | clip 0.35\n",
            "INFO: Epoch 131: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 132: loss 3.036 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.85 | clip 0.25\n",
            "INFO: Epoch 132: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 133: loss 3.032 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.947 | clip 0.4\n",
            "INFO: Epoch 133: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 134: loss 3.013 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.968 | clip 0.35\n",
            "INFO: Epoch 134: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 135: loss 3.017 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.036 | clip 0.4\n",
            "INFO: Epoch 135: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.4\n",
            "INFO: Epoch 136: loss 2.998 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.181 | clip 0.25\n",
            "INFO: Epoch 136: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 137: loss 2.996 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.735 | clip 0.35\n",
            "INFO: Epoch 137: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 138: loss 2.979 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.901 | clip 0.25\n",
            "INFO: Epoch 138: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.9\n",
            "INFO: Epoch 139: loss 2.969 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.59 | clip 0.25\n",
            "INFO: Epoch 139: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 140: loss 2.962 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.179 | clip 0.25\n",
            "INFO: Epoch 140: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 141: loss 2.955 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.696 | clip 0.3\n",
            "INFO: Epoch 141: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 142: loss 2.941 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.648 | clip 0.25\n",
            "INFO: Epoch 142: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.3\n",
            "INFO: Epoch 143: loss 2.937 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.633 | clip 0.3\n",
            "INFO: Epoch 143: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 144: loss 2.923 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.229 | clip 0.3\n",
            "INFO: Epoch 144: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 145: loss 2.921 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.682 | clip 0.35\n",
            "INFO: Epoch 145: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 146: loss 2.91 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.306 | clip 0.3\n",
            "INFO: Epoch 146: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 147: loss 2.907 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.835 | clip 0.35\n",
            "INFO: Epoch 147: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5\n",
            "INFO: Epoch 148: loss 2.889 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.796 | clip 0.25\n",
            "INFO: Epoch 148: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 149: loss 2.884 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.604 | clip 0.25\n",
            "INFO: Epoch 149: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 150: loss 2.87 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.974 | clip 0.3\n",
            "INFO: Epoch 150: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 151: loss 2.868 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.618 | clip 0.25\n",
            "INFO: Epoch 151: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 152: loss 2.857 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.789 | clip 0.25\n",
            "INFO: Epoch 152: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8\n",
            "INFO: Epoch 153: loss 2.854 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.615 | clip 0.35\n",
            "INFO: Epoch 153: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 154: loss 2.839 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.988 | clip 0.3\n",
            "INFO: Epoch 154: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 155: loss 2.835 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.749 | clip 0.35\n",
            "INFO: Epoch 155: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4\n",
            "INFO: Epoch 156: loss 2.819 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.167 | clip 0.25\n",
            "INFO: Epoch 156: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4\n",
            "INFO: Epoch 157: loss 2.823 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.925 | clip 0.35\n",
            "INFO: Epoch 157: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1\n",
            "INFO: Epoch 158: loss 2.807 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.467 | clip 0.3\n",
            "INFO: Epoch 158: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 159: loss 2.81 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.143 | clip 0.4\n",
            "INFO: Epoch 159: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 160: loss 2.788 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.995 | clip 0.25\n",
            "INFO: Epoch 160: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 161: loss 2.792 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.852 | clip 0.35\n",
            "INFO: Epoch 161: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 162: loss 2.776 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.864 | clip 0.25\n",
            "INFO: Epoch 162: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 163: loss 2.771 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.631 | clip 0.3\n",
            "INFO: Epoch 163: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 164: loss 2.76 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.079 | clip 0.25\n",
            "INFO: Epoch 164: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.3\n",
            "INFO: Epoch 165: loss 2.757 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.806 | clip 0.3\n",
            "INFO: Epoch 165: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 166: loss 2.745 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.96 | clip 0.3\n",
            "INFO: Epoch 166: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 167: loss 2.738 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.807 | clip 0.35\n",
            "INFO: Epoch 167: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 168: loss 2.726 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.091 | clip 0.35\n",
            "INFO: Epoch 168: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 169: loss 2.724 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.769 | clip 0.35\n",
            "INFO: Epoch 169: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 170: loss 2.71 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.421 | clip 0.25\n",
            "INFO: Epoch 170: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 171: loss 2.712 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.989 | clip 0.35\n",
            "INFO: Epoch 171: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 172: loss 2.697 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.193 | clip 0.3\n",
            "INFO: Epoch 172: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 173: loss 2.693 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.714 | clip 0.3\n",
            "INFO: Epoch 173: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 174: loss 2.682 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.343 | clip 0.35\n",
            "INFO: Epoch 174: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 175: loss 2.679 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.76 | clip 0.3\n",
            "INFO: Epoch 175: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 176: loss 2.666 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.265 | clip 0.3\n",
            "INFO: Epoch 176: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 177: loss 2.665 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.943 | clip 0.4\n",
            "INFO: Epoch 177: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 178: loss 2.652 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.463 | clip 0.3\n",
            "INFO: Epoch 178: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 179: loss 2.649 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.721 | clip 0.3\n",
            "INFO: Epoch 179: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 180: loss 2.636 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.039 | clip 0.35\n",
            "INFO: Epoch 180: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 181: loss 2.636 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.817 | clip 0.3\n",
            "INFO: Epoch 181: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 182: loss 2.618 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.767 | clip 0.25\n",
            "INFO: Epoch 182: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 183: loss 2.621 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.754 | clip 0.4\n",
            "INFO: Epoch 183: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 184: loss 2.607 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.869 | clip 0.25\n",
            "INFO: Epoch 184: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 185: loss 2.604 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.689 | clip 0.3\n",
            "INFO: Epoch 185: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 186: loss 2.593 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.777 | clip 0.3\n",
            "INFO: Epoch 186: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 187: loss 2.585 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.774 | clip 0.3\n",
            "INFO: Epoch 187: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 188: loss 2.579 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.751 | clip 0.3\n",
            "INFO: Epoch 188: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 189: loss 2.572 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.741 | clip 0.3\n",
            "INFO: Epoch 189: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 190: loss 2.569 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.996 | clip 0.4\n",
            "INFO: Epoch 190: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 191: loss 2.563 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.747 | clip 0.3\n",
            "INFO: Epoch 191: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 192: loss 2.549 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.937 | clip 0.35\n",
            "INFO: Epoch 192: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 193: loss 2.548 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.985 | clip 0.35\n",
            "INFO: Epoch 193: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 194: loss 2.538 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.497 | clip 0.4\n",
            "INFO: Epoch 194: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 195: loss 2.545 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.288 | clip 0.4\n",
            "INFO: Epoch 195: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 196: loss 2.525 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.956 | clip 0.35\n",
            "INFO: Epoch 196: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 197: loss 2.523 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.038 | clip 0.3\n",
            "INFO: Epoch 197: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 198: loss 2.513 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.002 | clip 0.4\n",
            "INFO: Epoch 198: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 199: loss 2.517 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.069 | clip 0.4\n",
            "INFO: Epoch 199: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 200: loss 2.501 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.661 | clip 0.4\n",
            "INFO: Epoch 200: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 201: loss 2.508 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.241 | clip 0.5\n",
            "INFO: Epoch 201: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 202: loss 2.486 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.148 | clip 0.4\n",
            "INFO: Epoch 202: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 203: loss 2.487 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.065 | clip 0.4\n",
            "INFO: Epoch 203: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 204: loss 2.478 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.738 | clip 0.4\n",
            "INFO: Epoch 204: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 205: loss 2.475 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.01 | clip 0.4\n",
            "INFO: Epoch 205: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 206: loss 2.464 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.566 | clip 0.4\n",
            "INFO: Epoch 206: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 207: loss 2.462 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.16 | clip 0.45\n",
            "INFO: Epoch 207: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 208: loss 2.452 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.86 | clip 0.35\n",
            "INFO: Epoch 208: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 209: loss 2.455 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.272 | clip 0.45\n",
            "INFO: Epoch 209: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 210: loss 2.436 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.547 | clip 0.3\n",
            "INFO: Epoch 210: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 211: loss 2.437 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.902 | clip 0.4\n",
            "INFO: Epoch 211: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 212: loss 2.422 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.409 | clip 0.4\n",
            "INFO: Epoch 212: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 213: loss 2.422 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.902 | clip 0.35\n",
            "INFO: Epoch 213: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 214: loss 2.413 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.292 | clip 0.45\n",
            "INFO: Epoch 214: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 215: loss 2.408 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.081 | clip 0.4\n",
            "INFO: Epoch 215: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 216: loss 2.402 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.927 | clip 0.4\n",
            "INFO: Epoch 216: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 217: loss 2.398 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.06 | clip 0.45\n",
            "INFO: Epoch 217: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 218: loss 2.388 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.57 | clip 0.35\n",
            "INFO: Epoch 218: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 219: loss 2.39 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.153 | clip 0.4\n",
            "INFO: Epoch 219: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 220: loss 2.378 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.415 | clip 0.4\n",
            "INFO: Epoch 220: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 221: loss 2.374 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.048 | clip 0.45\n",
            "INFO: Epoch 221: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 222: loss 2.364 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.397 | clip 0.4\n",
            "INFO: Epoch 222: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 223: loss 2.367 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.128 | clip 0.5\n",
            "INFO: Epoch 223: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 224: loss 2.357 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.528 | clip 0.4\n",
            "INFO: Epoch 224: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 225: loss 2.351 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.049 | clip 0.4\n",
            "INFO: Epoch 225: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 226: loss 2.341 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.013 | clip 0.4\n",
            "INFO: Epoch 226: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 227: loss 2.335 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.888 | clip 0.45\n",
            "INFO: Epoch 227: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 228: loss 2.33 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.962 | clip 0.4\n",
            "INFO: Epoch 228: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 229: loss 2.322 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.047 | clip 0.3\n",
            "INFO: Epoch 229: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 230: loss 2.319 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.914 | clip 0.3\n",
            "INFO: Epoch 230: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 231: loss 2.317 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.208 | clip 0.45\n",
            "INFO: Epoch 231: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 232: loss 2.304 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.788 | clip 0.35\n",
            "INFO: Epoch 232: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 233: loss 2.3 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.149 | clip 0.4\n",
            "INFO: Epoch 233: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 234: loss 2.296 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.83 | clip 0.35\n",
            "INFO: Epoch 234: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 235: loss 2.293 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.33 | clip 0.4\n",
            "INFO: Epoch 235: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 236: loss 2.294 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.06 | clip 0.4\n",
            "INFO: Epoch 236: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 237: loss 2.279 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.26 | clip 0.35\n",
            "INFO: Epoch 237: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 238: loss 2.282 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.194 | clip 0.45\n",
            "INFO: Epoch 238: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 239: loss 2.269 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.755 | clip 0.4\n",
            "INFO: Epoch 239: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 240: loss 2.266 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.065 | clip 0.45\n",
            "INFO: Epoch 240: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 241: loss 2.258 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.213 | clip 0.35\n",
            "INFO: Epoch 241: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 242: loss 2.258 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.101 | clip 0.45\n",
            "INFO: Epoch 242: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 243: loss 2.245 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.423 | clip 0.4\n",
            "INFO: Epoch 243: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 244: loss 2.248 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.399 | clip 0.5\n",
            "INFO: Epoch 244: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 245: loss 2.239 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.658 | clip 0.4\n",
            "INFO: Epoch 245: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 246: loss 2.24 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.496 | clip 0.5\n",
            "INFO: Epoch 246: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 247: loss 2.227 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.373 | clip 0.4\n",
            "INFO: Epoch 247: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 248: loss 2.229 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.552 | clip 0.5\n",
            "INFO: Epoch 248: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 249: loss 2.217 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.587 | clip 0.4\n",
            "INFO: Epoch 249: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 250: loss 2.214 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.52 | clip 0.55\n",
            "INFO: Epoch 250: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 251: loss 2.209 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.124 | clip 0.45\n",
            "INFO: Epoch 251: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 252: loss 2.207 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.196 | clip 0.5\n",
            "INFO: Epoch 252: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 253: loss 2.196 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.121 | clip 0.4\n",
            "INFO: Epoch 253: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 254: loss 2.193 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.294 | clip 0.5\n",
            "INFO: Epoch 254: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 255: loss 2.187 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.157 | clip 0.4\n",
            "INFO: Epoch 255: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 256: loss 2.186 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.095 | clip 0.4\n",
            "INFO: Epoch 256: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 257: loss 2.176 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.191 | clip 0.45\n",
            "INFO: Epoch 257: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 258: loss 2.177 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.131 | clip 0.45\n",
            "INFO: Epoch 258: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 259: loss 2.166 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.146 | clip 0.45\n",
            "INFO: Epoch 259: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 260: loss 2.16 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.988 | clip 0.4\n",
            "INFO: Epoch 260: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 261: loss 2.156 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.256 | clip 0.4\n",
            "INFO: Epoch 261: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 262: loss 2.154 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.363 | clip 0.45\n",
            "INFO: Epoch 262: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 263: loss 2.144 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.142 | clip 0.4\n",
            "INFO: Epoch 263: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 264: loss 2.148 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.11 | clip 0.45\n",
            "INFO: Epoch 264: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 265: loss 2.138 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.196 | clip 0.5\n",
            "INFO: Epoch 265: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 266: loss 2.125 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.555 | clip 0.5\n",
            "INFO: Epoch 266: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 267: loss 2.129 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.215 | clip 0.5\n",
            "INFO: Epoch 267: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 268: loss 2.123 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.378 | clip 0.5\n",
            "INFO: Epoch 268: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 269: loss 2.117 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.216 | clip 0.45\n",
            "INFO: Epoch 269: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 270: loss 2.112 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.495 | clip 0.45\n",
            "INFO: Epoch 270: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 271: loss 2.11 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.291 | clip 0.5\n",
            "INFO: Epoch 271: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 272: loss 2.103 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.679 | clip 0.45\n",
            "INFO: Epoch 272: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 273: loss 2.107 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.586 | clip 0.5\n",
            "INFO: Epoch 273: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 274: loss 2.097 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.923 | clip 0.45\n",
            "INFO: Epoch 274: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 275: loss 2.097 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.86 | clip 0.55\n",
            "INFO: Epoch 275: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 276: loss 2.085 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.676 | clip 0.45\n",
            "INFO: Epoch 276: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 277: loss 2.089 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.727 | clip 0.55\n",
            "INFO: Epoch 277: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 278: loss 2.075 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.795 | clip 0.4\n",
            "INFO: Epoch 278: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 279: loss 2.076 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.482 | clip 0.55\n",
            "INFO: Epoch 279: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 280: loss 2.064 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.441 | clip 0.45\n",
            "INFO: Epoch 280: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 281: loss 2.066 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.734 | clip 0.6\n",
            "INFO: Epoch 281: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 282: loss 2.06 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.276 | clip 0.45\n",
            "INFO: Epoch 282: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 283: loss 2.059 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.977 | clip 0.5\n",
            "INFO: Epoch 283: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 284: loss 2.05 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.236 | clip 0.5\n",
            "INFO: Epoch 284: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 285: loss 2.045 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.51 | clip 0.5\n",
            "INFO: Epoch 285: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 286: loss 2.04 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.384 | clip 0.45\n",
            "INFO: Epoch 286: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 287: loss 2.035 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.92 | clip 0.5\n",
            "INFO: Epoch 287: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 288: loss 2.033 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.586 | clip 0.45\n",
            "INFO: Epoch 288: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 289: loss 2.026 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.629 | clip 0.45\n",
            "INFO: Epoch 289: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 290: loss 2.026 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.297 | clip 0.55\n",
            "INFO: Epoch 290: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 291: loss 2.015 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.593 | clip 0.4\n",
            "INFO: Epoch 291: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 292: loss 2.019 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.35 | clip 0.55\n",
            "INFO: Epoch 292: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 293: loss 2.012 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.615 | clip 0.5\n",
            "INFO: Epoch 293: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 294: loss 2.009 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.44 | clip 0.5\n",
            "INFO: Epoch 294: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 295: loss 1.998 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.627 | clip 0.4\n",
            "INFO: Epoch 295: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 296: loss 1.997 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.447 | clip 0.6\n",
            "INFO: Epoch 296: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 297: loss 1.992 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.431 | clip 0.5\n",
            "INFO: Epoch 297: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 298: loss 1.991 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.413 | clip 0.6\n",
            "INFO: Epoch 298: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 299: loss 1.984 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.264 | clip 0.45\n",
            "INFO: Epoch 299: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 300: loss 1.986 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.473 | clip 0.5\n",
            "INFO: Epoch 300: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 301: loss 1.969 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.482 | clip 0.5\n",
            "INFO: Epoch 301: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 302: loss 1.973 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.491 | clip 0.55\n",
            "INFO: Epoch 302: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 303: loss 1.961 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.249 | clip 0.45\n",
            "INFO: Epoch 303: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 304: loss 1.962 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.548 | clip 0.5\n",
            "INFO: Epoch 304: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 305: loss 1.96 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.341 | clip 0.4\n",
            "INFO: Epoch 305: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 306: loss 1.955 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.328 | clip 0.45\n",
            "INFO: Epoch 306: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 307: loss 1.953 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.682 | clip 0.55\n",
            "INFO: Epoch 307: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 308: loss 1.947 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.149 | clip 0.45\n",
            "INFO: Epoch 308: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 309: loss 1.945 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.226 | clip 0.45\n",
            "INFO: Epoch 309: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 310: loss 1.939 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.268 | clip 0.5\n",
            "INFO: Epoch 310: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 311: loss 1.93 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.344 | clip 0.5\n",
            "INFO: Epoch 311: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 312: loss 1.927 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.211 | clip 0.5\n",
            "INFO: Epoch 312: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 313: loss 1.927 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.072 | clip 0.5\n",
            "INFO: Epoch 313: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 314: loss 1.927 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.445 | clip 0.6\n",
            "INFO: Epoch 314: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 315: loss 1.921 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.766 | clip 0.5\n",
            "INFO: Epoch 315: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 316: loss 1.919 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.512 | clip 0.45\n",
            "INFO: Epoch 316: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 317: loss 1.91 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.553 | clip 0.5\n",
            "INFO: Epoch 317: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 318: loss 1.915 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.089 | clip 0.55\n",
            "INFO: Epoch 318: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 319: loss 1.905 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.687 | clip 0.5\n",
            "INFO: Epoch 319: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 320: loss 1.901 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.973 | clip 0.65\n",
            "INFO: Epoch 320: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 321: loss 1.891 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.402 | clip 0.45\n",
            "INFO: Epoch 321: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 322: loss 1.89 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.763 | clip 0.55\n",
            "INFO: Epoch 322: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 323: loss 1.887 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.336 | clip 0.5\n",
            "INFO: Epoch 323: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 324: loss 1.877 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.353 | clip 0.5\n",
            "INFO: Epoch 324: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 325: loss 1.877 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.596 | clip 0.55\n",
            "INFO: Epoch 325: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 326: loss 1.875 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.72 | clip 0.55\n",
            "INFO: Epoch 326: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 327: loss 1.872 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.577 | clip 0.55\n",
            "INFO: Epoch 327: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 328: loss 1.865 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.303 | clip 0.5\n",
            "INFO: Epoch 328: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 329: loss 1.86 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.829 | clip 0.45\n",
            "INFO: Epoch 329: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 330: loss 1.863 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.496 | clip 0.5\n",
            "INFO: Epoch 330: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 331: loss 1.858 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.758 | clip 0.55\n",
            "INFO: Epoch 331: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 332: loss 1.851 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.441 | clip 0.45\n",
            "INFO: Epoch 332: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 333: loss 1.85 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.899 | clip 0.5\n",
            "INFO: Epoch 333: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 334: loss 1.851 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.712 | clip 0.6\n",
            "INFO: Epoch 334: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 335: loss 1.841 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.148 | clip 0.5\n",
            "INFO: Epoch 335: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 336: loss 1.843 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.658 | clip 0.6\n",
            "INFO: Epoch 336: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 337: loss 1.832 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.522 | clip 0.5\n",
            "INFO: Epoch 337: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 338: loss 1.834 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.661 | clip 0.55\n",
            "INFO: Epoch 338: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 339: loss 1.825 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.593 | clip 0.5\n",
            "INFO: Epoch 339: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 340: loss 1.828 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.549 | clip 0.5\n",
            "INFO: Epoch 340: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 341: loss 1.821 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.49 | clip 0.45\n",
            "INFO: Epoch 341: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 342: loss 1.814 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.304 | clip 0.6\n",
            "INFO: Epoch 342: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 343: loss 1.816 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.548 | clip 0.55\n",
            "INFO: Epoch 343: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 344: loss 1.812 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.649 | clip 0.5\n",
            "INFO: Epoch 344: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 345: loss 1.806 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.451 | clip 0.55\n",
            "INFO: Epoch 345: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 346: loss 1.803 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.796 | clip 0.45\n",
            "INFO: Epoch 346: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 347: loss 1.795 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.487 | clip 0.55\n",
            "INFO: Epoch 347: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 348: loss 1.794 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.902 | clip 0.5\n",
            "INFO: Epoch 348: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 349: loss 1.797 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.654 | clip 0.55\n",
            "INFO: Epoch 349: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 350: loss 1.788 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.142 | clip 0.6\n",
            "INFO: Epoch 350: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 351: loss 1.786 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.642 | clip 0.55\n",
            "INFO: Epoch 351: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 352: loss 1.78 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.646 | clip 0.5\n",
            "INFO: Epoch 352: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 353: loss 1.783 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.956 | clip 0.6\n",
            "INFO: Epoch 353: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 354: loss 1.777 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.813 | clip 0.55\n",
            "INFO: Epoch 354: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 355: loss 1.781 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.852 | clip 0.65\n",
            "INFO: Epoch 355: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 356: loss 1.77 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.791 | clip 0.55\n",
            "INFO: Epoch 356: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 357: loss 1.765 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.816 | clip 0.55\n",
            "INFO: Epoch 357: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 358: loss 1.764 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.535 | clip 0.5\n",
            "INFO: Epoch 358: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 359: loss 1.756 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.883 | clip 0.5\n",
            "INFO: Epoch 359: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 360: loss 1.753 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.62 | clip 0.55\n",
            "INFO: Epoch 360: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 361: loss 1.748 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.862 | clip 0.55\n",
            "INFO: Epoch 361: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 362: loss 1.751 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.111 | clip 0.65\n",
            "INFO: Epoch 362: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 363: loss 1.746 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.591 | clip 0.55\n",
            "INFO: Epoch 363: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 364: loss 1.742 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.956 | clip 0.5\n",
            "INFO: Epoch 364: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 365: loss 1.735 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.544 | clip 0.55\n",
            "INFO: Epoch 365: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 366: loss 1.733 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.733 | clip 0.5\n",
            "INFO: Epoch 366: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 367: loss 1.734 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.017 | clip 0.65\n",
            "INFO: Epoch 367: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 368: loss 1.729 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.803 | clip 0.5\n",
            "INFO: Epoch 368: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 369: loss 1.732 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.646 | clip 0.65\n",
            "INFO: Epoch 369: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 370: loss 1.724 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.661 | clip 0.5\n",
            "INFO: Epoch 370: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 371: loss 1.72 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.65 | clip 0.5\n",
            "INFO: Epoch 371: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 372: loss 1.711 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.604 | clip 0.5\n",
            "INFO: Epoch 372: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 373: loss 1.715 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.637 | clip 0.55\n",
            "INFO: Epoch 373: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 374: loss 1.705 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.57 | clip 0.55\n",
            "INFO: Epoch 374: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 375: loss 1.702 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.641 | clip 0.5\n",
            "INFO: Epoch 375: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 376: loss 1.699 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.621 | clip 0.55\n",
            "INFO: Epoch 376: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 377: loss 1.698 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.632 | clip 0.55\n",
            "INFO: Epoch 377: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 378: loss 1.697 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.66 | clip 0.5\n",
            "INFO: Epoch 378: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 379: loss 1.689 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.681 | clip 0.45\n",
            "INFO: Epoch 379: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 380: loss 1.691 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.734 | clip 0.5\n",
            "INFO: Epoch 380: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 381: loss 1.689 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.574 | clip 0.5\n",
            "INFO: Epoch 381: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 382: loss 1.684 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.795 | clip 0.6\n",
            "INFO: Epoch 382: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 383: loss 1.676 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.547 | clip 0.45\n",
            "INFO: Epoch 383: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 384: loss 1.677 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.601 | clip 0.45\n",
            "INFO: Epoch 384: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 385: loss 1.681 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.76 | clip 0.55\n",
            "INFO: Epoch 385: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 386: loss 1.672 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.674 | clip 0.5\n",
            "INFO: Epoch 386: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 387: loss 1.671 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.762 | clip 0.55\n",
            "INFO: Epoch 387: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 388: loss 1.664 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.742 | clip 0.55\n",
            "INFO: Epoch 388: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E16 bs1000+lr0.0005+dropout0.5"
      ],
      "metadata": {
        "id": "w0Z-l_POcepD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_16/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.0005\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGdsDltEckqy",
        "outputId": "8192b419-a499-4ddb-c44a-8a7cb11b0438"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_16/checkpoints --cuda --batch-size 1000 --lr 0.0005\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_16/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.5, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.5, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.238 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.123 | clip 0\n",
            "INFO: Epoch 000: valid_loss 8.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.4e+03\n",
            "INFO: Epoch 001: loss 7.905 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.292 | clip 0.6\n",
            "INFO: Epoch 001: valid_loss 7.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 1.45e+03\n",
            "INFO: Epoch 002: loss 6.83 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.36 | clip 1\n",
            "INFO: Epoch 002: valid_loss 6.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 458\n",
            "INFO: Epoch 003: loss 6.005 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.57 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 262\n",
            "INFO: Epoch 004: loss 5.638 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.099 | clip 1\n",
            "INFO: Epoch 004: valid_loss 5.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 208\n",
            "INFO: Epoch 005: loss 5.498 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.141 | clip 0.6\n",
            "INFO: Epoch 005: valid_loss 5.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 189\n",
            "INFO: Epoch 006: loss 5.426 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.262 | clip 0.5\n",
            "INFO: Epoch 006: valid_loss 5.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 176\n",
            "INFO: Epoch 007: loss 5.366 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.018 | clip 0.8\n",
            "INFO: Epoch 007: valid_loss 5.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 166\n",
            "INFO: Epoch 008: loss 5.297 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.171 | clip 0.8\n",
            "INFO: Epoch 008: valid_loss 5.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 156\n",
            "INFO: Epoch 009: loss 5.238 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.582 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 148\n",
            "INFO: Epoch 010: loss 5.179 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.834 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 4.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 141\n",
            "INFO: Epoch 011: loss 5.122 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.393 | clip 0.7\n",
            "INFO: Epoch 011: valid_loss 4.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 135\n",
            "INFO: Epoch 012: loss 5.076 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.426 | clip 0.7\n",
            "INFO: Epoch 012: valid_loss 4.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 129\n",
            "INFO: Epoch 013: loss 5.026 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.957 | clip 0.6\n",
            "INFO: Epoch 013: valid_loss 4.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 124\n",
            "INFO: Epoch 014: loss 4.982 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.018 | clip 0.7\n",
            "INFO: Epoch 014: valid_loss 4.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 120\n",
            "INFO: Epoch 015: loss 4.937 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.568 | clip 0.6\n",
            "INFO: Epoch 015: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116\n",
            "INFO: Epoch 016: loss 4.901 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.481 | clip 0.6\n",
            "INFO: Epoch 016: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112\n",
            "INFO: Epoch 017: loss 4.865 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.341 | clip 0.6\n",
            "INFO: Epoch 017: valid_loss 4.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 109\n",
            "INFO: Epoch 018: loss 4.831 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.288 | clip 0.6\n",
            "INFO: Epoch 018: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106\n",
            "INFO: Epoch 019: loss 4.8 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.963 | clip 0.5\n",
            "INFO: Epoch 019: valid_loss 4.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 103\n",
            "INFO: Epoch 020: loss 4.772 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.043 | clip 0.6\n",
            "INFO: Epoch 020: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 021: loss 4.742 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.65 | clip 0.4\n",
            "INFO: Epoch 021: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.4\n",
            "INFO: Epoch 022: loss 4.719 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.6 | clip 0.5\n",
            "INFO: Epoch 022: valid_loss 4.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96\n",
            "INFO: Epoch 023: loss 4.693 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.401 | clip 0.4\n",
            "INFO: Epoch 023: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.8\n",
            "INFO: Epoch 024: loss 4.669 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.366 | clip 0.4\n",
            "INFO: Epoch 024: valid_loss 4.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.7\n",
            "INFO: Epoch 025: loss 4.642 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.124 | clip 0.4\n",
            "INFO: Epoch 025: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.8\n",
            "INFO: Epoch 026: loss 4.621 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.236 | clip 0.4\n",
            "INFO: Epoch 026: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.1\n",
            "INFO: Epoch 027: loss 4.597 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.421 | clip 0.4\n",
            "INFO: Epoch 027: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.1\n",
            "INFO: Epoch 028: loss 4.573 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.172 | clip 0.4\n",
            "INFO: Epoch 028: valid_loss 4.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.3\n",
            "INFO: Epoch 029: loss 4.546 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.857 | clip 0.3\n",
            "INFO: Epoch 029: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.4\n",
            "INFO: Epoch 030: loss 4.522 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.064 | clip 0.3\n",
            "INFO: Epoch 030: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.9\n",
            "INFO: Epoch 031: loss 4.507 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.63 | clip 0.5\n",
            "INFO: Epoch 031: valid_loss 4.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 78.9\n",
            "INFO: Epoch 032: loss 4.479 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.192 | clip 0.4\n",
            "INFO: Epoch 032: valid_loss 4.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.2\n",
            "INFO: Epoch 033: loss 4.453 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.812 | clip 0.3\n",
            "INFO: Epoch 033: valid_loss 4.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.6\n",
            "INFO: Epoch 034: loss 4.427 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.779 | clip 0.3\n",
            "INFO: Epoch 034: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.6\n",
            "INFO: Epoch 035: loss 4.409 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.574 | clip 0.5\n",
            "INFO: Epoch 035: valid_loss 4.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72.1\n",
            "INFO: Epoch 036: loss 4.399 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.246 | clip 0.7\n",
            "INFO: Epoch 036: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.3\n",
            "INFO: Epoch 037: loss 4.358 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.529 | clip 0.2\n",
            "INFO: Epoch 037: valid_loss 4.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.7\n",
            "INFO: Epoch 038: loss 4.336 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.558 | clip 0.2\n",
            "INFO: Epoch 038: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.9\n",
            "INFO: Epoch 039: loss 4.334 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.338 | clip 0.6\n",
            "INFO: Epoch 039: valid_loss 4.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.4\n",
            "INFO: Epoch 040: loss 4.297 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.951 | clip 0.4\n",
            "INFO: Epoch 040: valid_loss 4.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.3\n",
            "INFO: Epoch 041: loss 4.273 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.046 | clip 0.2\n",
            "INFO: Epoch 041: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63.8\n",
            "INFO: Epoch 042: loss 4.251 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.392 | clip 0.2\n",
            "INFO: Epoch 042: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.4\n",
            "INFO: Epoch 043: loss 4.24 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.672 | clip 0.5\n",
            "INFO: Epoch 043: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61\n",
            "INFO: Epoch 044: loss 4.215 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.582 | clip 0.3\n",
            "INFO: Epoch 044: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.5\n",
            "INFO: Epoch 045: loss 4.183 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.881 | clip 0.2\n",
            "INFO: Epoch 045: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.7\n",
            "INFO: Epoch 046: loss 4.167 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.383 | clip 0.2\n",
            "INFO: Epoch 046: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.6\n",
            "INFO: Epoch 047: loss 4.171 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.069 | clip 0.7\n",
            "INFO: Epoch 047: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.6\n",
            "INFO: Epoch 048: loss 4.13 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.963 | clip 0.2\n",
            "INFO: Epoch 048: valid_loss 4.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.1\n",
            "INFO: Epoch 049: loss 4.11 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.968 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.6\n",
            "INFO: Epoch 050: loss 4.105 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.382 | clip 0.6\n",
            "INFO: Epoch 050: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.5\n",
            "INFO: Epoch 051: loss 4.093 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.658 | clip 0.8\n",
            "INFO: Epoch 051: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.6\n",
            "INFO: Epoch 052: loss 4.058 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.74 | clip 0.2\n",
            "INFO: Epoch 052: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.1\n",
            "INFO: Epoch 053: loss 4.048 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.84 | clip 0.5\n",
            "INFO: Epoch 053: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.3\n",
            "INFO: Epoch 054: loss 4.084 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.592 | clip 0.8\n",
            "INFO: Epoch 054: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.6\n",
            "INFO: Epoch 055: loss 4.021 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.699 | clip 0.2\n",
            "INFO: Epoch 055: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.1\n",
            "INFO: Epoch 056: loss 4.021 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.912 | clip 0.5\n",
            "INFO: Epoch 056: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.6\n",
            "INFO: Epoch 057: loss 3.985 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.904 | clip 0.2\n",
            "INFO: Epoch 057: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.6\n",
            "INFO: Epoch 058: loss 3.966 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.829 | clip 0.2\n",
            "INFO: Epoch 058: valid_loss 3.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.5\n",
            "INFO: Epoch 059: loss 3.956 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.207 | clip 0.2\n",
            "INFO: Epoch 059: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47\n",
            "INFO: Epoch 060: loss 3.935 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.681 | clip 0.2\n",
            "INFO: Epoch 060: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.6\n",
            "INFO: Epoch 061: loss 3.916 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.561 | clip 0.2\n",
            "INFO: Epoch 061: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.6\n",
            "INFO: Epoch 062: loss 3.907 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.187 | clip 0.3\n",
            "INFO: Epoch 062: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.2\n",
            "INFO: Epoch 063: loss 3.9 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.201 | clip 0.5\n",
            "INFO: Epoch 063: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.6\n",
            "INFO: Epoch 064: loss 3.878 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.657 | clip 0.1\n",
            "INFO: Epoch 064: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.8\n",
            "INFO: Epoch 065: loss 3.864 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.05 | clip 0.1\n",
            "INFO: Epoch 065: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.7\n",
            "INFO: Epoch 066: loss 3.859 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.266 | clip 0.6\n",
            "INFO: Epoch 066: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.7\n",
            "INFO: Epoch 067: loss 3.877 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.707 | clip 0.8\n",
            "INFO: Epoch 067: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.8\n",
            "INFO: Epoch 068: loss 3.838 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.762 | clip 0.2\n",
            "INFO: Epoch 068: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.5\n",
            "INFO: Epoch 069: loss 3.832 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.729 | clip 0.5\n",
            "INFO: Epoch 069: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.3\n",
            "INFO: Epoch 070: loss 3.824 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.904 | clip 0.4\n",
            "INFO: Epoch 070: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.3\n",
            "INFO: Epoch 071: loss 3.796 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.662 | clip 0.1\n",
            "INFO: Epoch 071: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.3\n",
            "INFO: Epoch 072: loss 3.782 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.084 | clip 0.2\n",
            "INFO: Epoch 072: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.4\n",
            "INFO: Epoch 073: loss 3.802 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.065 | clip 0.8\n",
            "INFO: Epoch 073: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.3\n",
            "INFO: Epoch 074: loss 3.762 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.713 | clip 0.1\n",
            "INFO: Epoch 074: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.2\n",
            "INFO: Epoch 075: loss 3.755 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.78 | clip 0.6\n",
            "INFO: Epoch 075: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.3\n",
            "INFO: Epoch 076: loss 3.782 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.653 | clip 0.9\n",
            "INFO: Epoch 076: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.1\n",
            "INFO: Epoch 077: loss 3.742 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.507 | clip 0.2\n",
            "INFO: Epoch 077: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.4\n",
            "INFO: Epoch 078: loss 3.741 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.019 | clip 0.5\n",
            "INFO: Epoch 078: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.1\n",
            "INFO: Epoch 079: loss 3.719 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.861 | clip 0.2\n",
            "INFO: Epoch 079: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.7\n",
            "INFO: Epoch 080: loss 3.7 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.31 | clip 0\n",
            "INFO: Epoch 080: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.1\n",
            "INFO: Epoch 081: loss 3.691 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.691 | clip 0.2\n",
            "INFO: Epoch 081: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.8\n",
            "INFO: Epoch 082: loss 3.682 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.843 | clip 0.2\n",
            "INFO: Epoch 082: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.5\n",
            "INFO: Epoch 083: loss 3.667 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.243 | clip 0.1\n",
            "INFO: Epoch 083: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.2\n",
            "INFO: Epoch 084: loss 3.654 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.351 | clip 0\n",
            "INFO: Epoch 084: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.8\n",
            "INFO: Epoch 085: loss 3.654 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.96 | clip 0.3\n",
            "INFO: Epoch 085: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.5\n",
            "INFO: Epoch 086: loss 3.642 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.258 | clip 0.4\n",
            "INFO: Epoch 086: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.3\n",
            "INFO: Epoch 087: loss 3.626 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.236 | clip 0\n",
            "INFO: Epoch 087: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.3\n",
            "INFO: Epoch 088: loss 3.621 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.985 | clip 0.2\n",
            "INFO: Epoch 088: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.1\n",
            "INFO: Epoch 089: loss 3.635 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.783 | clip 0.6\n",
            "INFO: Epoch 089: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.9\n",
            "INFO: Epoch 090: loss 3.601 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.419 | clip 0.1\n",
            "INFO: Epoch 090: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.5\n",
            "INFO: Epoch 091: loss 3.593 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.029 | clip 0.1\n",
            "INFO: Epoch 091: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.1\n",
            "INFO: Epoch 092: loss 3.603 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.119 | clip 0.7\n",
            "INFO: Epoch 092: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35\n",
            "INFO: Epoch 093: loss 3.573 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.453 | clip 0.1\n",
            "INFO: Epoch 093: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.6\n",
            "INFO: Epoch 094: loss 3.565 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.158 | clip 0.2\n",
            "INFO: Epoch 094: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.6\n",
            "INFO: Epoch 095: loss 3.59 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.921 | clip 0.8\n",
            "INFO: Epoch 095: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.9\n",
            "INFO: Epoch 096: loss 3.546 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.662 | clip 0.1\n",
            "INFO: Epoch 096: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.6\n",
            "INFO: Epoch 097: loss 3.541 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.689 | clip 0.4\n",
            "INFO: Epoch 097: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.6\n",
            "INFO: Epoch 098: loss 3.56 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.168 | clip 0.8\n",
            "INFO: Epoch 098: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.3\n",
            "INFO: Epoch 099: loss 3.532 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.534 | clip 0.2\n",
            "INFO: Epoch 099: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33\n",
            "INFO: Epoch 100: loss 3.522 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.507 | clip 0.3\n",
            "INFO: Epoch 100: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9\n",
            "INFO: Epoch 101: loss 3.529 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.348 | clip 0.7\n",
            "INFO: Epoch 101: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 102: loss 3.494 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.622 | clip 0.1\n",
            "INFO: Epoch 102: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.3\n",
            "INFO: Epoch 103: loss 3.49 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.695 | clip 0.4\n",
            "INFO: Epoch 103: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.4\n",
            "INFO: Epoch 104: loss 3.521 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.342 | clip 0.9\n",
            "INFO: Epoch 104: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33\n",
            "INFO: Epoch 105: loss 3.475 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.504 | clip 0.2\n",
            "INFO: Epoch 105: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.6\n",
            "INFO: Epoch 106: loss 3.47 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.024 | clip 0.4\n",
            "INFO: Epoch 106: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.4\n",
            "INFO: Epoch 107: loss 3.454 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.865 | clip 0.1\n",
            "INFO: Epoch 107: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.2\n",
            "INFO: Epoch 108: loss 3.442 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.343 | clip 0.1\n",
            "INFO: Epoch 108: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.8\n",
            "INFO: Epoch 109: loss 3.433 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.716 | clip 0.1\n",
            "INFO: Epoch 109: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.5\n",
            "INFO: Epoch 110: loss 3.431 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.058 | clip 0.2\n",
            "INFO: Epoch 110: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.4\n",
            "INFO: Epoch 111: loss 3.415 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.318 | clip 0.1\n",
            "INFO: Epoch 111: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.1\n",
            "INFO: Epoch 112: loss 3.402 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.572 | clip 0.1\n",
            "INFO: Epoch 112: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 113: loss 3.406 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.742 | clip 0.4\n",
            "INFO: Epoch 113: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.5\n",
            "INFO: Epoch 114: loss 3.395 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.912 | clip 0.1\n",
            "INFO: Epoch 114: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.9\n",
            "INFO: Epoch 115: loss 3.384 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.158 | clip 0.1\n",
            "INFO: Epoch 115: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.2\n",
            "INFO: Epoch 116: loss 3.388 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.619 | clip 0.5\n",
            "INFO: Epoch 116: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 117: loss 3.371 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.766 | clip 0.1\n",
            "INFO: Epoch 117: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 118: loss 3.353 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.571 | clip 0.1\n",
            "INFO: Epoch 118: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.4\n",
            "INFO: Epoch 119: loss 3.353 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.852 | clip 0.3\n",
            "INFO: Epoch 119: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.2\n",
            "INFO: Epoch 120: loss 3.358 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.963 | clip 0.3\n",
            "INFO: Epoch 120: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.5\n",
            "INFO: Epoch 121: loss 3.335 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.019 | clip 0.1\n",
            "INFO: Epoch 121: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 122: loss 3.327 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.501 | clip 0.2\n",
            "INFO: Epoch 122: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 123: loss 3.349 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.172 | clip 0.8\n",
            "INFO: Epoch 123: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28\n",
            "INFO: Epoch 124: loss 3.316 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.454 | clip 0.2\n",
            "INFO: Epoch 124: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 125: loss 3.311 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.224 | clip 0.5\n",
            "INFO: Epoch 125: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 126: loss 3.326 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.17 | clip 0.7\n",
            "INFO: Epoch 126: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.8\n",
            "INFO: Epoch 127: loss 3.296 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.155 | clip 0.1\n",
            "INFO: Epoch 127: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.7\n",
            "INFO: Epoch 128: loss 3.29 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.169 | clip 0.4\n",
            "INFO: Epoch 128: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 129: loss 3.285 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.775 | clip 0.3\n",
            "INFO: Epoch 129: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 130: loss 3.263 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.571 | clip 0.1\n",
            "INFO: Epoch 130: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 131: loss 3.252 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.103 | clip 0.1\n",
            "INFO: Epoch 131: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 132: loss 3.273 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.993 | clip 0.6\n",
            "INFO: Epoch 132: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 133: loss 3.248 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.328 | clip 0.2\n",
            "INFO: Epoch 133: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.5\n",
            "INFO: Epoch 134: loss 3.247 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.312 | clip 0.3\n",
            "INFO: Epoch 134: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.5\n",
            "INFO: Epoch 135: loss 3.236 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.378 | clip 0.2\n",
            "INFO: Epoch 135: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 136: loss 3.221 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.833 | clip 0.1\n",
            "INFO: Epoch 136: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.3\n",
            "INFO: Epoch 137: loss 3.213 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.196 | clip 0.3\n",
            "INFO: Epoch 137: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 138: loss 3.228 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.033 | clip 0.5\n",
            "INFO: Epoch 138: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 139: loss 3.199 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.696 | clip 0.1\n",
            "INFO: Epoch 139: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.6\n",
            "INFO: Epoch 140: loss 3.196 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.817 | clip 0.3\n",
            "INFO: Epoch 140: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 141: loss 3.197 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.669 | clip 0.2\n",
            "INFO: Epoch 141: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 142: loss 3.173 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.744 | clip 0.1\n",
            "INFO: Epoch 142: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.3\n",
            "INFO: Epoch 143: loss 3.172 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.819 | clip 0.4\n",
            "INFO: Epoch 143: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 144: loss 3.19 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.198 | clip 0.8\n",
            "INFO: Epoch 144: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 145: loss 3.163 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.536 | clip 0.2\n",
            "INFO: Epoch 145: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 146: loss 3.167 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.612 | clip 0.3\n",
            "INFO: Epoch 146: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 147: loss 3.137 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.561 | clip 0.1\n",
            "INFO: Epoch 147: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.6\n",
            "INFO: Epoch 148: loss 3.133 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.667 | clip 0.1\n",
            "INFO: Epoch 148: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 149: loss 3.132 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.729 | clip 0.1\n",
            "INFO: Epoch 149: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2\n",
            "INFO: Epoch 150: loss 3.125 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.367 | clip 0.2\n",
            "INFO: Epoch 150: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2\n",
            "INFO: Epoch 151: loss 3.119 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.977 | clip 0.3\n",
            "INFO: Epoch 151: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 152: loss 3.12 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.015 | clip 0.3\n",
            "INFO: Epoch 152: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 153: loss 3.103 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.753 | clip 0.1\n",
            "INFO: Epoch 153: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 154: loss 3.104 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.681 | clip 0.3\n",
            "INFO: Epoch 154: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 155: loss 3.091 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.845 | clip 0.1\n",
            "INFO: Epoch 155: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 156: loss 3.081 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.62 | clip 0.1\n",
            "INFO: Epoch 156: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.5\n",
            "INFO: Epoch 157: loss 3.073 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.148 | clip 0.2\n",
            "INFO: Epoch 157: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 158: loss 3.087 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.202 | clip 0.3\n",
            "INFO: Epoch 158: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.5\n",
            "INFO: Epoch 159: loss 3.061 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.866 | clip 0.1\n",
            "INFO: Epoch 159: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 160: loss 3.061 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.694 | clip 0.3\n",
            "INFO: Epoch 160: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 161: loss 3.075 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.665 | clip 0.7\n",
            "INFO: Epoch 161: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.3\n",
            "INFO: Epoch 162: loss 3.048 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.14 | clip 0.2\n",
            "INFO: Epoch 162: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 163: loss 3.049 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.84 | clip 0.4\n",
            "INFO: Epoch 163: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 164: loss 3.06 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.005 | clip 0.7\n",
            "INFO: Epoch 164: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 165: loss 3.03 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.876 | clip 0.1\n",
            "INFO: Epoch 165: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 166: loss 3.028 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.819 | clip 0.3\n",
            "INFO: Epoch 166: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 167: loss 3.026 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.556 | clip 0.1\n",
            "INFO: Epoch 167: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6\n",
            "INFO: Epoch 168: loss 3.007 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.684 | clip 0.1\n",
            "INFO: Epoch 168: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.2\n",
            "INFO: Epoch 169: loss 3.005 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.618 | clip 0.3\n",
            "INFO: Epoch 169: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.2\n",
            "INFO: Epoch 170: loss 3.025 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.744 | clip 0.7\n",
            "INFO: Epoch 170: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5\n",
            "INFO: Epoch 171: loss 2.998 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.219 | clip 0.1\n",
            "INFO: Epoch 171: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8\n",
            "INFO: Epoch 172: loss 2.992 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.791 | clip 0.3\n",
            "INFO: Epoch 172: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8\n",
            "INFO: Epoch 173: loss 2.99 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.493 | clip 0.2\n",
            "INFO: Epoch 173: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 174: loss 2.979 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.981 | clip 0.2\n",
            "INFO: Epoch 174: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 175: loss 2.98 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.79 | clip 0.4\n",
            "INFO: Epoch 175: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 176: loss 2.989 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.733 | clip 0.4\n",
            "INFO: Epoch 176: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8\n",
            "INFO: Epoch 177: loss 2.961 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.943 | clip 0.2\n",
            "INFO: Epoch 177: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 178: loss 2.959 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.788 | clip 0.3\n",
            "INFO: Epoch 178: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 179: loss 2.96 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.656 | clip 0.2\n",
            "INFO: Epoch 179: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 180: loss 2.941 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.657 | clip 0.1\n",
            "INFO: Epoch 180: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 181: loss 2.939 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.215 | clip 0.1\n",
            "INFO: Epoch 181: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 182: loss 2.961 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.137 | clip 0.5\n",
            "INFO: Epoch 182: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 183: loss 2.937 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.167 | clip 0.2\n",
            "INFO: Epoch 183: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7\n",
            "INFO: Epoch 184: loss 2.931 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.145 | clip 0.3\n",
            "INFO: Epoch 184: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7\n",
            "INFO: Epoch 185: loss 2.93 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.762 | clip 0.2\n",
            "INFO: Epoch 185: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 186: loss 2.907 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.806 | clip 0.1\n",
            "INFO: Epoch 186: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 187: loss 2.905 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.64 | clip 0.3\n",
            "INFO: Epoch 187: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 188: loss 2.921 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.79 | clip 0.4\n",
            "INFO: Epoch 188: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 189: loss 2.902 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.218 | clip 0.2\n",
            "INFO: Epoch 189: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 190: loss 2.897 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.248 | clip 0.3\n",
            "INFO: Epoch 190: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 191: loss 2.89 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.43 | clip 0.2\n",
            "INFO: Epoch 191: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 192: loss 2.883 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.843 | clip 0.1\n",
            "INFO: Epoch 192: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 193: loss 2.878 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.359 | clip 0.2\n",
            "INFO: Epoch 193: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19\n",
            "INFO: Epoch 194: loss 2.887 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.585 | clip 0.3\n",
            "INFO: Epoch 194: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 195: loss 2.864 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.975 | clip 0.1\n",
            "INFO: Epoch 195: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 196: loss 2.868 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.193 | clip 0.3\n",
            "INFO: Epoch 196: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 197: loss 2.863 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.686 | clip 0.2\n",
            "INFO: Epoch 197: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 198: loss 2.848 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.749 | clip 0.1\n",
            "INFO: Epoch 198: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 199: loss 2.846 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.272 | clip 0.3\n",
            "INFO: Epoch 199: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 200: loss 2.862 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.745 | clip 0.4\n",
            "INFO: Epoch 200: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 201: loss 2.84 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.176 | clip 0.2\n",
            "INFO: Epoch 201: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 202: loss 2.835 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.98 | clip 0.3\n",
            "INFO: Epoch 202: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 203: loss 2.838 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.673 | clip 0.2\n",
            "INFO: Epoch 203: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 204: loss 2.823 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.912 | clip 0.1\n",
            "INFO: Epoch 204: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 205: loss 2.821 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.637 | clip 0.4\n",
            "INFO: Epoch 205: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 206: loss 2.836 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.142 | clip 0.6\n",
            "INFO: Epoch 206: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 207: loss 2.812 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.431 | clip 0.2\n",
            "INFO: Epoch 207: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 208: loss 2.808 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.332 | clip 0.3\n",
            "INFO: Epoch 208: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 209: loss 2.812 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.848 | clip 0.2\n",
            "INFO: Epoch 209: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 210: loss 2.793 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.899 | clip 0.1\n",
            "INFO: Epoch 210: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 211: loss 2.791 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.514 | clip 0.3\n",
            "INFO: Epoch 211: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 212: loss 2.804 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.43 | clip 0.3\n",
            "INFO: Epoch 212: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 213: loss 2.777 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.342 | clip 0.2\n",
            "INFO: Epoch 213: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 214: loss 2.774 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.103 | clip 0.3\n",
            "INFO: Epoch 214: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 215: loss 2.773 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.471 | clip 0.2\n",
            "INFO: Epoch 215: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 216: loss 2.764 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.792 | clip 0.1\n",
            "INFO: Epoch 216: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 217: loss 2.763 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.616 | clip 0.3\n",
            "INFO: Epoch 217: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 218: loss 2.773 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.392 | clip 0.4\n",
            "INFO: Epoch 218: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 219: loss 2.755 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.303 | clip 0.3\n",
            "INFO: Epoch 219: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 220: loss 2.751 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.448 | clip 0.3\n",
            "INFO: Epoch 220: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 221: loss 2.747 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.19 | clip 0.2\n",
            "INFO: Epoch 221: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 222: loss 2.739 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.093 | clip 0.2\n",
            "INFO: Epoch 222: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 223: loss 2.734 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.24 | clip 0.3\n",
            "INFO: Epoch 223: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 224: loss 2.743 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.379 | clip 0.3\n",
            "INFO: Epoch 224: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 225: loss 2.724 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.878 | clip 0.1\n",
            "INFO: Epoch 225: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 226: loss 2.71 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.189 | clip 0.2\n",
            "INFO: Epoch 226: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 227: loss 2.713 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.064 | clip 0.2\n",
            "INFO: Epoch 227: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 228: loss 2.715 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.682 | clip 0.2\n",
            "INFO: Epoch 228: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 229: loss 2.71 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.011 | clip 0.2\n",
            "INFO: Epoch 229: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 230: loss 2.703 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.771 | clip 0.3\n",
            "INFO: Epoch 230: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 231: loss 2.695 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.996 | clip 0.1\n",
            "INFO: Epoch 231: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 232: loss 2.694 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.344 | clip 0.2\n",
            "INFO: Epoch 232: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 233: loss 2.686 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.97 | clip 0.1\n",
            "INFO: Epoch 233: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 234: loss 2.684 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.706 | clip 0.3\n",
            "INFO: Epoch 234: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 235: loss 2.682 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.128 | clip 0.2\n",
            "INFO: Epoch 235: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 236: loss 2.681 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.845 | clip 0.3\n",
            "INFO: Epoch 236: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 237: loss 2.673 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.623 | clip 0.2\n",
            "INFO: Epoch 237: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 238: loss 2.663 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.384 | clip 0.2\n",
            "INFO: Epoch 238: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 239: loss 2.661 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.167 | clip 0.2\n",
            "INFO: Epoch 239: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 240: loss 2.664 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.241 | clip 0.3\n",
            "INFO: Epoch 240: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 241: loss 2.651 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.983 | clip 0.1\n",
            "INFO: Epoch 241: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 242: loss 2.652 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.062 | clip 0.1\n",
            "INFO: Epoch 242: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 243: loss 2.648 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.585 | clip 0.3\n",
            "INFO: Epoch 243: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 244: loss 2.653 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.996 | clip 0.3\n",
            "INFO: Epoch 244: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 245: loss 2.636 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.143 | clip 0.2\n",
            "INFO: Epoch 245: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 246: loss 2.636 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.882 | clip 0.3\n",
            "INFO: Epoch 246: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 247: loss 2.633 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.629 | clip 0.2\n",
            "INFO: Epoch 247: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 248: loss 2.622 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.24 | clip 0.2\n",
            "INFO: Epoch 248: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 249: loss 2.622 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.718 | clip 0.4\n",
            "INFO: Epoch 249: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 250: loss 2.637 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.336 | clip 0.6\n",
            "INFO: Epoch 250: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 251: loss 2.616 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.266 | clip 0.2\n",
            "INFO: Epoch 251: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 252: loss 2.61 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.032 | clip 0.4\n",
            "INFO: Epoch 252: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 253: loss 2.614 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.556 | clip 0.6\n",
            "INFO: Epoch 253: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 254: loss 2.604 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.342 | clip 0.2\n",
            "INFO: Epoch 254: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 255: loss 2.601 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.026 | clip 0.4\n",
            "INFO: Epoch 255: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 256: loss 2.608 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.572 | clip 0.7\n",
            "INFO: Epoch 256: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 257: loss 2.591 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.402 | clip 0.2\n",
            "INFO: Epoch 257: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 258: loss 2.588 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.496 | clip 0.4\n",
            "INFO: Epoch 258: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 259: loss 2.583 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.919 | clip 0.2\n",
            "INFO: Epoch 259: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 260: loss 2.576 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.169 | clip 0.2\n",
            "INFO: Epoch 260: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 261: loss 2.568 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.643 | clip 0.3\n",
            "INFO: Epoch 261: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 262: loss 2.589 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.97 | clip 0.5\n",
            "INFO: Epoch 262: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 263: loss 2.566 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.669 | clip 0.3\n",
            "INFO: Epoch 263: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 264: loss 2.565 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.388 | clip 0.3\n",
            "INFO: Epoch 264: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 265: loss 2.564 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.942 | clip 0.3\n",
            "INFO: Epoch 265: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 266: loss 2.55 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.302 | clip 0.2\n",
            "INFO: Epoch 266: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E17 bs1000+lr0.0005+dropout0.30"
      ],
      "metadata": {
        "id": "AjFj3cVWm6gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_17/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.0005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi0WaYNhnA0G",
        "outputId": "abe330b1-9cbf-42bd-f753-f0a4544a9b3b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_17/checkpoints --cuda --batch-size 1000 --lr 0.0005\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_17/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.237 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.133 | clip 0\n",
            "INFO: Epoch 000: valid_loss 8.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.39e+03\n",
            "INFO: Epoch 001: loss 7.886 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.602 | clip 0.7\n",
            "INFO: Epoch 001: valid_loss 7.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 1.4e+03\n",
            "INFO: Epoch 002: loss 6.754 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.56 | clip 1\n",
            "INFO: Epoch 002: valid_loss 6.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 447\n",
            "INFO: Epoch 003: loss 5.907 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.63 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 259\n",
            "INFO: Epoch 004: loss 5.538 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.983 | clip 1\n",
            "INFO: Epoch 004: valid_loss 5.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 207\n",
            "INFO: Epoch 005: loss 5.409 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.051 | clip 0.6\n",
            "INFO: Epoch 005: valid_loss 5.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 190\n",
            "INFO: Epoch 006: loss 5.348 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.972 | clip 0.5\n",
            "INFO: Epoch 006: valid_loss 5.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 180\n",
            "INFO: Epoch 007: loss 5.304 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.929 | clip 0.7\n",
            "INFO: Epoch 007: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169\n",
            "INFO: Epoch 008: loss 5.238 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.765 | clip 0.7\n",
            "INFO: Epoch 008: valid_loss 5.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 160\n",
            "INFO: Epoch 009: loss 5.187 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.983 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 5.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 151\n",
            "INFO: Epoch 010: loss 5.124 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.38 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 144\n",
            "INFO: Epoch 011: loss 5.075 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.505 | clip 0.7\n",
            "INFO: Epoch 011: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 012: loss 5.03 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.558 | clip 0.7\n",
            "INFO: Epoch 012: valid_loss 4.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 132\n",
            "INFO: Epoch 013: loss 4.985 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.798 | clip 0.6\n",
            "INFO: Epoch 013: valid_loss 4.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 128\n",
            "INFO: Epoch 014: loss 4.955 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.3 | clip 0.7\n",
            "INFO: Epoch 014: valid_loss 4.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 123\n",
            "INFO: Epoch 015: loss 4.911 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.473 | clip 0.6\n",
            "INFO: Epoch 015: valid_loss 4.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 120\n",
            "INFO: Epoch 016: loss 4.885 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.023 | clip 0.7\n",
            "INFO: Epoch 016: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116\n",
            "INFO: Epoch 017: loss 4.844 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.749 | clip 0.4\n",
            "INFO: Epoch 017: valid_loss 4.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 113\n",
            "INFO: Epoch 018: loss 4.82 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.49 | clip 0.6\n",
            "INFO: Epoch 018: valid_loss 4.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 110\n",
            "INFO: Epoch 019: loss 4.785 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.575 | clip 0.4\n",
            "INFO: Epoch 019: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 107\n",
            "INFO: Epoch 020: loss 4.755 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.545 | clip 0.4\n",
            "INFO: Epoch 020: valid_loss 4.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104\n",
            "INFO: Epoch 021: loss 4.729 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.416 | clip 0.4\n",
            "INFO: Epoch 021: valid_loss 4.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 022: loss 4.698 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.948 | clip 0.3\n",
            "INFO: Epoch 022: valid_loss 4.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 99\n",
            "INFO: Epoch 023: loss 4.675 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.887 | clip 0.3\n",
            "INFO: Epoch 023: valid_loss 4.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 97.1\n",
            "INFO: Epoch 024: loss 4.651 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.211 | clip 0.3\n",
            "INFO: Epoch 024: valid_loss 4.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95.7\n",
            "INFO: Epoch 025: loss 4.636 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.014 | clip 0.6\n",
            "INFO: Epoch 025: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.1\n",
            "INFO: Epoch 026: loss 4.609 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.328 | clip 0.4\n",
            "INFO: Epoch 026: valid_loss 4.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.4\n",
            "INFO: Epoch 027: loss 4.579 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.498 | clip 0.2\n",
            "INFO: Epoch 027: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.9\n",
            "INFO: Epoch 028: loss 4.562 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.963 | clip 0.4\n",
            "INFO: Epoch 028: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.8\n",
            "INFO: Epoch 029: loss 4.57 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.707 | clip 0.7\n",
            "INFO: Epoch 029: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.7\n",
            "INFO: Epoch 030: loss 4.537 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.566 | clip 0.7\n",
            "INFO: Epoch 030: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.2\n",
            "INFO: Epoch 031: loss 4.501 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.518 | clip 0.2\n",
            "INFO: Epoch 031: valid_loss 4.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 83.4\n",
            "INFO: Epoch 032: loss 4.483 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.932 | clip 0.3\n",
            "INFO: Epoch 032: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.8\n",
            "INFO: Epoch 033: loss 4.462 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.219 | clip 0.3\n",
            "INFO: Epoch 033: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80\n",
            "INFO: Epoch 034: loss 4.438 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.983 | clip 0.3\n",
            "INFO: Epoch 034: valid_loss 4.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 78.1\n",
            "INFO: Epoch 035: loss 4.412 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.593 | clip 0.3\n",
            "INFO: Epoch 035: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.4\n",
            "INFO: Epoch 036: loss 4.39 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.529 | clip 0.3\n",
            "INFO: Epoch 036: valid_loss 4.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.8\n",
            "INFO: Epoch 037: loss 4.368 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.112 | clip 0.4\n",
            "INFO: Epoch 037: valid_loss 4.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.2\n",
            "INFO: Epoch 038: loss 4.355 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.081 | clip 0.7\n",
            "INFO: Epoch 038: valid_loss 4.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.3\n",
            "INFO: Epoch 039: loss 4.324 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.56 | clip 0.2\n",
            "INFO: Epoch 039: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.8\n",
            "INFO: Epoch 040: loss 4.295 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.142 | clip 0.2\n",
            "INFO: Epoch 040: valid_loss 4.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.1\n",
            "INFO: Epoch 041: loss 4.275 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.467 | clip 0.2\n",
            "INFO: Epoch 041: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.5\n",
            "INFO: Epoch 042: loss 4.276 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.123 | clip 0.8\n",
            "INFO: Epoch 042: valid_loss 4.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.3\n",
            "INFO: Epoch 043: loss 4.238 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.074 | clip 0.3\n",
            "INFO: Epoch 043: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64\n",
            "INFO: Epoch 044: loss 4.207 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.032 | clip 0.2\n",
            "INFO: Epoch 044: valid_loss 4.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.7\n",
            "INFO: Epoch 045: loss 4.182 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.124 | clip 0.2\n",
            "INFO: Epoch 045: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.6\n",
            "INFO: Epoch 046: loss 4.168 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.628 | clip 0.3\n",
            "INFO: Epoch 046: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.2\n",
            "INFO: Epoch 047: loss 4.172 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.28 | clip 0.8\n",
            "INFO: Epoch 047: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.2\n",
            "INFO: Epoch 048: loss 4.126 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.151 | clip 0.2\n",
            "INFO: Epoch 048: valid_loss 4.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.3\n",
            "INFO: Epoch 049: loss 4.106 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.915 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.1\n",
            "INFO: Epoch 050: loss 4.091 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.593 | clip 0.3\n",
            "INFO: Epoch 050: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57\n",
            "INFO: Epoch 051: loss 4.094 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.113 | clip 0.8\n",
            "INFO: Epoch 051: valid_loss 4.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.2\n",
            "INFO: Epoch 052: loss 4.051 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.921 | clip 0.2\n",
            "INFO: Epoch 052: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.4\n",
            "INFO: Epoch 053: loss 4.033 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.087 | clip 0.2\n",
            "INFO: Epoch 053: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.5\n",
            "INFO: Epoch 054: loss 4.023 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.254 | clip 0.5\n",
            "INFO: Epoch 054: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.9\n",
            "INFO: Epoch 055: loss 4.021 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.914 | clip 0.8\n",
            "INFO: Epoch 055: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.2\n",
            "INFO: Epoch 056: loss 3.981 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.919 | clip 0.1\n",
            "INFO: Epoch 056: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.8\n",
            "INFO: Epoch 057: loss 3.966 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.505 | clip 0.4\n",
            "INFO: Epoch 057: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.7\n",
            "INFO: Epoch 058: loss 3.988 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.772 | clip 0.8\n",
            "INFO: Epoch 058: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.9\n",
            "INFO: Epoch 059: loss 3.933 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.515 | clip 0.2\n",
            "INFO: Epoch 059: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.4\n",
            "INFO: Epoch 060: loss 3.919 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.21 | clip 0.2\n",
            "INFO: Epoch 060: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.3\n",
            "INFO: Epoch 061: loss 3.921 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.418 | clip 0.8\n",
            "INFO: Epoch 061: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.9\n",
            "INFO: Epoch 062: loss 3.88 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.357 | clip 0.1\n",
            "INFO: Epoch 062: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.5\n",
            "INFO: Epoch 063: loss 3.863 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.079 | clip 0.2\n",
            "INFO: Epoch 063: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.2\n",
            "INFO: Epoch 064: loss 3.867 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.966 | clip 0.6\n",
            "INFO: Epoch 064: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.4\n",
            "INFO: Epoch 065: loss 3.849 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.93 | clip 0.4\n",
            "INFO: Epoch 065: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45\n",
            "INFO: Epoch 066: loss 3.817 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.444 | clip 0\n",
            "INFO: Epoch 066: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.4\n",
            "INFO: Epoch 067: loss 3.808 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.627 | clip 0.5\n",
            "INFO: Epoch 067: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.9\n",
            "INFO: Epoch 068: loss 3.832 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.717 | clip 0.8\n",
            "INFO: Epoch 068: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.1\n",
            "INFO: Epoch 069: loss 3.777 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.305 | clip 0\n",
            "INFO: Epoch 069: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.5\n",
            "INFO: Epoch 070: loss 3.767 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.207 | clip 0.3\n",
            "INFO: Epoch 070: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.4\n",
            "INFO: Epoch 071: loss 3.771 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.014 | clip 0.8\n",
            "INFO: Epoch 071: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.3\n",
            "INFO: Epoch 072: loss 3.734 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.6 | clip 0\n",
            "INFO: Epoch 072: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.4\n",
            "INFO: Epoch 073: loss 3.724 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.401 | clip 0.3\n",
            "INFO: Epoch 073: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.7\n",
            "INFO: Epoch 074: loss 3.744 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.16 | clip 0.8\n",
            "INFO: Epoch 074: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.1\n",
            "INFO: Epoch 075: loss 3.695 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.279 | clip 0.1\n",
            "INFO: Epoch 075: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.8\n",
            "INFO: Epoch 076: loss 3.686 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.975 | clip 0.2\n",
            "INFO: Epoch 076: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40\n",
            "INFO: Epoch 077: loss 3.702 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.621 | clip 0.8\n",
            "INFO: Epoch 077: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.9\n",
            "INFO: Epoch 078: loss 3.659 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.425 | clip 0\n",
            "INFO: Epoch 078: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.7\n",
            "INFO: Epoch 079: loss 3.656 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.086 | clip 0.6\n",
            "INFO: Epoch 079: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.4\n",
            "INFO: Epoch 080: loss 3.704 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.288 | clip 0.9\n",
            "INFO: Epoch 080: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.8\n",
            "INFO: Epoch 081: loss 3.649 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.238 | clip 0.6\n",
            "INFO: Epoch 081: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.5\n",
            "INFO: Epoch 082: loss 3.661 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.61 | clip 0.8\n",
            "INFO: Epoch 082: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.4\n",
            "INFO: Epoch 083: loss 3.614 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.363 | clip 0.3\n",
            "INFO: Epoch 083: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.6\n",
            "INFO: Epoch 084: loss 3.617 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.966 | clip 0.5\n",
            "INFO: Epoch 084: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.4\n",
            "INFO: Epoch 085: loss 3.582 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.049 | clip 0\n",
            "INFO: Epoch 085: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.5\n",
            "INFO: Epoch 086: loss 3.571 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.771 | clip 0.2\n",
            "INFO: Epoch 086: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.2\n",
            "INFO: Epoch 087: loss 3.569 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.826 | clip 0.3\n",
            "INFO: Epoch 087: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36\n",
            "INFO: Epoch 088: loss 3.542 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.237 | clip 0\n",
            "INFO: Epoch 088: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.6\n",
            "INFO: Epoch 089: loss 3.535 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.366 | clip 0.3\n",
            "INFO: Epoch 089: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.9\n",
            "INFO: Epoch 090: loss 3.564 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.105 | clip 0.8\n",
            "INFO: Epoch 090: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.9\n",
            "INFO: Epoch 091: loss 3.515 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.743 | clip 0.1\n",
            "INFO: Epoch 091: valid_loss 3.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.5\n",
            "INFO: Epoch 092: loss 3.512 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.153 | clip 0.5\n",
            "INFO: Epoch 092: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 093: loss 3.503 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.9 | clip 0.4\n",
            "INFO: Epoch 093: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.3\n",
            "INFO: Epoch 094: loss 3.472 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.446 | clip 0.1\n",
            "INFO: Epoch 094: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.5\n",
            "INFO: Epoch 095: loss 3.469 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.587 | clip 0.4\n",
            "INFO: Epoch 095: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8\n",
            "INFO: Epoch 096: loss 3.505 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.411 | clip 0.9\n",
            "INFO: Epoch 096: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 097: loss 3.453 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.557 | clip 0.3\n",
            "INFO: Epoch 097: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 098: loss 3.458 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.931 | clip 0.5\n",
            "INFO: Epoch 098: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.7\n",
            "INFO: Epoch 099: loss 3.422 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.018 | clip 0\n",
            "INFO: Epoch 099: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.1\n",
            "INFO: Epoch 100: loss 3.412 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.702 | clip 0.1\n",
            "INFO: Epoch 100: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.7\n",
            "INFO: Epoch 101: loss 3.407 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.234 | clip 0.2\n",
            "INFO: Epoch 101: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.6\n",
            "INFO: Epoch 102: loss 3.382 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.214 | clip 0.1\n",
            "INFO: Epoch 102: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.1\n",
            "INFO: Epoch 103: loss 3.375 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.331 | clip 0.4\n",
            "INFO: Epoch 103: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 104: loss 3.401 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.899 | clip 0.9\n",
            "INFO: Epoch 104: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.6\n",
            "INFO: Epoch 105: loss 3.359 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.39 | clip 0.1\n",
            "INFO: Epoch 105: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.4\n",
            "INFO: Epoch 106: loss 3.356 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.146 | clip 0.4\n",
            "INFO: Epoch 106: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.1\n",
            "INFO: Epoch 107: loss 3.337 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.82 | clip 0.1\n",
            "INFO: Epoch 107: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.9\n",
            "INFO: Epoch 108: loss 3.32 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.12 | clip 0\n",
            "INFO: Epoch 108: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.6\n",
            "INFO: Epoch 109: loss 3.311 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.014 | clip 0.1\n",
            "INFO: Epoch 109: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.5\n",
            "INFO: Epoch 110: loss 3.323 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.816 | clip 0.7\n",
            "INFO: Epoch 110: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.4\n",
            "INFO: Epoch 111: loss 3.289 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.196 | clip 0\n",
            "INFO: Epoch 111: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 112: loss 3.284 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.04 | clip 0.1\n",
            "INFO: Epoch 112: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 113: loss 3.298 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.11 | clip 0.7\n",
            "INFO: Epoch 113: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29\n",
            "INFO: Epoch 114: loss 3.262 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.874 | clip 0.1\n",
            "INFO: Epoch 114: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.1\n",
            "INFO: Epoch 115: loss 3.261 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.206 | clip 0.5\n",
            "INFO: Epoch 115: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.1\n",
            "INFO: Epoch 116: loss 3.261 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.725 | clip 0.7\n",
            "INFO: Epoch 116: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.2\n",
            "INFO: Epoch 117: loss 3.234 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.823 | clip 0.1\n",
            "INFO: Epoch 117: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 118: loss 3.227 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.288 | clip 0.3\n",
            "INFO: Epoch 118: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.4\n",
            "INFO: Epoch 119: loss 3.23 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.994 | clip 0.5\n",
            "INFO: Epoch 119: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.5\n",
            "INFO: Epoch 120: loss 3.204 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.713 | clip 0.2\n",
            "INFO: Epoch 120: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.8\n",
            "INFO: Epoch 121: loss 3.198 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.922 | clip 0.5\n",
            "INFO: Epoch 121: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.8\n",
            "INFO: Epoch 122: loss 3.211 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.848 | clip 0.7\n",
            "INFO: Epoch 122: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 123: loss 3.178 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.231 | clip 0.1\n",
            "INFO: Epoch 123: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 124: loss 3.173 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.624 | clip 0.4\n",
            "INFO: Epoch 124: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 125: loss 3.16 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.849 | clip 0.1\n",
            "INFO: Epoch 125: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 126: loss 3.142 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.179 | clip 0\n",
            "INFO: Epoch 126: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 127: loss 3.139 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.87 | clip 0.1\n",
            "INFO: Epoch 127: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 128: loss 3.147 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.397 | clip 0.4\n",
            "INFO: Epoch 128: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 129: loss 3.12 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.571 | clip 0.1\n",
            "INFO: Epoch 129: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.9\n",
            "INFO: Epoch 130: loss 3.118 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.197 | clip 0.1\n",
            "INFO: Epoch 130: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 131: loss 3.13 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.877 | clip 0.7\n",
            "INFO: Epoch 131: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 132: loss 3.095 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.212 | clip 0.2\n",
            "INFO: Epoch 132: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 133: loss 3.099 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.661 | clip 0.5\n",
            "INFO: Epoch 133: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 134: loss 3.095 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.486 | clip 0.4\n",
            "INFO: Epoch 134: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.6\n",
            "INFO: Epoch 135: loss 3.068 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.58 | clip 0.1\n",
            "INFO: Epoch 135: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 136: loss 3.062 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.208 | clip 0.2\n",
            "INFO: Epoch 136: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 137: loss 3.059 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.831 | clip 0.3\n",
            "INFO: Epoch 137: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 138: loss 3.039 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.754 | clip 0.1\n",
            "INFO: Epoch 138: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 139: loss 3.038 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.861 | clip 0.5\n",
            "INFO: Epoch 139: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 140: loss 3.055 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.333 | clip 0.8\n",
            "INFO: Epoch 140: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 141: loss 3.02 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.241 | clip 0.1\n",
            "INFO: Epoch 141: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 142: loss 3.019 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.878 | clip 0.3\n",
            "INFO: Epoch 142: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.9\n",
            "INFO: Epoch 143: loss 3 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.829 | clip 0.1\n",
            "INFO: Epoch 143: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 144: loss 2.985 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.195 | clip 0.1\n",
            "INFO: Epoch 144: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 145: loss 2.977 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.558 | clip 0.1\n",
            "INFO: Epoch 145: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.3\n",
            "INFO: Epoch 146: loss 2.983 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.931 | clip 0.3\n",
            "INFO: Epoch 146: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.3\n",
            "INFO: Epoch 147: loss 2.96 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.32 | clip 0\n",
            "INFO: Epoch 147: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 148: loss 2.951 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.868 | clip 0.1\n",
            "INFO: Epoch 148: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9\n",
            "INFO: Epoch 149: loss 2.953 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.575 | clip 0.2\n",
            "INFO: Epoch 149: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 150: loss 2.933 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.591 | clip 0.1\n",
            "INFO: Epoch 150: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5\n",
            "INFO: Epoch 151: loss 2.926 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.039 | clip 0.1\n",
            "INFO: Epoch 151: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.5\n",
            "INFO: Epoch 152: loss 2.944 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.352 | clip 0.7\n",
            "INFO: Epoch 152: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 153: loss 2.912 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.838 | clip 0.1\n",
            "INFO: Epoch 153: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 154: loss 2.9 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.631 | clip 0.4\n",
            "INFO: Epoch 154: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 155: loss 2.907 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.101 | clip 0.5\n",
            "INFO: Epoch 155: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 156: loss 2.883 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.851 | clip 0.2\n",
            "INFO: Epoch 156: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 157: loss 2.876 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.426 | clip 0.3\n",
            "INFO: Epoch 157: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 158: loss 2.889 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.432 | clip 0.5\n",
            "INFO: Epoch 158: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.8\n",
            "INFO: Epoch 159: loss 2.861 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.184 | clip 0.2\n",
            "INFO: Epoch 159: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 160: loss 2.852 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.77 | clip 0.3\n",
            "INFO: Epoch 160: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 161: loss 2.85 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.549 | clip 0.2\n",
            "INFO: Epoch 161: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1\n",
            "INFO: Epoch 162: loss 2.833 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.564 | clip 0.1\n",
            "INFO: Epoch 162: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 163: loss 2.829 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.152 | clip 0.2\n",
            "INFO: Epoch 163: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 164: loss 2.84 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.821 | clip 0.7\n",
            "INFO: Epoch 164: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 165: loss 2.811 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.815 | clip 0.1\n",
            "INFO: Epoch 165: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 166: loss 2.808 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.449 | clip 0.3\n",
            "INFO: Epoch 166: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 167: loss 2.807 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.856 | clip 0.3\n",
            "INFO: Epoch 167: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 168: loss 2.785 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.915 | clip 0.2\n",
            "INFO: Epoch 168: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 169: loss 2.781 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.841 | clip 0.4\n",
            "INFO: Epoch 169: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 170: loss 2.793 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.661 | clip 0.6\n",
            "INFO: Epoch 170: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 171: loss 2.769 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.008 | clip 0.1\n",
            "INFO: Epoch 171: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 172: loss 2.758 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.477 | clip 0.3\n",
            "INFO: Epoch 172: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 173: loss 2.759 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.348 | clip 0.2\n",
            "INFO: Epoch 173: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 174: loss 2.74 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.682 | clip 0.1\n",
            "INFO: Epoch 174: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 175: loss 2.736 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.376 | clip 0.3\n",
            "INFO: Epoch 175: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 176: loss 2.745 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.288 | clip 0.5\n",
            "INFO: Epoch 176: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 177: loss 2.721 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.12 | clip 0.2\n",
            "INFO: Epoch 177: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 178: loss 2.711 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.643 | clip 0.3\n",
            "INFO: Epoch 178: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 179: loss 2.715 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.802 | clip 0.2\n",
            "INFO: Epoch 179: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 180: loss 2.695 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.732 | clip 0.1\n",
            "INFO: Epoch 180: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 181: loss 2.689 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.251 | clip 0.1\n",
            "INFO: Epoch 181: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 182: loss 2.701 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.524 | clip 0.3\n",
            "INFO: Epoch 182: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 183: loss 2.675 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.97 | clip 0.2\n",
            "INFO: Epoch 183: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 184: loss 2.67 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.507 | clip 0.3\n",
            "INFO: Epoch 184: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 185: loss 2.673 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.895 | clip 0.3\n",
            "INFO: Epoch 185: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 186: loss 2.652 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.985 | clip 0.2\n",
            "INFO: Epoch 186: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 187: loss 2.647 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.642 | clip 0.4\n",
            "INFO: Epoch 187: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 188: loss 2.655 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.224 | clip 0.3\n",
            "INFO: Epoch 188: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 189: loss 2.633 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.131 | clip 0.2\n",
            "INFO: Epoch 189: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 190: loss 2.627 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.536 | clip 0.3\n",
            "INFO: Epoch 190: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 191: loss 2.622 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.429 | clip 0.2\n",
            "INFO: Epoch 191: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 192: loss 2.611 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.932 | clip 0.2\n",
            "INFO: Epoch 192: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 193: loss 2.608 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.44 | clip 0.3\n",
            "INFO: Epoch 193: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 194: loss 2.611 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.289 | clip 0.3\n",
            "INFO: Epoch 194: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 195: loss 2.594 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.039 | clip 0.2\n",
            "INFO: Epoch 195: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 196: loss 2.587 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.631 | clip 0.3\n",
            "INFO: Epoch 196: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 197: loss 2.589 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.899 | clip 0.3\n",
            "INFO: Epoch 197: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 198: loss 2.569 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.039 | clip 0.2\n",
            "INFO: Epoch 198: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 199: loss 2.566 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.434 | clip 0.3\n",
            "INFO: Epoch 199: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 200: loss 2.575 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.136 | clip 0.3\n",
            "INFO: Epoch 200: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 201: loss 2.555 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.218 | clip 0.3\n",
            "INFO: Epoch 201: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 202: loss 2.547 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.801 | clip 0.3\n",
            "INFO: Epoch 202: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 203: loss 2.543 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.678 | clip 0.2\n",
            "INFO: Epoch 203: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 204: loss 2.529 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.772 | clip 0.1\n",
            "INFO: Epoch 204: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 205: loss 2.522 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.406 | clip 0.3\n",
            "INFO: Epoch 205: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 206: loss 2.535 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.486 | clip 0.3\n",
            "INFO: Epoch 206: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 207: loss 2.512 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.344 | clip 0.2\n",
            "INFO: Epoch 207: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 208: loss 2.507 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.047 | clip 0.4\n",
            "INFO: Epoch 208: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 209: loss 2.509 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.67 | clip 0.2\n",
            "INFO: Epoch 209: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 210: loss 2.49 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.821 | clip 0.1\n",
            "INFO: Epoch 210: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 211: loss 2.488 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.496 | clip 0.3\n",
            "INFO: Epoch 211: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 212: loss 2.499 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.422 | clip 0.5\n",
            "INFO: Epoch 212: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 213: loss 2.477 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.32 | clip 0.2\n",
            "INFO: Epoch 213: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 214: loss 2.47 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.148 | clip 0.3\n",
            "INFO: Epoch 214: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 215: loss 2.465 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.604 | clip 0.2\n",
            "INFO: Epoch 215: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 216: loss 2.453 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.87 | clip 0.1\n",
            "INFO: Epoch 216: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 217: loss 2.451 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.155 | clip 0.2\n",
            "INFO: Epoch 217: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 218: loss 2.449 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.582 | clip 0.2\n",
            "INFO: Epoch 218: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 219: loss 2.434 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.06 | clip 0.2\n",
            "INFO: Epoch 219: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 220: loss 2.431 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.835 | clip 0.3\n",
            "INFO: Epoch 220: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 221: loss 2.431 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.553 | clip 0.2\n",
            "INFO: Epoch 221: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 222: loss 2.416 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.915 | clip 0.1\n",
            "INFO: Epoch 222: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 223: loss 2.407 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.279 | clip 0.3\n",
            "INFO: Epoch 223: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 224: loss 2.418 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.318 | clip 0.3\n",
            "INFO: Epoch 224: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 225: loss 2.402 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.274 | clip 0.2\n",
            "INFO: Epoch 225: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 226: loss 2.392 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.774 | clip 0.3\n",
            "INFO: Epoch 226: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 227: loss 2.397 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.909 | clip 0.3\n",
            "INFO: Epoch 227: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 228: loss 2.378 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.173 | clip 0.2\n",
            "INFO: Epoch 228: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 229: loss 2.384 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.876 | clip 0.3\n",
            "INFO: Epoch 229: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 230: loss 2.383 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.183 | clip 0.3\n",
            "INFO: Epoch 230: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 231: loss 2.365 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.369 | clip 0.2\n",
            "INFO: Epoch 231: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 232: loss 2.359 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.825 | clip 0.3\n",
            "INFO: Epoch 232: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 233: loss 2.358 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.487 | clip 0.2\n",
            "INFO: Epoch 233: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 234: loss 2.345 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.072 | clip 0.2\n",
            "INFO: Epoch 234: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 235: loss 2.347 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.436 | clip 0.3\n",
            "INFO: Epoch 235: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 236: loss 2.346 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.546 | clip 0.4\n",
            "INFO: Epoch 236: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 237: loss 2.332 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.301 | clip 0.3\n",
            "INFO: Epoch 237: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 238: loss 2.324 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.751 | clip 0.4\n",
            "INFO: Epoch 238: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 239: loss 2.333 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.935 | clip 0.3\n",
            "INFO: Epoch 239: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 240: loss 2.319 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.206 | clip 0.2\n",
            "INFO: Epoch 240: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 241: loss 2.307 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.73 | clip 0.3\n",
            "INFO: Epoch 241: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 242: loss 2.316 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.985 | clip 0.3\n",
            "INFO: Epoch 242: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 243: loss 2.3 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.141 | clip 0.2\n",
            "INFO: Epoch 243: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 244: loss 2.295 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.788 | clip 0.3\n",
            "INFO: Epoch 244: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 245: loss 2.293 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.861 | clip 0.3\n",
            "INFO: Epoch 245: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 246: loss 2.277 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.056 | clip 0.2\n",
            "INFO: Epoch 246: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 247: loss 2.277 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.564 | clip 0.3\n",
            "INFO: Epoch 247: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 248: loss 2.281 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.539 | clip 0.3\n",
            "INFO: Epoch 248: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 249: loss 2.271 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.507 | clip 0.3\n",
            "INFO: Epoch 249: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 250: loss 2.261 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.725 | clip 0.3\n",
            "INFO: Epoch 250: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 251: loss 2.261 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.912 | clip 0.3\n",
            "INFO: Epoch 251: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 252: loss 2.25 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.314 | clip 0.2\n",
            "INFO: Epoch 252: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 253: loss 2.242 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.087 | clip 0.3\n",
            "INFO: Epoch 253: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 254: loss 2.252 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.284 | clip 0.3\n",
            "INFO: Epoch 254: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 255: loss 2.233 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.235 | clip 0.2\n",
            "INFO: Epoch 255: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 256: loss 2.226 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.556 | clip 0.3\n",
            "INFO: Epoch 256: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 257: loss 2.229 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.626 | clip 0.1\n",
            "INFO: Epoch 257: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 258: loss 2.214 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.171 | clip 0.2\n",
            "INFO: Epoch 258: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 259: loss 2.205 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.557 | clip 0.3\n",
            "INFO: Epoch 259: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 260: loss 2.217 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.995 | clip 0.3\n",
            "INFO: Epoch 260: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 261: loss 2.201 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.393 | clip 0.2\n",
            "INFO: Epoch 261: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 262: loss 2.199 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.759 | clip 0.3\n",
            "INFO: Epoch 262: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 263: loss 2.187 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.032 | clip 0.1\n",
            "INFO: Epoch 263: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 264: loss 2.182 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.868 | clip 0.1\n",
            "INFO: Epoch 264: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 265: loss 2.178 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.743 | clip 0.3\n",
            "INFO: Epoch 265: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 266: loss 2.183 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.386 | clip 0.3\n",
            "INFO: Epoch 266: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 267: loss 2.173 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.45 | clip 0.3\n",
            "INFO: Epoch 267: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 268: loss 2.168 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.573 | clip 0.3\n",
            "INFO: Epoch 268: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 269: loss 2.165 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.701 | clip 0.2\n",
            "INFO: Epoch 269: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 270: loss 2.154 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.183 | clip 0.2\n",
            "INFO: Epoch 270: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 271: loss 2.154 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.829 | clip 0.3\n",
            "INFO: Epoch 271: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 272: loss 2.157 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.422 | clip 0.4\n",
            "INFO: Epoch 272: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 273: loss 2.135 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.243 | clip 0.2\n",
            "INFO: Epoch 273: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 274: loss 2.139 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.614 | clip 0.3\n",
            "INFO: Epoch 274: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 275: loss 2.134 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.617 | clip 0.1\n",
            "INFO: Epoch 275: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 276: loss 2.121 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.223 | clip 0.2\n",
            "INFO: Epoch 276: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 277: loss 2.123 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.161 | clip 0.4\n",
            "INFO: Epoch 277: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 278: loss 2.129 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.627 | clip 0.5\n",
            "INFO: Epoch 278: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 279: loss 2.113 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.823 | clip 0.3\n",
            "INFO: Epoch 279: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 280: loss 2.108 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.168 | clip 0.3\n",
            "INFO: Epoch 280: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 281: loss 2.106 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.586 | clip 0.1\n",
            "INFO: Epoch 281: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 282: loss 2.097 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.077 | clip 0.1\n",
            "INFO: Epoch 282: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 283: loss 2.088 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.962 | clip 0.3\n",
            "INFO: Epoch 283: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 284: loss 2.091 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.594 | clip 0.2\n",
            "INFO: Epoch 284: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 285: loss 2.085 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.153 | clip 0.1\n",
            "INFO: Epoch 285: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 286: loss 2.08 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.213 | clip 0.2\n",
            "INFO: Epoch 286: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 287: loss 2.074 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.335 | clip 0.2\n",
            "INFO: Epoch 287: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 288: loss 2.067 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.331 | clip 0.2\n",
            "INFO: Epoch 288: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 289: loss 2.065 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.073 | clip 0.3\n",
            "INFO: Epoch 289: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 290: loss 2.064 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.554 | clip 0.2\n",
            "INFO: Epoch 290: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 291: loss 2.055 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.657 | clip 0.2\n",
            "INFO: Epoch 291: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 292: loss 2.059 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.247 | clip 0.3\n",
            "INFO: Epoch 292: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 293: loss 2.048 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.536 | clip 0.2\n",
            "INFO: Epoch 293: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 294: loss 2.048 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.67 | clip 0.3\n",
            "INFO: Epoch 294: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 295: loss 2.04 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.348 | clip 0.3\n",
            "INFO: Epoch 295: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 296: loss 2.029 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.189 | clip 0.2\n",
            "INFO: Epoch 296: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 297: loss 2.023 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.335 | clip 0.2\n",
            "INFO: Epoch 297: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 298: loss 2.026 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.749 | clip 0.3\n",
            "INFO: Epoch 298: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 299: loss 2.016 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.679 | clip 0.3\n",
            "INFO: Epoch 299: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 300: loss 2.014 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.524 | clip 0.3\n",
            "INFO: Epoch 300: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 301: loss 2.012 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.007 | clip 0.3\n",
            "INFO: Epoch 301: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 302: loss 2.006 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.347 | clip 0.2\n",
            "INFO: Epoch 302: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 303: loss 1.999 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.334 | clip 0.2\n",
            "INFO: Epoch 303: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 304: loss 1.998 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.802 | clip 0.3\n",
            "INFO: Epoch 304: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 305: loss 1.994 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.74 | clip 0.2\n",
            "INFO: Epoch 305: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 306: loss 1.991 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.795 | clip 0.3\n",
            "INFO: Epoch 306: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 307: loss 1.996 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.863 | clip 0.3\n",
            "INFO: Epoch 307: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 308: loss 1.978 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.937 | clip 0.1\n",
            "INFO: Epoch 308: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 309: loss 1.978 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.533 | clip 0.3\n",
            "INFO: Epoch 309: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 310: loss 1.97 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.445 | clip 0.2\n",
            "INFO: Epoch 310: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 311: loss 1.967 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.456 | clip 0.2\n",
            "INFO: Epoch 311: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 312: loss 1.969 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.249 | clip 0.4\n",
            "INFO: Epoch 312: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 313: loss 1.965 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.117 | clip 0.3\n",
            "INFO: Epoch 313: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 314: loss 1.955 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.826 | clip 0.3\n",
            "INFO: Epoch 314: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 315: loss 1.954 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.219 | clip 0.4\n",
            "INFO: Epoch 315: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 316: loss 1.959 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.32 | clip 0.4\n",
            "INFO: Epoch 316: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 317: loss 1.946 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.918 | clip 0.3\n",
            "INFO: Epoch 317: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 318: loss 1.949 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.346 | clip 0.4\n",
            "INFO: Epoch 318: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 319: loss 1.951 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.418 | clip 0.5\n",
            "INFO: Epoch 319: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 320: loss 1.935 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.953 | clip 0.3\n",
            "INFO: Epoch 320: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 321: loss 1.927 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.113 | clip 0.3\n",
            "INFO: Epoch 321: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 322: loss 1.926 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.721 | clip 0.2\n",
            "INFO: Epoch 322: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 323: loss 1.92 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.599 | clip 0.2\n",
            "INFO: Epoch 323: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 324: loss 1.916 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.053 | clip 0.4\n",
            "INFO: Epoch 324: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 325: loss 1.921 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.907 | clip 0.3\n",
            "INFO: Epoch 325: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 326: loss 1.912 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.746 | clip 0.2\n",
            "INFO: Epoch 326: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 327: loss 1.906 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.354 | clip 0.4\n",
            "INFO: Epoch 327: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 328: loss 1.903 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.831 | clip 0.3\n",
            "INFO: Epoch 328: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 329: loss 1.896 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.552 | clip 0.2\n",
            "INFO: Epoch 329: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 330: loss 1.894 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.616 | clip 0.4\n",
            "INFO: Epoch 330: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 331: loss 1.898 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.949 | clip 0.4\n",
            "INFO: Epoch 331: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 332: loss 1.884 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.906 | clip 0.2\n",
            "INFO: Epoch 332: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 333: loss 1.892 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.907 | clip 0.3\n",
            "INFO: Epoch 333: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 334: loss 1.88 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.778 | clip 0.3\n",
            "INFO: Epoch 334: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 335: loss 1.878 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.83 | clip 0.2\n",
            "INFO: Epoch 335: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 336: loss 1.884 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.487 | clip 0.3\n",
            "INFO: Epoch 336: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 337: loss 1.867 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.373 | clip 0.2\n",
            "INFO: Epoch 337: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 338: loss 1.855 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.257 | clip 0.1\n",
            "INFO: Epoch 338: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 339: loss 1.864 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.095 | clip 0.2\n",
            "INFO: Epoch 339: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 340: loss 1.857 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.496 | clip 0.2\n",
            "INFO: Epoch 340: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 341: loss 1.859 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.452 | clip 0.4\n",
            "INFO: Epoch 341: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 342: loss 1.866 | lr 0.0005 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.75 | clip 0.6\n",
            "INFO: Epoch 342: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E18. bs1000+lr0.0004+dropout0.3"
      ],
      "metadata": {
        "id": "hrE7F3nTvtC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_18/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 1000 \\\n",
        "    --lr 0.0004"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn2lhfbZwQsY",
        "outputId": "7a571cfa-0b64-4dfa-f583-9ef4791a0a10"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_18/checkpoints --cuda --batch-size 1000 --lr 0.0004\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1000, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0004, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_18/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.249 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.03 | clip 0\n",
            "INFO: Epoch 000: valid_loss 8.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.55e+03\n",
            "INFO: Epoch 001: loss 8.061 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.17 | clip 0.4\n",
            "INFO: Epoch 001: valid_loss 7.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 2.33e+03\n",
            "INFO: Epoch 002: loss 7.281 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 10.57 | clip 1\n",
            "INFO: Epoch 002: valid_loss 6.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 733\n",
            "INFO: Epoch 003: loss 6.315 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 11.51 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 357\n",
            "INFO: Epoch 004: loss 5.776 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 9.877 | clip 1\n",
            "INFO: Epoch 004: valid_loss 5.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 247\n",
            "INFO: Epoch 005: loss 5.523 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 7.476 | clip 1\n",
            "INFO: Epoch 005: valid_loss 5.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 208\n",
            "INFO: Epoch 006: loss 5.417 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.952 | clip 0.6\n",
            "INFO: Epoch 006: valid_loss 5.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 193\n",
            "INFO: Epoch 007: loss 5.36 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.202 | clip 0.5\n",
            "INFO: Epoch 007: valid_loss 5.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 183\n",
            "INFO: Epoch 008: loss 5.318 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.332 | clip 0.5\n",
            "INFO: Epoch 008: valid_loss 5.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 174\n",
            "INFO: Epoch 009: loss 5.274 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.715 | clip 0.7\n",
            "INFO: Epoch 009: valid_loss 5.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 166\n",
            "INFO: Epoch 010: loss 5.222 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.345 | clip 0.8\n",
            "INFO: Epoch 010: valid_loss 5.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 158\n",
            "INFO: Epoch 011: loss 5.173 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.61 | clip 0.8\n",
            "INFO: Epoch 011: valid_loss 5.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 151\n",
            "INFO: Epoch 012: loss 5.119 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.289 | clip 0.7\n",
            "INFO: Epoch 012: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 144\n",
            "INFO: Epoch 013: loss 5.084 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.801 | clip 0.8\n",
            "INFO: Epoch 013: valid_loss 4.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 139\n",
            "INFO: Epoch 014: loss 5.037 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.046 | clip 0.7\n",
            "INFO: Epoch 014: valid_loss 4.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 134\n",
            "INFO: Epoch 015: loss 5.01 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.56 | clip 0.7\n",
            "INFO: Epoch 015: valid_loss 4.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 130\n",
            "INFO: Epoch 016: loss 4.968 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.689 | clip 0.6\n",
            "INFO: Epoch 016: valid_loss 4.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 126\n",
            "INFO: Epoch 017: loss 4.947 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.393 | clip 0.7\n",
            "INFO: Epoch 017: valid_loss 4.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 123\n",
            "INFO: Epoch 018: loss 4.905 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.396 | clip 0.5\n",
            "INFO: Epoch 018: valid_loss 4.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 120\n",
            "INFO: Epoch 019: loss 4.89 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.298 | clip 0.7\n",
            "INFO: Epoch 019: valid_loss 4.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 117\n",
            "INFO: Epoch 020: loss 4.848 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.671 | clip 0.5\n",
            "INFO: Epoch 020: valid_loss 4.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 114\n",
            "INFO: Epoch 021: loss 4.839 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.104 | clip 0.7\n",
            "INFO: Epoch 021: valid_loss 4.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112\n",
            "INFO: Epoch 022: loss 4.796 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.172 | clip 0.3\n",
            "INFO: Epoch 022: valid_loss 4.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 109\n",
            "INFO: Epoch 023: loss 4.786 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.82 | clip 0.7\n",
            "INFO: Epoch 023: valid_loss 4.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 106\n",
            "INFO: Epoch 024: loss 4.744 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.901 | clip 0.3\n",
            "INFO: Epoch 024: valid_loss 4.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104\n",
            "INFO: Epoch 025: loss 4.728 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.874 | clip 0.5\n",
            "INFO: Epoch 025: valid_loss 4.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 102\n",
            "INFO: Epoch 026: loss 4.705 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.623 | clip 0.4\n",
            "INFO: Epoch 026: valid_loss 4.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 99.7\n",
            "INFO: Epoch 027: loss 4.679 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.893 | clip 0.3\n",
            "INFO: Epoch 027: valid_loss 4.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98\n",
            "INFO: Epoch 028: loss 4.66 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.968 | clip 0.3\n",
            "INFO: Epoch 028: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.5\n",
            "INFO: Epoch 029: loss 4.645 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.481 | clip 0.4\n",
            "INFO: Epoch 029: valid_loss 4.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 94.8\n",
            "INFO: Epoch 030: loss 4.625 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.382 | clip 0.4\n",
            "INFO: Epoch 030: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.1\n",
            "INFO: Epoch 031: loss 4.605 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.743 | clip 0.3\n",
            "INFO: Epoch 031: valid_loss 4.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.6\n",
            "INFO: Epoch 032: loss 4.587 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.604 | clip 0.3\n",
            "INFO: Epoch 032: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.3\n",
            "INFO: Epoch 033: loss 4.572 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.411 | clip 0.4\n",
            "INFO: Epoch 033: valid_loss 4.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 89.8\n",
            "INFO: Epoch 034: loss 4.571 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.106 | clip 0.7\n",
            "INFO: Epoch 034: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 87.8\n",
            "INFO: Epoch 035: loss 4.531 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.309 | clip 0.2\n",
            "INFO: Epoch 035: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.1\n",
            "INFO: Epoch 036: loss 4.517 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.859 | clip 0.3\n",
            "INFO: Epoch 036: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.5\n",
            "INFO: Epoch 037: loss 4.513 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.793 | clip 0.6\n",
            "INFO: Epoch 037: valid_loss 4.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 83.5\n",
            "INFO: Epoch 038: loss 4.479 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.619 | clip 0.3\n",
            "INFO: Epoch 038: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.9\n",
            "INFO: Epoch 039: loss 4.464 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.884 | clip 0.3\n",
            "INFO: Epoch 039: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.7\n",
            "INFO: Epoch 040: loss 4.45 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.734 | clip 0.5\n",
            "INFO: Epoch 040: valid_loss 4.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.1\n",
            "INFO: Epoch 041: loss 4.427 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.805 | clip 0.3\n",
            "INFO: Epoch 041: valid_loss 4.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 77.7\n",
            "INFO: Epoch 042: loss 4.408 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.584 | clip 0.3\n",
            "INFO: Epoch 042: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.4\n",
            "INFO: Epoch 043: loss 4.393 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.256 | clip 0.5\n",
            "INFO: Epoch 043: valid_loss 4.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.1\n",
            "INFO: Epoch 044: loss 4.376 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.817 | clip 0.5\n",
            "INFO: Epoch 044: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.7\n",
            "INFO: Epoch 045: loss 4.348 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.719 | clip 0.3\n",
            "INFO: Epoch 045: valid_loss 4.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 72.4\n",
            "INFO: Epoch 046: loss 4.333 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.675 | clip 0.2\n",
            "INFO: Epoch 046: valid_loss 4.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.1\n",
            "INFO: Epoch 047: loss 4.316 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.451 | clip 0.5\n",
            "INFO: Epoch 047: valid_loss 4.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 69.7\n",
            "INFO: Epoch 048: loss 4.301 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.844 | clip 0.5\n",
            "INFO: Epoch 048: valid_loss 4.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.6\n",
            "INFO: Epoch 049: loss 4.277 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.501 | clip 0.2\n",
            "INFO: Epoch 049: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.3\n",
            "INFO: Epoch 050: loss 4.257 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.322 | clip 0.2\n",
            "INFO: Epoch 050: valid_loss 4.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 66.3\n",
            "INFO: Epoch 051: loss 4.244 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.439 | clip 0.5\n",
            "INFO: Epoch 051: valid_loss 4.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.1\n",
            "INFO: Epoch 052: loss 4.236 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.242 | clip 0.7\n",
            "INFO: Epoch 052: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.1\n",
            "INFO: Epoch 053: loss 4.207 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.203 | clip 0.2\n",
            "INFO: Epoch 053: valid_loss 4.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 63\n",
            "INFO: Epoch 054: loss 4.187 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.303 | clip 0.2\n",
            "INFO: Epoch 054: valid_loss 4.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.1\n",
            "INFO: Epoch 055: loss 4.18 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.818 | clip 0.5\n",
            "INFO: Epoch 055: valid_loss 4.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.9\n",
            "INFO: Epoch 056: loss 4.17 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.32 | clip 0.8\n",
            "INFO: Epoch 056: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.2\n",
            "INFO: Epoch 057: loss 4.14 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.013 | clip 0.2\n",
            "INFO: Epoch 057: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.1\n",
            "INFO: Epoch 058: loss 4.122 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.319 | clip 0.2\n",
            "INFO: Epoch 058: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58.9\n",
            "INFO: Epoch 059: loss 4.132 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.086 | clip 0.7\n",
            "INFO: Epoch 059: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.5\n",
            "INFO: Epoch 060: loss 4.091 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.946 | clip 0.2\n",
            "INFO: Epoch 060: valid_loss 4.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.5\n",
            "INFO: Epoch 061: loss 4.073 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.921 | clip 0.2\n",
            "INFO: Epoch 061: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.5\n",
            "INFO: Epoch 062: loss 4.065 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.843 | clip 0.4\n",
            "INFO: Epoch 062: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.7\n",
            "INFO: Epoch 063: loss 4.049 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.162 | clip 0.5\n",
            "INFO: Epoch 063: valid_loss 3.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.8\n",
            "INFO: Epoch 064: loss 4.032 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.006 | clip 0.2\n",
            "INFO: Epoch 064: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.6\n",
            "INFO: Epoch 065: loss 4.014 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.189 | clip 0.2\n",
            "INFO: Epoch 065: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.6\n",
            "INFO: Epoch 066: loss 3.999 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.534 | clip 0.5\n",
            "INFO: Epoch 066: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.7\n",
            "INFO: Epoch 067: loss 4.016 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.812 | clip 0.8\n",
            "INFO: Epoch 067: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.3\n",
            "INFO: Epoch 068: loss 3.971 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.997 | clip 0.2\n",
            "INFO: Epoch 068: valid_loss 3.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.6\n",
            "INFO: Epoch 069: loss 3.965 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.08 | clip 0.5\n",
            "INFO: Epoch 069: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.8\n",
            "INFO: Epoch 070: loss 3.987 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.878 | clip 0.9\n",
            "INFO: Epoch 070: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.9\n",
            "INFO: Epoch 071: loss 3.94 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.975 | clip 0.4\n",
            "INFO: Epoch 071: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.3\n",
            "INFO: Epoch 072: loss 3.938 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.08 | clip 0.5\n",
            "INFO: Epoch 072: valid_loss 3.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.8\n",
            "INFO: Epoch 073: loss 3.914 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.259 | clip 0.2\n",
            "INFO: Epoch 073: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.9\n",
            "INFO: Epoch 074: loss 3.895 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.433 | clip 0.2\n",
            "INFO: Epoch 074: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.2\n",
            "INFO: Epoch 075: loss 3.882 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.437 | clip 0.2\n",
            "INFO: Epoch 075: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.7\n",
            "INFO: Epoch 076: loss 3.871 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.858 | clip 0.2\n",
            "INFO: Epoch 076: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.1\n",
            "INFO: Epoch 077: loss 3.861 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.34 | clip 0.3\n",
            "INFO: Epoch 077: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.3\n",
            "INFO: Epoch 078: loss 3.844 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.699 | clip 0.2\n",
            "INFO: Epoch 078: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.1\n",
            "INFO: Epoch 079: loss 3.828 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.399 | clip 0.1\n",
            "INFO: Epoch 079: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.4\n",
            "INFO: Epoch 080: loss 3.818 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.788 | clip 0.2\n",
            "INFO: Epoch 080: valid_loss 3.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.2\n",
            "INFO: Epoch 081: loss 3.81 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.45 | clip 0.4\n",
            "INFO: Epoch 081: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.6\n",
            "INFO: Epoch 082: loss 3.798 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.543 | clip 0.4\n",
            "INFO: Epoch 082: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.8\n",
            "INFO: Epoch 083: loss 3.779 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.642 | clip 0.1\n",
            "INFO: Epoch 083: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.7\n",
            "INFO: Epoch 084: loss 3.764 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.888 | clip 0.1\n",
            "INFO: Epoch 084: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.3\n",
            "INFO: Epoch 085: loss 3.758 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.147 | clip 0.2\n",
            "INFO: Epoch 085: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.1\n",
            "INFO: Epoch 086: loss 3.753 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.767 | clip 0.6\n",
            "INFO: Epoch 086: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.3\n",
            "INFO: Epoch 087: loss 3.753 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.76 | clip 0.8\n",
            "INFO: Epoch 087: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.6\n",
            "INFO: Epoch 088: loss 3.727 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.083 | clip 0.2\n",
            "INFO: Epoch 088: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.7\n",
            "INFO: Epoch 089: loss 3.723 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.519 | clip 0.7\n",
            "INFO: Epoch 089: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.1\n",
            "INFO: Epoch 090: loss 3.79 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 8.69 | clip 0.9\n",
            "INFO: Epoch 090: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.5\n",
            "INFO: Epoch 091: loss 3.727 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.093 | clip 0.7\n",
            "INFO: Epoch 091: valid_loss 3.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.2\n",
            "INFO: Epoch 092: loss 3.733 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.733 | clip 0.9\n",
            "INFO: Epoch 092: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.5\n",
            "INFO: Epoch 093: loss 3.706 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.68 | clip 0.7\n",
            "INFO: Epoch 093: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.7\n",
            "INFO: Epoch 094: loss 3.705 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.156 | clip 0.8\n",
            "INFO: Epoch 094: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.6\n",
            "INFO: Epoch 095: loss 3.675 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.161 | clip 0.1\n",
            "INFO: Epoch 095: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.8\n",
            "INFO: Epoch 096: loss 3.673 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.529 | clip 0.5\n",
            "INFO: Epoch 096: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.6\n",
            "INFO: Epoch 097: loss 3.649 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.738 | clip 0.2\n",
            "INFO: Epoch 097: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.1\n",
            "INFO: Epoch 098: loss 3.63 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.102 | clip 0\n",
            "INFO: Epoch 098: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.8\n",
            "INFO: Epoch 099: loss 3.623 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.359 | clip 0.1\n",
            "INFO: Epoch 099: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.4\n",
            "INFO: Epoch 100: loss 3.614 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.716 | clip 0.2\n",
            "INFO: Epoch 100: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37\n",
            "INFO: Epoch 101: loss 3.608 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.926 | clip 0.2\n",
            "INFO: Epoch 101: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.8\n",
            "INFO: Epoch 102: loss 3.589 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.207 | clip 0\n",
            "INFO: Epoch 102: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.5\n",
            "INFO: Epoch 103: loss 3.581 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.037 | clip 0.2\n",
            "INFO: Epoch 103: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.9\n",
            "INFO: Epoch 104: loss 3.602 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.314 | clip 0.8\n",
            "INFO: Epoch 104: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.7\n",
            "INFO: Epoch 105: loss 3.562 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.952 | clip 0.2\n",
            "INFO: Epoch 105: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.6\n",
            "INFO: Epoch 106: loss 3.558 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.971 | clip 0.6\n",
            "INFO: Epoch 106: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.8\n",
            "INFO: Epoch 107: loss 3.575 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.72 | clip 0.9\n",
            "INFO: Epoch 107: valid_loss 3.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.1\n",
            "INFO: Epoch 108: loss 3.541 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.637 | clip 0.2\n",
            "INFO: Epoch 108: valid_loss 3.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35\n",
            "INFO: Epoch 109: loss 3.534 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.117 | clip 0.4\n",
            "INFO: Epoch 109: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.8\n",
            "INFO: Epoch 110: loss 3.529 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.827 | clip 0.5\n",
            "INFO: Epoch 110: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.7\n",
            "INFO: Epoch 111: loss 3.506 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.473 | clip 0.1\n",
            "INFO: Epoch 111: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.3\n",
            "INFO: Epoch 112: loss 3.507 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4 | clip 0.5\n",
            "INFO: Epoch 112: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.6\n",
            "INFO: Epoch 113: loss 3.543 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 6.963 | clip 0.9\n",
            "INFO: Epoch 113: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.7\n",
            "INFO: Epoch 114: loss 3.501 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.233 | clip 0.7\n",
            "INFO: Epoch 114: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8\n",
            "INFO: Epoch 115: loss 3.507 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.467 | clip 0.8\n",
            "INFO: Epoch 115: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34\n",
            "INFO: Epoch 116: loss 3.466 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.778 | clip 0.1\n",
            "INFO: Epoch 116: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 117: loss 3.47 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.503 | clip 0.4\n",
            "INFO: Epoch 117: valid_loss 3.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.9\n",
            "INFO: Epoch 118: loss 3.448 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.267 | clip 0\n",
            "INFO: Epoch 118: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.6\n",
            "INFO: Epoch 119: loss 3.435 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.188 | clip 0\n",
            "INFO: Epoch 119: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.1\n",
            "INFO: Epoch 120: loss 3.429 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.781 | clip 0.1\n",
            "INFO: Epoch 120: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.8\n",
            "INFO: Epoch 121: loss 3.417 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.47 | clip 0.1\n",
            "INFO: Epoch 121: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.6\n",
            "INFO: Epoch 122: loss 3.403 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.143 | clip 0\n",
            "INFO: Epoch 122: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 123: loss 3.394 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.273 | clip 0.2\n",
            "INFO: Epoch 123: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 124: loss 3.406 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.777 | clip 0.7\n",
            "INFO: Epoch 124: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 125: loss 3.375 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.486 | clip 0.2\n",
            "INFO: Epoch 125: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.5\n",
            "INFO: Epoch 126: loss 3.37 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.929 | clip 0.5\n",
            "INFO: Epoch 126: valid_loss 3.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.7\n",
            "INFO: Epoch 127: loss 3.389 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.333 | clip 0.8\n",
            "INFO: Epoch 127: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.8\n",
            "INFO: Epoch 128: loss 3.355 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.577 | clip 0.4\n",
            "INFO: Epoch 128: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.9\n",
            "INFO: Epoch 129: loss 3.345 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.21 | clip 0.4\n",
            "INFO: Epoch 129: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 130: loss 3.338 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.816 | clip 0.2\n",
            "INFO: Epoch 130: valid_loss 3.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.5\n",
            "INFO: Epoch 131: loss 3.318 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.104 | clip 0\n",
            "INFO: Epoch 131: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.2\n",
            "INFO: Epoch 132: loss 3.309 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.786 | clip 0.1\n",
            "INFO: Epoch 132: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.1\n",
            "INFO: Epoch 133: loss 3.313 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.821 | clip 0.4\n",
            "INFO: Epoch 133: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 134: loss 3.288 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.436 | clip 0.1\n",
            "INFO: Epoch 134: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.6\n",
            "INFO: Epoch 135: loss 3.285 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.079 | clip 0.2\n",
            "INFO: Epoch 135: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 136: loss 3.313 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.599 | clip 0.8\n",
            "INFO: Epoch 136: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.7\n",
            "INFO: Epoch 137: loss 3.267 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.063 | clip 0.2\n",
            "INFO: Epoch 137: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.9\n",
            "INFO: Epoch 138: loss 3.263 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.034 | clip 0.5\n",
            "INFO: Epoch 138: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.9\n",
            "INFO: Epoch 139: loss 3.264 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.844 | clip 0.2\n",
            "INFO: Epoch 139: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.9\n",
            "INFO: Epoch 140: loss 3.242 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.919 | clip 0.1\n",
            "INFO: Epoch 140: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 141: loss 3.231 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.308 | clip 0.1\n",
            "INFO: Epoch 141: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.3\n",
            "INFO: Epoch 142: loss 3.239 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.035 | clip 0.5\n",
            "INFO: Epoch 142: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.2\n",
            "INFO: Epoch 143: loss 3.216 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.786 | clip 0.2\n",
            "INFO: Epoch 143: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.8\n",
            "INFO: Epoch 144: loss 3.218 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.32 | clip 0.5\n",
            "INFO: Epoch 144: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.9\n",
            "INFO: Epoch 145: loss 3.233 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.628 | clip 0.9\n",
            "INFO: Epoch 145: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.2\n",
            "INFO: Epoch 146: loss 3.199 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.344 | clip 0.1\n",
            "INFO: Epoch 146: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 147: loss 3.195 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.1 | clip 0.4\n",
            "INFO: Epoch 147: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 148: loss 3.179 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.891 | clip 0.1\n",
            "INFO: Epoch 148: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26\n",
            "INFO: Epoch 149: loss 3.166 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.168 | clip 0.1\n",
            "INFO: Epoch 149: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 150: loss 3.161 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.098 | clip 0.1\n",
            "INFO: Epoch 150: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 151: loss 3.171 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.539 | clip 0.6\n",
            "INFO: Epoch 151: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.7\n",
            "INFO: Epoch 152: loss 3.146 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.932 | clip 0.2\n",
            "INFO: Epoch 152: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.1\n",
            "INFO: Epoch 153: loss 3.142 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.333 | clip 0.2\n",
            "INFO: Epoch 153: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 154: loss 3.145 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.434 | clip 0.6\n",
            "INFO: Epoch 154: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 155: loss 3.121 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.869 | clip 0.2\n",
            "INFO: Epoch 155: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.7\n",
            "INFO: Epoch 156: loss 3.118 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.968 | clip 0.5\n",
            "INFO: Epoch 156: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 157: loss 3.133 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.252 | clip 0.8\n",
            "INFO: Epoch 157: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 158: loss 3.104 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.108 | clip 0.1\n",
            "INFO: Epoch 158: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 159: loss 3.095 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.654 | clip 0.3\n",
            "INFO: Epoch 159: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 160: loss 3.089 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.234 | clip 0.1\n",
            "INFO: Epoch 160: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.1\n",
            "INFO: Epoch 161: loss 3.071 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.308 | clip 0.1\n",
            "INFO: Epoch 161: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 162: loss 3.067 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.182 | clip 0.1\n",
            "INFO: Epoch 162: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.9\n",
            "INFO: Epoch 163: loss 3.076 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.246 | clip 0.5\n",
            "INFO: Epoch 163: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 164: loss 3.051 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.753 | clip 0.1\n",
            "INFO: Epoch 164: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 165: loss 3.047 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.419 | clip 0.2\n",
            "INFO: Epoch 165: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 166: loss 3.05 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.09 | clip 0.3\n",
            "INFO: Epoch 166: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2\n",
            "INFO: Epoch 167: loss 3.028 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.952 | clip 0.2\n",
            "INFO: Epoch 167: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.9\n",
            "INFO: Epoch 168: loss 3.026 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.8 | clip 0.5\n",
            "INFO: Epoch 168: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 169: loss 3.041 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.75 | clip 0.8\n",
            "INFO: Epoch 169: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.3\n",
            "INFO: Epoch 170: loss 3.009 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.093 | clip 0.1\n",
            "INFO: Epoch 170: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.5\n",
            "INFO: Epoch 171: loss 3.009 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.886 | clip 0.3\n",
            "INFO: Epoch 171: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 172: loss 2.994 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.834 | clip 0.1\n",
            "INFO: Epoch 172: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.4\n",
            "INFO: Epoch 173: loss 2.987 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.272 | clip 0.1\n",
            "INFO: Epoch 173: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 174: loss 2.975 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.81 | clip 0.1\n",
            "INFO: Epoch 174: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.1\n",
            "INFO: Epoch 175: loss 2.98 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.914 | clip 0.3\n",
            "INFO: Epoch 175: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 176: loss 2.966 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.721 | clip 0.2\n",
            "INFO: Epoch 176: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6\n",
            "INFO: Epoch 177: loss 2.956 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.46 | clip 0.3\n",
            "INFO: Epoch 177: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.7\n",
            "INFO: Epoch 178: loss 2.97 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.765 | clip 0.7\n",
            "INFO: Epoch 178: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 179: loss 2.941 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.106 | clip 0.2\n",
            "INFO: Epoch 179: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 180: loss 2.942 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.103 | clip 0.4\n",
            "INFO: Epoch 180: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 181: loss 2.94 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.908 | clip 0.3\n",
            "INFO: Epoch 181: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 182: loss 2.92 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.684 | clip 0.1\n",
            "INFO: Epoch 182: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 183: loss 2.916 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.328 | clip 0.3\n",
            "INFO: Epoch 183: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 184: loss 2.921 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.918 | clip 0.2\n",
            "INFO: Epoch 184: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 185: loss 2.9 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.875 | clip 0.2\n",
            "INFO: Epoch 185: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 186: loss 2.893 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.649 | clip 0.4\n",
            "INFO: Epoch 186: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 187: loss 2.906 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.829 | clip 0.6\n",
            "INFO: Epoch 187: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 188: loss 2.881 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.063 | clip 0.2\n",
            "INFO: Epoch 188: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 189: loss 2.877 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.693 | clip 0.3\n",
            "INFO: Epoch 189: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 190: loss 2.874 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.757 | clip 0.2\n",
            "INFO: Epoch 190: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 191: loss 2.857 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.738 | clip 0.1\n",
            "INFO: Epoch 191: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 192: loss 2.852 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.489 | clip 0.3\n",
            "INFO: Epoch 192: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1\n",
            "INFO: Epoch 193: loss 2.86 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.38 | clip 0.6\n",
            "INFO: Epoch 193: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1\n",
            "INFO: Epoch 194: loss 2.839 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.898 | clip 0.1\n",
            "INFO: Epoch 194: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7\n",
            "INFO: Epoch 195: loss 2.832 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.218 | clip 0.1\n",
            "INFO: Epoch 195: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7\n",
            "INFO: Epoch 196: loss 2.839 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.281 | clip 0.4\n",
            "INFO: Epoch 196: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.7\n",
            "INFO: Epoch 197: loss 2.82 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.875 | clip 0.1\n",
            "INFO: Epoch 197: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 198: loss 2.816 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.599 | clip 0.3\n",
            "INFO: Epoch 198: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 199: loss 2.822 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.382 | clip 0.3\n",
            "INFO: Epoch 199: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 200: loss 2.803 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.821 | clip 0.2\n",
            "INFO: Epoch 200: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 201: loss 2.796 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.687 | clip 0.2\n",
            "INFO: Epoch 201: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 202: loss 2.8 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.969 | clip 0.3\n",
            "INFO: Epoch 202: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 203: loss 2.782 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.695 | clip 0.1\n",
            "INFO: Epoch 203: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 204: loss 2.775 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.484 | clip 0.3\n",
            "INFO: Epoch 204: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19\n",
            "INFO: Epoch 205: loss 2.783 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.501 | clip 0.4\n",
            "INFO: Epoch 205: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 206: loss 2.762 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.753 | clip 0.1\n",
            "INFO: Epoch 206: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 207: loss 2.753 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.387 | clip 0.3\n",
            "INFO: Epoch 207: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 208: loss 2.761 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.083 | clip 0.4\n",
            "INFO: Epoch 208: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 209: loss 2.744 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.971 | clip 0.1\n",
            "INFO: Epoch 209: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 210: loss 2.738 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.539 | clip 0.3\n",
            "INFO: Epoch 210: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 211: loss 2.743 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.44 | clip 0.3\n",
            "INFO: Epoch 211: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 212: loss 2.726 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.849 | clip 0.1\n",
            "INFO: Epoch 212: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 213: loss 2.72 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.481 | clip 0.2\n",
            "INFO: Epoch 213: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 214: loss 2.722 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.421 | clip 0.3\n",
            "INFO: Epoch 214: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 215: loss 2.706 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.258 | clip 0.2\n",
            "INFO: Epoch 215: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 216: loss 2.704 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.996 | clip 0.4\n",
            "INFO: Epoch 216: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 217: loss 2.705 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.189 | clip 0.3\n",
            "INFO: Epoch 217: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 218: loss 2.689 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.857 | clip 0.2\n",
            "INFO: Epoch 218: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 219: loss 2.679 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.549 | clip 0.3\n",
            "INFO: Epoch 219: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 220: loss 2.685 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.854 | clip 0.2\n",
            "INFO: Epoch 220: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 221: loss 2.668 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.825 | clip 0.2\n",
            "INFO: Epoch 221: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 222: loss 2.667 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.612 | clip 0.3\n",
            "INFO: Epoch 222: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 223: loss 2.665 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.876 | clip 0.3\n",
            "INFO: Epoch 223: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 224: loss 2.652 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.797 | clip 0.2\n",
            "INFO: Epoch 224: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 225: loss 2.647 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.36 | clip 0.2\n",
            "INFO: Epoch 225: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 226: loss 2.645 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.88 | clip 0.3\n",
            "INFO: Epoch 226: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 227: loss 2.634 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.047 | clip 0.2\n",
            "INFO: Epoch 227: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 228: loss 2.627 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.88 | clip 0.3\n",
            "INFO: Epoch 228: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 229: loss 2.637 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.156 | clip 0.2\n",
            "INFO: Epoch 229: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 230: loss 2.619 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.691 | clip 0.1\n",
            "INFO: Epoch 230: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 231: loss 2.609 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.243 | clip 0.1\n",
            "INFO: Epoch 231: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 232: loss 2.618 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.307 | clip 0.4\n",
            "INFO: Epoch 232: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 233: loss 2.603 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.193 | clip 0.2\n",
            "INFO: Epoch 233: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 234: loss 2.596 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.796 | clip 0.3\n",
            "INFO: Epoch 234: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 235: loss 2.605 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.429 | clip 0.3\n",
            "INFO: Epoch 235: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 236: loss 2.585 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.097 | clip 0.2\n",
            "INFO: Epoch 236: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 237: loss 2.58 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.541 | clip 0.3\n",
            "INFO: Epoch 237: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 238: loss 2.581 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.968 | clip 0.2\n",
            "INFO: Epoch 238: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 239: loss 2.568 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.001 | clip 0.2\n",
            "INFO: Epoch 239: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 240: loss 2.568 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.752 | clip 0.3\n",
            "INFO: Epoch 240: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 241: loss 2.568 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.543 | clip 0.3\n",
            "INFO: Epoch 241: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 242: loss 2.553 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.087 | clip 0.2\n",
            "INFO: Epoch 242: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 243: loss 2.544 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.895 | clip 0.3\n",
            "INFO: Epoch 243: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 244: loss 2.554 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.394 | clip 0.4\n",
            "INFO: Epoch 244: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 245: loss 2.533 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.068 | clip 0.2\n",
            "INFO: Epoch 245: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 246: loss 2.531 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.883 | clip 0.3\n",
            "INFO: Epoch 246: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 247: loss 2.533 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.811 | clip 0.2\n",
            "INFO: Epoch 247: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 248: loss 2.513 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.777 | clip 0.1\n",
            "INFO: Epoch 248: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 249: loss 2.515 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.528 | clip 0.3\n",
            "INFO: Epoch 249: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 250: loss 2.52 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.58 | clip 0.5\n",
            "INFO: Epoch 250: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 251: loss 2.504 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.39 | clip 0.2\n",
            "INFO: Epoch 251: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 252: loss 2.503 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.015 | clip 0.3\n",
            "INFO: Epoch 252: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 253: loss 2.506 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.484 | clip 0.3\n",
            "INFO: Epoch 253: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 254: loss 2.491 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.96 | clip 0.1\n",
            "INFO: Epoch 254: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 255: loss 2.481 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.922 | clip 0.3\n",
            "INFO: Epoch 255: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 256: loss 2.486 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.131 | clip 0.3\n",
            "INFO: Epoch 256: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 257: loss 2.477 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.265 | clip 0.2\n",
            "INFO: Epoch 257: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 258: loss 2.471 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.434 | clip 0.3\n",
            "INFO: Epoch 258: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 259: loss 2.467 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.827 | clip 0.2\n",
            "INFO: Epoch 259: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 260: loss 2.455 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.874 | clip 0.2\n",
            "INFO: Epoch 260: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 261: loss 2.45 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.438 | clip 0.3\n",
            "INFO: Epoch 261: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 262: loss 2.458 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.426 | clip 0.3\n",
            "INFO: Epoch 262: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 263: loss 2.442 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.36 | clip 0.2\n",
            "INFO: Epoch 263: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 264: loss 2.44 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.913 | clip 0.3\n",
            "INFO: Epoch 264: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 265: loss 2.441 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.905 | clip 0.3\n",
            "INFO: Epoch 265: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 266: loss 2.423 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.983 | clip 0.1\n",
            "INFO: Epoch 266: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 267: loss 2.423 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.278 | clip 0.1\n",
            "INFO: Epoch 267: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 268: loss 2.427 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.954 | clip 0.2\n",
            "INFO: Epoch 268: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 269: loss 2.412 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.985 | clip 0.2\n",
            "INFO: Epoch 269: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 270: loss 2.407 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.597 | clip 0.3\n",
            "INFO: Epoch 270: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 271: loss 2.412 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.33 | clip 0.2\n",
            "INFO: Epoch 271: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 272: loss 2.395 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.191 | clip 0.3\n",
            "INFO: Epoch 272: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 273: loss 2.393 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.118 | clip 0.4\n",
            "INFO: Epoch 273: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 274: loss 2.405 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.727 | clip 0.4\n",
            "INFO: Epoch 274: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 275: loss 2.388 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.318 | clip 0.2\n",
            "INFO: Epoch 275: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 276: loss 2.375 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.773 | clip 0.3\n",
            "INFO: Epoch 276: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 277: loss 2.385 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.272 | clip 0.3\n",
            "INFO: Epoch 277: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 278: loss 2.365 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.964 | clip 0.2\n",
            "INFO: Epoch 278: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 279: loss 2.362 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.721 | clip 0.2\n",
            "INFO: Epoch 279: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 280: loss 2.373 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.659 | clip 0.4\n",
            "INFO: Epoch 280: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 281: loss 2.355 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.255 | clip 0.2\n",
            "INFO: Epoch 281: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 282: loss 2.354 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.079 | clip 0.3\n",
            "INFO: Epoch 282: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 283: loss 2.351 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.748 | clip 0.2\n",
            "INFO: Epoch 283: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 284: loss 2.342 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.016 | clip 0.1\n",
            "INFO: Epoch 284: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 285: loss 2.339 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.551 | clip 0.3\n",
            "INFO: Epoch 285: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 286: loss 2.341 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.859 | clip 0.2\n",
            "INFO: Epoch 286: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 287: loss 2.327 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.317 | clip 0.2\n",
            "INFO: Epoch 287: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 288: loss 2.323 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.935 | clip 0.4\n",
            "INFO: Epoch 288: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 289: loss 2.332 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.659 | clip 0.4\n",
            "INFO: Epoch 289: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 290: loss 2.314 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.295 | clip 0.3\n",
            "INFO: Epoch 290: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 291: loss 2.308 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.194 | clip 0.3\n",
            "INFO: Epoch 291: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 292: loss 2.311 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.126 | clip 0.2\n",
            "INFO: Epoch 292: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 293: loss 2.299 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.229 | clip 0.2\n",
            "INFO: Epoch 293: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 294: loss 2.299 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.075 | clip 0.4\n",
            "INFO: Epoch 294: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 295: loss 2.303 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.453 | clip 0.3\n",
            "INFO: Epoch 295: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 296: loss 2.288 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.193 | clip 0.1\n",
            "INFO: Epoch 296: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 297: loss 2.275 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.067 | clip 0.3\n",
            "INFO: Epoch 297: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 298: loss 2.279 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.684 | clip 0.2\n",
            "INFO: Epoch 298: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 299: loss 2.27 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 2.99 | clip 0.2\n",
            "INFO: Epoch 299: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 300: loss 2.268 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.794 | clip 0.3\n",
            "INFO: Epoch 300: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 301: loss 2.271 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.018 | clip 0.3\n",
            "INFO: Epoch 301: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 302: loss 2.261 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.25 | clip 0.3\n",
            "INFO: Epoch 302: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 303: loss 2.257 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.04 | clip 0.3\n",
            "INFO: Epoch 303: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 304: loss 2.257 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.181 | clip 0.3\n",
            "INFO: Epoch 304: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 305: loss 2.249 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.578 | clip 0.2\n",
            "INFO: Epoch 305: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 306: loss 2.247 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.389 | clip 0.4\n",
            "INFO: Epoch 306: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 307: loss 2.247 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.084 | clip 0.2\n",
            "INFO: Epoch 307: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 308: loss 2.232 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.057 | clip 0.2\n",
            "INFO: Epoch 308: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 309: loss 2.236 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.99 | clip 0.3\n",
            "INFO: Epoch 309: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 310: loss 2.235 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.977 | clip 0.3\n",
            "INFO: Epoch 310: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 311: loss 2.224 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.303 | clip 0.2\n",
            "INFO: Epoch 311: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 312: loss 2.218 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.779 | clip 0.2\n",
            "INFO: Epoch 312: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 313: loss 2.215 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.093 | clip 0.2\n",
            "INFO: Epoch 313: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 314: loss 2.208 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.345 | clip 0.3\n",
            "INFO: Epoch 314: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 315: loss 2.209 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.371 | clip 0.4\n",
            "INFO: Epoch 315: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 316: loss 2.215 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.134 | clip 0.6\n",
            "INFO: Epoch 316: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 317: loss 2.199 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.622 | clip 0.3\n",
            "INFO: Epoch 317: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 318: loss 2.202 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.549 | clip 0.4\n",
            "INFO: Epoch 318: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 319: loss 2.199 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.007 | clip 0.2\n",
            "INFO: Epoch 319: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 320: loss 2.18 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.007 | clip 0.2\n",
            "INFO: Epoch 320: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 321: loss 2.177 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.547 | clip 0.3\n",
            "INFO: Epoch 321: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 322: loss 2.184 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.937 | clip 0.3\n",
            "INFO: Epoch 322: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 323: loss 2.169 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.203 | clip 0.2\n",
            "INFO: Epoch 323: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 324: loss 2.167 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.607 | clip 0.3\n",
            "INFO: Epoch 324: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 325: loss 2.171 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.082 | clip 0.2\n",
            "INFO: Epoch 325: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 326: loss 2.162 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.066 | clip 0.2\n",
            "INFO: Epoch 326: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 327: loss 2.154 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.704 | clip 0.2\n",
            "INFO: Epoch 327: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 328: loss 2.161 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.322 | clip 0.3\n",
            "INFO: Epoch 328: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 329: loss 2.15 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.444 | clip 0.2\n",
            "INFO: Epoch 329: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 330: loss 2.149 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.969 | clip 0.3\n",
            "INFO: Epoch 330: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 331: loss 2.148 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.336 | clip 0.2\n",
            "INFO: Epoch 331: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 332: loss 2.134 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.231 | clip 0.3\n",
            "INFO: Epoch 332: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 333: loss 2.139 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.224 | clip 0.4\n",
            "INFO: Epoch 333: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 334: loss 2.143 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 5.013 | clip 0.5\n",
            "INFO: Epoch 334: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 335: loss 2.129 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.56 | clip 0.3\n",
            "INFO: Epoch 335: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 336: loss 2.131 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.239 | clip 0.4\n",
            "INFO: Epoch 336: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 337: loss 2.128 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.145 | clip 0.3\n",
            "INFO: Epoch 337: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 338: loss 2.111 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.096 | clip 0.2\n",
            "INFO: Epoch 338: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 339: loss 2.108 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.599 | clip 0.2\n",
            "INFO: Epoch 339: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 340: loss 2.109 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.884 | clip 0.2\n",
            "INFO: Epoch 340: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 341: loss 2.104 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.282 | clip 0.2\n",
            "INFO: Epoch 341: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 342: loss 2.1 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.298 | clip 0.4\n",
            "INFO: Epoch 342: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 343: loss 2.106 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.298 | clip 0.3\n",
            "INFO: Epoch 343: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 344: loss 2.09 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.272 | clip 0.2\n",
            "INFO: Epoch 344: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 345: loss 2.091 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.18 | clip 0.4\n",
            "INFO: Epoch 345: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 346: loss 2.094 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.32 | clip 0.3\n",
            "INFO: Epoch 346: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 347: loss 2.081 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.779 | clip 0.3\n",
            "INFO: Epoch 347: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 348: loss 2.082 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.821 | clip 0.4\n",
            "INFO: Epoch 348: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 349: loss 2.076 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.655 | clip 0.1\n",
            "INFO: Epoch 349: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 350: loss 2.066 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.154 | clip 0.2\n",
            "INFO: Epoch 350: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 351: loss 2.065 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.779 | clip 0.4\n",
            "INFO: Epoch 351: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 352: loss 2.069 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.027 | clip 0.2\n",
            "INFO: Epoch 352: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 353: loss 2.059 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.411 | clip 0.3\n",
            "INFO: Epoch 353: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 354: loss 2.058 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.254 | clip 0.3\n",
            "INFO: Epoch 354: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 355: loss 2.056 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.193 | clip 0.4\n",
            "INFO: Epoch 355: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 356: loss 2.048 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.511 | clip 0.3\n",
            "INFO: Epoch 356: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 357: loss 2.043 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.977 | clip 0.5\n",
            "INFO: Epoch 357: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 358: loss 2.052 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.675 | clip 0.3\n",
            "INFO: Epoch 358: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 359: loss 2.038 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.855 | clip 0.3\n",
            "INFO: Epoch 359: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 360: loss 2.037 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.728 | clip 0.4\n",
            "INFO: Epoch 360: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 361: loss 2.042 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.507 | clip 0.5\n",
            "INFO: Epoch 361: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 362: loss 2.031 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.344 | clip 0.3\n",
            "INFO: Epoch 362: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 363: loss 2.028 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.089 | clip 0.3\n",
            "INFO: Epoch 363: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 364: loss 2.03 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.315 | clip 0.3\n",
            "INFO: Epoch 364: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 365: loss 2.017 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.532 | clip 0.2\n",
            "INFO: Epoch 365: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 366: loss 2.011 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.189 | clip 0.3\n",
            "INFO: Epoch 366: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 367: loss 2.021 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.238 | clip 0.3\n",
            "INFO: Epoch 367: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 368: loss 2.01 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.38 | clip 0.2\n",
            "INFO: Epoch 368: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 369: loss 2.002 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.157 | clip 0.4\n",
            "INFO: Epoch 369: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 370: loss 2.004 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.252 | clip 0.3\n",
            "INFO: Epoch 370: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 371: loss 1.997 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.588 | clip 0.2\n",
            "INFO: Epoch 371: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 372: loss 1.991 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.213 | clip 0.4\n",
            "INFO: Epoch 372: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 373: loss 2.001 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.415 | clip 0.3\n",
            "INFO: Epoch 373: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 374: loss 1.989 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.375 | clip 0.3\n",
            "INFO: Epoch 374: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 375: loss 1.985 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.952 | clip 0.3\n",
            "INFO: Epoch 375: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 376: loss 1.986 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.1 | clip 0.2\n",
            "INFO: Epoch 376: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 377: loss 1.97 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.636 | clip 0.3\n",
            "INFO: Epoch 377: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 378: loss 1.971 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.615 | clip 0.4\n",
            "INFO: Epoch 378: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 379: loss 1.977 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.333 | clip 0.2\n",
            "INFO: Epoch 379: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 380: loss 1.966 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.504 | clip 0.3\n",
            "INFO: Epoch 380: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 381: loss 1.964 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.421 | clip 0.4\n",
            "INFO: Epoch 381: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 382: loss 1.968 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.274 | clip 0.3\n",
            "INFO: Epoch 382: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 383: loss 1.959 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.72 | clip 0.2\n",
            "INFO: Epoch 383: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 384: loss 1.954 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.393 | clip 0.3\n",
            "INFO: Epoch 384: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 385: loss 1.955 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.162 | clip 0.3\n",
            "INFO: Epoch 385: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 386: loss 1.945 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.314 | clip 0.2\n",
            "INFO: Epoch 386: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 387: loss 1.942 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.948 | clip 0.3\n",
            "INFO: Epoch 387: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 388: loss 1.945 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.844 | clip 0.3\n",
            "INFO: Epoch 388: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 389: loss 1.934 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.46 | clip 0.2\n",
            "INFO: Epoch 389: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 390: loss 1.936 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.813 | clip 0.3\n",
            "INFO: Epoch 390: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 391: loss 1.931 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.969 | clip 0.2\n",
            "INFO: Epoch 391: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 392: loss 1.924 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.391 | clip 0.3\n",
            "INFO: Epoch 392: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 393: loss 1.926 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.143 | clip 0.4\n",
            "INFO: Epoch 393: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 394: loss 1.918 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 4.483 | clip 0.4\n",
            "INFO: Epoch 394: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 395: loss 1.918 | lr 0.0004 | num_tokens 9.1 | batch_size 1000 | grad_norm 3.92 | clip 0.3\n",
            "INFO: Epoch 395: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E19 bs100+lr0.0005+dropout0.3"
      ],
      "metadata": {
        "id": "EB5cL6bV137l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_19/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 100 \\\n",
        "    --lr 0.0005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dOBuDQF2Fl5",
        "outputId": "33ff307b-0ab4-4e91-ac92-5c4fe745a069"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_19/checkpoints --cuda --batch-size 100 --lr 0.0005\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 100, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_19/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 6.219 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.507 | clip 0.86\n",
            "INFO: Epoch 000: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169\n",
            "INFO: Epoch 001: loss 4.971 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.593 | clip 0.91\n",
            "INFO: Epoch 001: valid_loss 5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 148\n",
            "INFO: Epoch 002: loss 4.805 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.487 | clip 0.78\n",
            "INFO: Epoch 002: valid_loss 4.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 133\n",
            "INFO: Epoch 003: loss 4.699 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.126 | clip 0.79\n",
            "INFO: Epoch 003: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 115\n",
            "INFO: Epoch 004: loss 4.585 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.828 | clip 0.77\n",
            "INFO: Epoch 004: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 99\n",
            "INFO: Epoch 005: loss 4.46 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.682 | clip 0.74\n",
            "INFO: Epoch 005: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.4\n",
            "INFO: Epoch 006: loss 4.355 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.551 | clip 0.73\n",
            "INFO: Epoch 006: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.7\n",
            "INFO: Epoch 007: loss 4.258 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.387 | clip 0.73\n",
            "INFO: Epoch 007: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.9\n",
            "INFO: Epoch 008: loss 4.164 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.311 | clip 0.72\n",
            "INFO: Epoch 008: valid_loss 4.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67\n",
            "INFO: Epoch 009: loss 4.075 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.173 | clip 0.69\n",
            "INFO: Epoch 009: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.5\n",
            "INFO: Epoch 010: loss 3.997 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.158 | clip 0.68\n",
            "INFO: Epoch 010: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.1\n",
            "INFO: Epoch 011: loss 3.923 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.092 | clip 0.69\n",
            "INFO: Epoch 011: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.2\n",
            "INFO: Epoch 012: loss 3.851 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.091 | clip 0.67\n",
            "INFO: Epoch 012: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.9\n",
            "INFO: Epoch 013: loss 3.792 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.097 | clip 0.67\n",
            "INFO: Epoch 013: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.9\n",
            "INFO: Epoch 014: loss 3.723 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.115 | clip 0.67\n",
            "INFO: Epoch 014: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.2\n",
            "INFO: Epoch 015: loss 3.669 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.126 | clip 0.65\n",
            "INFO: Epoch 015: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.8\n",
            "INFO: Epoch 016: loss 3.607 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.087 | clip 0.67\n",
            "INFO: Epoch 016: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.9\n",
            "INFO: Epoch 017: loss 3.558 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.114 | clip 0.69\n",
            "INFO: Epoch 017: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.9\n",
            "INFO: Epoch 018: loss 3.502 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.064 | clip 0.73\n",
            "INFO: Epoch 018: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.4\n",
            "INFO: Epoch 019: loss 3.453 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.055 | clip 0.71\n",
            "INFO: Epoch 019: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.6\n",
            "INFO: Epoch 020: loss 3.403 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.104 | clip 0.71\n",
            "INFO: Epoch 020: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 021: loss 3.354 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.105 | clip 0.72\n",
            "INFO: Epoch 021: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.5\n",
            "INFO: Epoch 022: loss 3.307 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.175 | clip 0.76\n",
            "INFO: Epoch 022: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 023: loss 3.265 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.147 | clip 0.72\n",
            "INFO: Epoch 023: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.7\n",
            "INFO: Epoch 024: loss 3.216 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.182 | clip 0.74\n",
            "INFO: Epoch 024: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 025: loss 3.175 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.208 | clip 0.75\n",
            "INFO: Epoch 025: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.2\n",
            "INFO: Epoch 026: loss 3.138 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.232 | clip 0.79\n",
            "INFO: Epoch 026: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 027: loss 3.095 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.246 | clip 0.79\n",
            "INFO: Epoch 027: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 028: loss 3.057 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.298 | clip 0.79\n",
            "INFO: Epoch 028: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 029: loss 3.017 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.281 | clip 0.76\n",
            "INFO: Epoch 029: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 030: loss 2.981 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.365 | clip 0.75\n",
            "INFO: Epoch 030: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 031: loss 2.947 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.449 | clip 0.79\n",
            "INFO: Epoch 031: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 032: loss 2.925 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.554 | clip 0.81\n",
            "INFO: Epoch 032: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 033: loss 2.879 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.418 | clip 0.82\n",
            "INFO: Epoch 033: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 034: loss 2.854 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.55 | clip 0.81\n",
            "INFO: Epoch 034: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 035: loss 2.822 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.584 | clip 0.82\n",
            "INFO: Epoch 035: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 036: loss 2.794 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.596 | clip 0.84\n",
            "INFO: Epoch 036: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 037: loss 2.767 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.682 | clip 0.85\n",
            "INFO: Epoch 037: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 038: loss 2.742 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.636 | clip 0.81\n",
            "INFO: Epoch 038: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 039: loss 2.707 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.69 | clip 0.8\n",
            "INFO: Epoch 039: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 040: loss 2.688 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.752 | clip 0.83\n",
            "INFO: Epoch 040: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 041: loss 2.658 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.702 | clip 0.82\n",
            "INFO: Epoch 041: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 042: loss 2.634 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.871 | clip 0.85\n",
            "INFO: Epoch 042: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 043: loss 2.61 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.843 | clip 0.83\n",
            "INFO: Epoch 043: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 044: loss 2.583 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.989 | clip 0.83\n",
            "INFO: Epoch 044: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 045: loss 2.566 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.038 | clip 0.87\n",
            "INFO: Epoch 045: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 046: loss 2.543 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.938 | clip 0.84\n",
            "INFO: Epoch 046: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 047: loss 2.517 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.07 | clip 0.85\n",
            "INFO: Epoch 047: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 048: loss 2.501 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.116 | clip 0.88\n",
            "INFO: Epoch 048: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 049: loss 2.478 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.104 | clip 0.85\n",
            "INFO: Epoch 049: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 050: loss 2.454 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.162 | clip 0.88\n",
            "INFO: Epoch 050: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 051: loss 2.434 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.252 | clip 0.86\n",
            "INFO: Epoch 051: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 052: loss 2.412 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.226 | clip 0.87\n",
            "INFO: Epoch 052: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 053: loss 2.39 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.279 | clip 0.84\n",
            "INFO: Epoch 053: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 054: loss 2.373 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.252 | clip 0.89\n",
            "INFO: Epoch 054: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 055: loss 2.355 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.525 | clip 0.84\n",
            "INFO: Epoch 055: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 056: loss 2.334 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.352 | clip 0.84\n",
            "INFO: Epoch 056: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 057: loss 2.317 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.365 | clip 0.88\n",
            "INFO: Epoch 057: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 058: loss 2.301 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.362 | clip 0.9\n",
            "INFO: Epoch 058: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 059: loss 2.277 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.491 | clip 0.89\n",
            "INFO: Epoch 059: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 060: loss 2.268 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.467 | clip 0.93\n",
            "INFO: Epoch 060: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 061: loss 2.243 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.46 | clip 0.86\n",
            "INFO: Epoch 061: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 062: loss 2.23 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.603 | clip 0.88\n",
            "INFO: Epoch 062: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 063: loss 2.209 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.626 | clip 0.91\n",
            "INFO: Epoch 063: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 064: loss 2.198 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.603 | clip 0.89\n",
            "INFO: Epoch 064: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 065: loss 2.176 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.716 | clip 0.89\n",
            "INFO: Epoch 065: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 066: loss 2.161 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.666 | clip 0.91\n",
            "INFO: Epoch 066: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 067: loss 2.148 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.898 | clip 0.91\n",
            "INFO: Epoch 067: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 068: loss 2.136 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.853 | clip 0.92\n",
            "INFO: Epoch 068: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 069: loss 2.113 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.855 | clip 0.92\n",
            "INFO: Epoch 069: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 070: loss 2.101 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.837 | clip 0.9\n",
            "INFO: Epoch 070: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 071: loss 2.084 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.873 | clip 0.91\n",
            "INFO: Epoch 071: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 072: loss 2.072 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.936 | clip 0.91\n",
            "INFO: Epoch 072: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 073: loss 2.059 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.015 | clip 0.93\n",
            "INFO: Epoch 073: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 074: loss 2.045 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.933 | clip 0.93\n",
            "INFO: Epoch 074: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 075: loss 2.027 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.026 | clip 0.9\n",
            "INFO: Epoch 075: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 076: loss 2.013 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.07 | clip 0.92\n",
            "INFO: Epoch 076: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 077: loss 2.004 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.967 | clip 0.93\n",
            "INFO: Epoch 077: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 078: loss 1.989 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.025 | clip 0.94\n",
            "INFO: Epoch 078: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 079: loss 1.975 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.174 | clip 0.93\n",
            "INFO: Epoch 079: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 080: loss 1.966 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.111 | clip 0.94\n",
            "INFO: Epoch 080: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 081: loss 1.948 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.158 | clip 0.93\n",
            "INFO: Epoch 081: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 082: loss 1.941 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.158 | clip 0.94\n",
            "INFO: Epoch 082: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 083: loss 1.929 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.276 | clip 0.92\n",
            "INFO: Epoch 083: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 084: loss 1.916 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.172 | clip 0.95\n",
            "INFO: Epoch 084: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 085: loss 1.899 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.299 | clip 0.94\n",
            "INFO: Epoch 085: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 086: loss 1.893 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.248 | clip 0.93\n",
            "INFO: Epoch 086: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 087: loss 1.87 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.37 | clip 0.94\n",
            "INFO: Epoch 087: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 088: loss 1.872 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.284 | clip 0.94\n",
            "INFO: Epoch 088: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 089: loss 1.852 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.246 | clip 0.95\n",
            "INFO: Epoch 089: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 090: loss 1.844 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.486 | clip 0.95\n",
            "INFO: Epoch 090: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 091: loss 1.833 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.422 | clip 0.95\n",
            "INFO: Epoch 091: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 092: loss 1.817 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.447 | clip 0.95\n",
            "INFO: Epoch 092: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 093: loss 1.812 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.45 | clip 0.95\n",
            "INFO: Epoch 093: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 094: loss 1.798 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.622 | clip 0.96\n",
            "INFO: Epoch 094: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 095: loss 1.794 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.393 | clip 0.94\n",
            "INFO: Epoch 095: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 096: loss 1.773 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.527 | clip 0.92\n",
            "INFO: Epoch 096: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 097: loss 1.77 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.487 | clip 0.96\n",
            "INFO: Epoch 097: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 098: loss 1.764 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.858 | clip 0.98\n",
            "INFO: Epoch 098: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 099: loss 1.753 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.531 | clip 0.95\n",
            "INFO: Epoch 099: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 100: loss 1.735 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.72 | clip 0.94\n",
            "INFO: Epoch 100: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 101: loss 1.733 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.549 | clip 0.95\n",
            "INFO: Epoch 101: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 102: loss 1.72 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.737 | clip 0.96\n",
            "INFO: Epoch 102: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 103: loss 1.709 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.746 | clip 0.93\n",
            "INFO: Epoch 103: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 104: loss 1.701 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.64 | clip 0.95\n",
            "INFO: Epoch 104: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 105: loss 1.689 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.747 | clip 0.95\n",
            "INFO: Epoch 105: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 106: loss 1.684 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.716 | clip 0.97\n",
            "INFO: Epoch 106: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 107: loss 1.67 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.696 | clip 0.97\n",
            "INFO: Epoch 107: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 108: loss 1.67 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.699 | clip 0.95\n",
            "INFO: Epoch 108: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 109: loss 1.658 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.611 | clip 0.98\n",
            "INFO: Epoch 109: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 110: loss 1.649 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.67 | clip 0.92\n",
            "INFO: Epoch 110: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 111: loss 1.642 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.674 | clip 0.96\n",
            "INFO: Epoch 111: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 112: loss 1.632 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.795 | clip 0.97\n",
            "INFO: Epoch 112: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 113: loss 1.621 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.74 | clip 0.96\n",
            "INFO: Epoch 113: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 114: loss 1.608 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.846 | clip 0.94\n",
            "INFO: Epoch 114: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 115: loss 1.61 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.866 | clip 0.96\n",
            "INFO: Epoch 115: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 116: loss 1.595 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.706 | clip 0.95\n",
            "INFO: Epoch 116: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 117: loss 1.592 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.046 | clip 0.91\n",
            "INFO: Epoch 117: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 118: loss 1.587 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.833 | clip 0.98\n",
            "INFO: Epoch 118: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 119: loss 1.571 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.905 | clip 0.95\n",
            "INFO: Epoch 119: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 120: loss 1.567 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.828 | clip 0.96\n",
            "INFO: Epoch 120: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 121: loss 1.56 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.824 | clip 0.95\n",
            "INFO: Epoch 121: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10\n",
            "INFO: Epoch 122: loss 1.554 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.848 | clip 0.94\n",
            "INFO: Epoch 122: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 123: loss 1.545 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.964 | clip 0.96\n",
            "INFO: Epoch 123: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 124: loss 1.537 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.862 | clip 0.95\n",
            "INFO: Epoch 124: valid_loss 2.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.93\n",
            "INFO: Epoch 125: loss 1.527 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.916 | clip 0.94\n",
            "INFO: Epoch 125: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 126: loss 1.525 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.95 | clip 0.93\n",
            "INFO: Epoch 126: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.83\n",
            "INFO: Epoch 127: loss 1.517 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.891 | clip 0.96\n",
            "INFO: Epoch 127: valid_loss 2.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.94\n",
            "INFO: Epoch 128: loss 1.514 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.924 | clip 0.95\n",
            "INFO: Epoch 128: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.9\n",
            "INFO: Epoch 129: loss 1.502 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.015 | clip 0.95\n",
            "INFO: Epoch 129: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.81\n",
            "INFO: Epoch 130: loss 1.486 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.925 | clip 0.97\n",
            "INFO: Epoch 130: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.85\n",
            "INFO: Epoch 131: loss 1.493 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.948 | clip 0.96\n",
            "INFO: Epoch 131: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.77\n",
            "INFO: Epoch 132: loss 1.477 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.968 | clip 0.93\n",
            "INFO: Epoch 132: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.83\n",
            "INFO: Epoch 133: loss 1.473 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.987 | clip 0.96\n",
            "INFO: Epoch 133: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.72\n",
            "INFO: Epoch 134: loss 1.469 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.023 | clip 0.96\n",
            "INFO: Epoch 134: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.68\n",
            "INFO: Epoch 135: loss 1.466 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.973 | clip 0.95\n",
            "INFO: Epoch 135: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.77\n",
            "INFO: Epoch 136: loss 1.459 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.137 | clip 0.97\n",
            "INFO: Epoch 136: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.86\n",
            "INFO: Epoch 137: loss 1.449 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.973 | clip 0.97\n",
            "INFO: Epoch 137: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.67\n",
            "INFO: Epoch 138: loss 1.453 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.007 | clip 0.95\n",
            "INFO: Epoch 138: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.67\n",
            "INFO: Epoch 139: loss 1.437 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.957 | clip 0.92\n",
            "INFO: Epoch 139: valid_loss 2.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.62\n",
            "INFO: Epoch 140: loss 1.43 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.236 | clip 0.96\n",
            "INFO: Epoch 140: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.76\n",
            "INFO: Epoch 141: loss 1.426 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.014 | clip 0.97\n",
            "INFO: Epoch 141: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.7\n",
            "INFO: Epoch 142: loss 1.419 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.066 | clip 0.94\n",
            "INFO: Epoch 142: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.81\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E20 bs100+lr0.0005+dropout0.25"
      ],
      "metadata": {
        "id": "73zL07Jf7RBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_20/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 100 \\\n",
        "    --lr 0.0005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKKsLO2F7pyT",
        "outputId": "36625aba-4ad6-453e-d071-e89beb42263d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_20/checkpoints --cuda --batch-size 100 --lr 0.0005\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 100, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_20/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 6.204 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.472 | clip 0.85\n",
            "INFO: Epoch 000: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169\n",
            "INFO: Epoch 001: loss 4.963 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.673 | clip 0.91\n",
            "INFO: Epoch 001: valid_loss 4.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 141\n",
            "INFO: Epoch 002: loss 4.787 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.536 | clip 0.78\n",
            "INFO: Epoch 002: valid_loss 4.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 135\n",
            "INFO: Epoch 003: loss 4.692 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.268 | clip 0.8\n",
            "INFO: Epoch 003: valid_loss 4.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 117\n",
            "INFO: Epoch 004: loss 4.582 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.959 | clip 0.76\n",
            "INFO: Epoch 004: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 100\n",
            "INFO: Epoch 005: loss 4.448 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.78 | clip 0.74\n",
            "INFO: Epoch 005: valid_loss 4.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.9\n",
            "INFO: Epoch 006: loss 4.35 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.647 | clip 0.75\n",
            "INFO: Epoch 006: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.4\n",
            "INFO: Epoch 007: loss 4.244 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.015 | clip 0.71\n",
            "INFO: Epoch 007: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.5\n",
            "INFO: Epoch 008: loss 4.139 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.418 | clip 0.73\n",
            "INFO: Epoch 008: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.1\n",
            "INFO: Epoch 009: loss 4.06 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.245 | clip 0.68\n",
            "INFO: Epoch 009: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.3\n",
            "INFO: Epoch 010: loss 3.976 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.269 | clip 0.69\n",
            "INFO: Epoch 010: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.5\n",
            "INFO: Epoch 011: loss 3.906 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.155 | clip 0.68\n",
            "INFO: Epoch 011: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.8\n",
            "INFO: Epoch 012: loss 3.838 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.098 | clip 0.67\n",
            "INFO: Epoch 012: valid_loss 3.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.6\n",
            "INFO: Epoch 013: loss 3.785 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.118 | clip 0.67\n",
            "INFO: Epoch 013: valid_loss 3.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.7\n",
            "INFO: Epoch 014: loss 3.719 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.08 | clip 0.65\n",
            "INFO: Epoch 014: valid_loss 3.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.1\n",
            "INFO: Epoch 015: loss 3.665 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.113 | clip 0.63\n",
            "INFO: Epoch 015: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.9\n",
            "INFO: Epoch 016: loss 3.606 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.037 | clip 0.67\n",
            "INFO: Epoch 016: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.1\n",
            "INFO: Epoch 017: loss 3.555 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.055 | clip 0.68\n",
            "INFO: Epoch 017: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.3\n",
            "INFO: Epoch 018: loss 3.502 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.014 | clip 0.66\n",
            "INFO: Epoch 018: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.4\n",
            "INFO: Epoch 019: loss 3.456 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.017 | clip 0.65\n",
            "INFO: Epoch 019: valid_loss 3.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37\n",
            "INFO: Epoch 020: loss 3.403 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.02 | clip 0.67\n",
            "INFO: Epoch 020: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35\n",
            "INFO: Epoch 021: loss 3.353 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.043 | clip 0.69\n",
            "INFO: Epoch 021: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.8\n",
            "INFO: Epoch 022: loss 3.307 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.026 | clip 0.72\n",
            "INFO: Epoch 022: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.6\n",
            "INFO: Epoch 023: loss 3.267 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.057 | clip 0.68\n",
            "INFO: Epoch 023: valid_loss 3.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31\n",
            "INFO: Epoch 024: loss 3.22 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.052 | clip 0.71\n",
            "INFO: Epoch 024: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30\n",
            "INFO: Epoch 025: loss 3.175 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.063 | clip 0.72\n",
            "INFO: Epoch 025: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29\n",
            "INFO: Epoch 026: loss 3.133 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.136 | clip 0.75\n",
            "INFO: Epoch 026: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.4\n",
            "INFO: Epoch 027: loss 3.089 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.128 | clip 0.74\n",
            "INFO: Epoch 027: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.8\n",
            "INFO: Epoch 028: loss 3.048 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.142 | clip 0.74\n",
            "INFO: Epoch 028: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.9\n",
            "INFO: Epoch 029: loss 3.01 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.18 | clip 0.74\n",
            "INFO: Epoch 029: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 030: loss 2.975 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.25 | clip 0.75\n",
            "INFO: Epoch 030: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.4\n",
            "INFO: Epoch 031: loss 2.938 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.258 | clip 0.76\n",
            "INFO: Epoch 031: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 032: loss 2.905 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.298 | clip 0.74\n",
            "INFO: Epoch 032: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 033: loss 2.867 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.298 | clip 0.74\n",
            "INFO: Epoch 033: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 034: loss 2.839 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.437 | clip 0.75\n",
            "INFO: Epoch 034: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 035: loss 2.799 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.431 | clip 0.76\n",
            "INFO: Epoch 035: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 036: loss 2.772 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.386 | clip 0.78\n",
            "INFO: Epoch 036: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 037: loss 2.742 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.426 | clip 0.8\n",
            "INFO: Epoch 037: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 038: loss 2.716 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.571 | clip 0.81\n",
            "INFO: Epoch 038: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 039: loss 2.687 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.523 | clip 0.81\n",
            "INFO: Epoch 039: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 040: loss 2.657 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.605 | clip 0.78\n",
            "INFO: Epoch 040: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19\n",
            "INFO: Epoch 041: loss 2.629 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.602 | clip 0.79\n",
            "INFO: Epoch 041: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 042: loss 2.602 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.661 | clip 0.8\n",
            "INFO: Epoch 042: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 043: loss 2.578 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.686 | clip 0.79\n",
            "INFO: Epoch 043: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 044: loss 2.549 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.75 | clip 0.84\n",
            "INFO: Epoch 044: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 045: loss 2.531 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.779 | clip 0.8\n",
            "INFO: Epoch 045: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 046: loss 2.504 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.983 | clip 0.79\n",
            "INFO: Epoch 046: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 047: loss 2.479 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.843 | clip 0.82\n",
            "INFO: Epoch 047: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 048: loss 2.456 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.909 | clip 0.79\n",
            "INFO: Epoch 048: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 049: loss 2.437 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.915 | clip 0.83\n",
            "INFO: Epoch 049: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 050: loss 2.413 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.05 | clip 0.8\n",
            "INFO: Epoch 050: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 051: loss 2.39 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.946 | clip 0.84\n",
            "INFO: Epoch 051: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 052: loss 2.369 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.044 | clip 0.84\n",
            "INFO: Epoch 052: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 053: loss 2.345 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.129 | clip 0.83\n",
            "INFO: Epoch 053: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 054: loss 2.327 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.11 | clip 0.84\n",
            "INFO: Epoch 054: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 055: loss 2.31 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.283 | clip 0.84\n",
            "INFO: Epoch 055: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 056: loss 2.29 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.236 | clip 0.83\n",
            "INFO: Epoch 056: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 057: loss 2.267 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.274 | clip 0.82\n",
            "INFO: Epoch 057: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 058: loss 2.249 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.24 | clip 0.85\n",
            "INFO: Epoch 058: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 059: loss 2.225 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.563 | clip 0.83\n",
            "INFO: Epoch 059: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 060: loss 2.221 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.522 | clip 0.88\n",
            "INFO: Epoch 060: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 061: loss 2.186 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.364 | clip 0.83\n",
            "INFO: Epoch 061: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 062: loss 2.172 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.46 | clip 0.85\n",
            "INFO: Epoch 062: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 063: loss 2.154 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.47 | clip 0.85\n",
            "INFO: Epoch 063: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 064: loss 2.137 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.521 | clip 0.87\n",
            "INFO: Epoch 064: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 065: loss 2.119 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.66 | clip 0.87\n",
            "INFO: Epoch 065: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 066: loss 2.104 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.638 | clip 0.9\n",
            "INFO: Epoch 066: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 067: loss 2.085 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.629 | clip 0.87\n",
            "INFO: Epoch 067: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 068: loss 2.069 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.777 | clip 0.91\n",
            "INFO: Epoch 068: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 069: loss 2.05 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.784 | clip 0.88\n",
            "INFO: Epoch 069: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 070: loss 2.039 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.806 | clip 0.89\n",
            "INFO: Epoch 070: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 071: loss 2.02 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.782 | clip 0.86\n",
            "INFO: Epoch 071: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 072: loss 2.007 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.876 | clip 0.89\n",
            "INFO: Epoch 072: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 073: loss 1.994 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.711 | clip 0.91\n",
            "INFO: Epoch 073: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 074: loss 1.979 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.876 | clip 0.91\n",
            "INFO: Epoch 074: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 075: loss 1.965 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.061 | clip 0.89\n",
            "INFO: Epoch 075: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 076: loss 1.949 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.852 | clip 0.89\n",
            "INFO: Epoch 076: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 077: loss 1.932 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.829 | clip 0.9\n",
            "INFO: Epoch 077: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 078: loss 1.92 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.92 | clip 0.9\n",
            "INFO: Epoch 078: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 079: loss 1.903 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.914 | clip 0.92\n",
            "INFO: Epoch 079: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 080: loss 1.895 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.25 | clip 0.91\n",
            "INFO: Epoch 080: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 081: loss 1.879 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.081 | clip 0.93\n",
            "INFO: Epoch 081: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 082: loss 1.863 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.16 | clip 0.9\n",
            "INFO: Epoch 082: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 083: loss 1.856 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.118 | clip 0.93\n",
            "INFO: Epoch 083: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 084: loss 1.836 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.113 | clip 0.91\n",
            "INFO: Epoch 084: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 085: loss 1.824 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.161 | clip 0.94\n",
            "INFO: Epoch 085: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 086: loss 1.815 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.261 | clip 0.9\n",
            "INFO: Epoch 086: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 087: loss 1.795 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.137 | clip 0.91\n",
            "INFO: Epoch 087: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 088: loss 1.789 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.308 | clip 0.94\n",
            "INFO: Epoch 088: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 089: loss 1.774 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.209 | clip 0.94\n",
            "INFO: Epoch 089: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 090: loss 1.759 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.168 | clip 0.94\n",
            "INFO: Epoch 090: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 091: loss 1.751 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.358 | clip 0.92\n",
            "INFO: Epoch 091: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 092: loss 1.739 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.48 | clip 0.94\n",
            "INFO: Epoch 092: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 093: loss 1.729 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.37 | clip 0.91\n",
            "INFO: Epoch 093: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 094: loss 1.716 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.487 | clip 0.93\n",
            "INFO: Epoch 094: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 095: loss 1.709 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.42 | clip 0.93\n",
            "INFO: Epoch 095: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 096: loss 1.692 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.452 | clip 0.9\n",
            "INFO: Epoch 096: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 097: loss 1.688 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.341 | clip 0.96\n",
            "INFO: Epoch 097: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 098: loss 1.676 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.518 | clip 0.96\n",
            "INFO: Epoch 098: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E21 bs500+lr0.0003+dropout0.3"
      ],
      "metadata": {
        "id": "yuLXxOXw-L_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_21/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 500 \\\n",
        "    --lr 0.0003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4szxQ9x-Twz",
        "outputId": "afe74f6d-3f3f-4951-db4e-8473a8038701"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_21/checkpoints --cuda --batch-size 500 --lr 0.0003\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 500, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_21/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 8.216 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 2.382 | clip 0.05\n",
            "INFO: Epoch 000: valid_loss 8.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 3.12e+03\n",
            "INFO: Epoch 001: loss 7.444 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 9.486 | clip 0.9\n",
            "INFO: Epoch 001: valid_loss 6.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 670\n",
            "INFO: Epoch 002: loss 6.107 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 11.19 | clip 1\n",
            "INFO: Epoch 002: valid_loss 5.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 278\n",
            "INFO: Epoch 003: loss 5.561 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 7.914 | clip 1\n",
            "INFO: Epoch 003: valid_loss 5.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 206\n",
            "INFO: Epoch 004: loss 5.4 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.819 | clip 0.65\n",
            "INFO: Epoch 004: valid_loss 5.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 188\n",
            "INFO: Epoch 005: loss 5.334 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.444 | clip 0.65\n",
            "INFO: Epoch 005: valid_loss 5.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 177\n",
            "INFO: Epoch 006: loss 5.273 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.132 | clip 0.75\n",
            "INFO: Epoch 006: valid_loss 5.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 166\n",
            "INFO: Epoch 007: loss 5.207 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 7.03 | clip 0.85\n",
            "INFO: Epoch 007: valid_loss 5.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 154\n",
            "INFO: Epoch 008: loss 5.137 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.845 | clip 0.8\n",
            "INFO: Epoch 008: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 144\n",
            "INFO: Epoch 009: loss 5.073 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.477 | clip 0.8\n",
            "INFO: Epoch 009: valid_loss 4.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 137\n",
            "INFO: Epoch 010: loss 5.022 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.422 | clip 0.75\n",
            "INFO: Epoch 010: valid_loss 4.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 130\n",
            "INFO: Epoch 011: loss 4.968 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 6.085 | clip 0.7\n",
            "INFO: Epoch 011: valid_loss 4.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 125\n",
            "INFO: Epoch 012: loss 4.924 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.905 | clip 0.7\n",
            "INFO: Epoch 012: valid_loss 4.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 120\n",
            "INFO: Epoch 013: loss 4.884 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.627 | clip 0.75\n",
            "INFO: Epoch 013: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116\n",
            "INFO: Epoch 014: loss 4.842 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.363 | clip 0.75\n",
            "INFO: Epoch 014: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112\n",
            "INFO: Epoch 015: loss 4.805 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.133 | clip 0.65\n",
            "INFO: Epoch 015: valid_loss 4.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 108\n",
            "INFO: Epoch 016: loss 4.77 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.917 | clip 0.65\n",
            "INFO: Epoch 016: valid_loss 4.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 105\n",
            "INFO: Epoch 017: loss 4.74 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.787 | clip 0.55\n",
            "INFO: Epoch 017: valid_loss 4.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 102\n",
            "INFO: Epoch 018: loss 4.71 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.714 | clip 0.55\n",
            "INFO: Epoch 018: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 100\n",
            "INFO: Epoch 019: loss 4.681 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.697 | clip 0.5\n",
            "INFO: Epoch 019: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.3\n",
            "INFO: Epoch 020: loss 4.66 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.989 | clip 0.55\n",
            "INFO: Epoch 020: valid_loss 4.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 96.1\n",
            "INFO: Epoch 021: loss 4.636 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.997 | clip 0.65\n",
            "INFO: Epoch 021: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.4\n",
            "INFO: Epoch 022: loss 4.606 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.854 | clip 0.65\n",
            "INFO: Epoch 022: valid_loss 4.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 90.9\n",
            "INFO: Epoch 023: loss 4.575 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.59 | clip 0.55\n",
            "INFO: Epoch 023: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.5\n",
            "INFO: Epoch 024: loss 4.547 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.5 | clip 0.45\n",
            "INFO: Epoch 024: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.2\n",
            "INFO: Epoch 025: loss 4.518 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.491 | clip 0.45\n",
            "INFO: Epoch 025: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.5\n",
            "INFO: Epoch 026: loss 4.498 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.667 | clip 0.5\n",
            "INFO: Epoch 026: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.6\n",
            "INFO: Epoch 027: loss 4.47 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.769 | clip 0.6\n",
            "INFO: Epoch 027: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81\n",
            "INFO: Epoch 028: loss 4.452 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.084 | clip 0.7\n",
            "INFO: Epoch 028: valid_loss 4.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 78.9\n",
            "INFO: Epoch 029: loss 4.426 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.034 | clip 0.7\n",
            "INFO: Epoch 029: valid_loss 4.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 76.8\n",
            "INFO: Epoch 030: loss 4.396 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.865 | clip 0.55\n",
            "INFO: Epoch 030: valid_loss 4.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 74.9\n",
            "INFO: Epoch 031: loss 4.371 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.682 | clip 0.55\n",
            "INFO: Epoch 031: valid_loss 4.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.1\n",
            "INFO: Epoch 032: loss 4.35 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.661 | clip 0.4\n",
            "INFO: Epoch 032: valid_loss 4.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 71.9\n",
            "INFO: Epoch 033: loss 4.328 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.895 | clip 0.65\n",
            "INFO: Epoch 033: valid_loss 4.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 70.3\n",
            "INFO: Epoch 034: loss 4.31 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.983 | clip 0.7\n",
            "INFO: Epoch 034: valid_loss 4.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 68.7\n",
            "INFO: Epoch 035: loss 4.284 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.923 | clip 0.7\n",
            "INFO: Epoch 035: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.1\n",
            "INFO: Epoch 036: loss 4.26 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.629 | clip 0.55\n",
            "INFO: Epoch 036: valid_loss 4.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 65.6\n",
            "INFO: Epoch 037: loss 4.235 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.504 | clip 0.45\n",
            "INFO: Epoch 037: valid_loss 4.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 64.2\n",
            "INFO: Epoch 038: loss 4.214 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.383 | clip 0.45\n",
            "INFO: Epoch 038: valid_loss 4.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 62.9\n",
            "INFO: Epoch 039: loss 4.195 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.341 | clip 0.45\n",
            "INFO: Epoch 039: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.8\n",
            "INFO: Epoch 040: loss 4.17 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.25 | clip 0.45\n",
            "INFO: Epoch 040: valid_loss 4.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 60.4\n",
            "INFO: Epoch 041: loss 4.149 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.159 | clip 0.4\n",
            "INFO: Epoch 041: valid_loss 4.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 59.1\n",
            "INFO: Epoch 042: loss 4.127 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.098 | clip 0.3\n",
            "INFO: Epoch 042: valid_loss 4.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 58\n",
            "INFO: Epoch 043: loss 4.106 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.025 | clip 0.4\n",
            "INFO: Epoch 043: valid_loss 4.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 56.6\n",
            "INFO: Epoch 044: loss 4.084 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.862 | clip 0.3\n",
            "INFO: Epoch 044: valid_loss 4.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.5\n",
            "INFO: Epoch 045: loss 4.061 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.767 | clip 0.3\n",
            "INFO: Epoch 045: valid_loss 4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 54.5\n",
            "INFO: Epoch 046: loss 4.043 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.721 | clip 0.3\n",
            "INFO: Epoch 046: valid_loss 3.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.5\n",
            "INFO: Epoch 047: loss 4.019 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.584 | clip 0.25\n",
            "INFO: Epoch 047: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.5\n",
            "INFO: Epoch 048: loss 4.003 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.668 | clip 0.25\n",
            "INFO: Epoch 048: valid_loss 3.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 51.7\n",
            "INFO: Epoch 049: loss 3.984 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.55 | clip 0.3\n",
            "INFO: Epoch 049: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.8\n",
            "INFO: Epoch 050: loss 3.965 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.565 | clip 0.25\n",
            "INFO: Epoch 050: valid_loss 3.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50\n",
            "INFO: Epoch 051: loss 3.947 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.568 | clip 0.3\n",
            "INFO: Epoch 051: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.2\n",
            "INFO: Epoch 052: loss 3.934 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.609 | clip 0.25\n",
            "INFO: Epoch 052: valid_loss 3.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 48.4\n",
            "INFO: Epoch 053: loss 3.914 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.572 | clip 0.25\n",
            "INFO: Epoch 053: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.7\n",
            "INFO: Epoch 054: loss 3.897 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.588 | clip 0.3\n",
            "INFO: Epoch 054: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47\n",
            "INFO: Epoch 055: loss 3.882 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.544 | clip 0.25\n",
            "INFO: Epoch 055: valid_loss 3.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 46.3\n",
            "INFO: Epoch 056: loss 3.864 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.731 | clip 0.3\n",
            "INFO: Epoch 056: valid_loss 3.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.7\n",
            "INFO: Epoch 057: loss 3.852 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.639 | clip 0.3\n",
            "INFO: Epoch 057: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.1\n",
            "INFO: Epoch 058: loss 3.834 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.437 | clip 0.3\n",
            "INFO: Epoch 058: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.5\n",
            "INFO: Epoch 059: loss 3.821 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.498 | clip 0.25\n",
            "INFO: Epoch 059: valid_loss 3.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.9\n",
            "INFO: Epoch 060: loss 3.807 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.678 | clip 0.45\n",
            "INFO: Epoch 060: valid_loss 3.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43.4\n",
            "INFO: Epoch 061: loss 3.794 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.57 | clip 0.3\n",
            "INFO: Epoch 061: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 43\n",
            "INFO: Epoch 062: loss 3.773 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.62 | clip 0.25\n",
            "INFO: Epoch 062: valid_loss 3.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.6\n",
            "INFO: Epoch 063: loss 3.772 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.952 | clip 0.3\n",
            "INFO: Epoch 063: valid_loss 3.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42\n",
            "INFO: Epoch 064: loss 3.748 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.601 | clip 0.35\n",
            "INFO: Epoch 064: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.6\n",
            "INFO: Epoch 065: loss 3.741 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.953 | clip 0.35\n",
            "INFO: Epoch 065: valid_loss 3.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.2\n",
            "INFO: Epoch 066: loss 3.723 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.834 | clip 0.35\n",
            "INFO: Epoch 066: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41\n",
            "INFO: Epoch 067: loss 3.722 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.237 | clip 0.4\n",
            "INFO: Epoch 067: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.3\n",
            "INFO: Epoch 068: loss 3.699 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.985 | clip 0.3\n",
            "INFO: Epoch 068: valid_loss 3.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.3\n",
            "INFO: Epoch 069: loss 3.702 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.256 | clip 0.4\n",
            "INFO: Epoch 069: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.3\n",
            "INFO: Epoch 070: loss 3.674 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.918 | clip 0.25\n",
            "INFO: Epoch 070: valid_loss 3.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.5\n",
            "INFO: Epoch 071: loss 3.677 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.279 | clip 0.45\n",
            "INFO: Epoch 071: valid_loss 3.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.6\n",
            "INFO: Epoch 072: loss 3.651 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.825 | clip 0.2\n",
            "INFO: Epoch 072: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.7\n",
            "INFO: Epoch 073: loss 3.65 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.109 | clip 0.4\n",
            "INFO: Epoch 073: valid_loss 3.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.9\n",
            "INFO: Epoch 074: loss 3.627 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.659 | clip 0.15\n",
            "INFO: Epoch 074: valid_loss 3.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38\n",
            "INFO: Epoch 075: loss 3.628 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.017 | clip 0.35\n",
            "INFO: Epoch 075: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.2\n",
            "INFO: Epoch 076: loss 3.605 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.628 | clip 0.3\n",
            "INFO: Epoch 076: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.3\n",
            "INFO: Epoch 077: loss 3.605 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.983 | clip 0.4\n",
            "INFO: Epoch 077: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.5\n",
            "INFO: Epoch 078: loss 3.584 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.55 | clip 0.2\n",
            "INFO: Epoch 078: valid_loss 3.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36.6\n",
            "INFO: Epoch 079: loss 3.583 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.032 | clip 0.45\n",
            "INFO: Epoch 079: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36\n",
            "INFO: Epoch 080: loss 3.562 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.623 | clip 0.35\n",
            "INFO: Epoch 080: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 36\n",
            "INFO: Epoch 081: loss 3.562 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.997 | clip 0.4\n",
            "INFO: Epoch 081: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.4\n",
            "INFO: Epoch 082: loss 3.54 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.019 | clip 0.35\n",
            "INFO: Epoch 082: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.5\n",
            "INFO: Epoch 083: loss 3.545 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.189 | clip 0.35\n",
            "INFO: Epoch 083: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.8\n",
            "INFO: Epoch 084: loss 3.517 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.633 | clip 0.25\n",
            "INFO: Epoch 084: valid_loss 3.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.8\n",
            "INFO: Epoch 085: loss 3.517 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.803 | clip 0.35\n",
            "INFO: Epoch 085: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 086: loss 3.497 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.524 | clip 0.2\n",
            "INFO: Epoch 086: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.2\n",
            "INFO: Epoch 087: loss 3.5 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.999 | clip 0.4\n",
            "INFO: Epoch 087: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.6\n",
            "INFO: Epoch 088: loss 3.477 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.841 | clip 0.2\n",
            "INFO: Epoch 088: valid_loss 3.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.7\n",
            "INFO: Epoch 089: loss 3.475 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.078 | clip 0.4\n",
            "INFO: Epoch 089: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 090: loss 3.452 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.559 | clip 0.2\n",
            "INFO: Epoch 090: valid_loss 3.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.1\n",
            "INFO: Epoch 091: loss 3.454 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.929 | clip 0.4\n",
            "INFO: Epoch 091: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.4\n",
            "INFO: Epoch 092: loss 3.433 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.746 | clip 0.2\n",
            "INFO: Epoch 092: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.6\n",
            "INFO: Epoch 093: loss 3.429 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.896 | clip 0.4\n",
            "INFO: Epoch 093: valid_loss 3.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32\n",
            "INFO: Epoch 094: loss 3.406 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.468 | clip 0.15\n",
            "INFO: Epoch 094: valid_loss 3.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.9\n",
            "INFO: Epoch 095: loss 3.412 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.09 | clip 0.4\n",
            "INFO: Epoch 095: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 096: loss 3.386 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.672 | clip 0.2\n",
            "INFO: Epoch 096: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.4\n",
            "INFO: Epoch 097: loss 3.391 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.993 | clip 0.4\n",
            "INFO: Epoch 097: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.9\n",
            "INFO: Epoch 098: loss 3.366 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.542 | clip 0.2\n",
            "INFO: Epoch 098: valid_loss 3.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.9\n",
            "INFO: Epoch 099: loss 3.37 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.965 | clip 0.4\n",
            "INFO: Epoch 099: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.3\n",
            "INFO: Epoch 100: loss 3.344 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.703 | clip 0.15\n",
            "INFO: Epoch 100: valid_loss 3.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.3\n",
            "INFO: Epoch 101: loss 3.346 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.938 | clip 0.45\n",
            "INFO: Epoch 101: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.8\n",
            "INFO: Epoch 102: loss 3.323 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.561 | clip 0.15\n",
            "INFO: Epoch 102: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.7\n",
            "INFO: Epoch 103: loss 3.318 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.66 | clip 0.35\n",
            "INFO: Epoch 103: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.2\n",
            "INFO: Epoch 104: loss 3.302 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.564 | clip 0.2\n",
            "INFO: Epoch 104: valid_loss 3.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29\n",
            "INFO: Epoch 105: loss 3.292 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.397 | clip 0.25\n",
            "INFO: Epoch 105: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.6\n",
            "INFO: Epoch 106: loss 3.281 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.616 | clip 0.2\n",
            "INFO: Epoch 106: valid_loss 3.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.4\n",
            "INFO: Epoch 107: loss 3.267 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.36 | clip 0.2\n",
            "INFO: Epoch 107: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.1\n",
            "INFO: Epoch 108: loss 3.257 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.553 | clip 0.2\n",
            "INFO: Epoch 108: valid_loss 3.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.9\n",
            "INFO: Epoch 109: loss 3.254 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.546 | clip 0.3\n",
            "INFO: Epoch 109: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.6\n",
            "INFO: Epoch 110: loss 3.235 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.617 | clip 0.3\n",
            "INFO: Epoch 110: valid_loss 3.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.5\n",
            "INFO: Epoch 111: loss 3.232 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.673 | clip 0.3\n",
            "INFO: Epoch 111: valid_loss 3.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.1\n",
            "INFO: Epoch 112: loss 3.211 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.699 | clip 0.3\n",
            "INFO: Epoch 112: valid_loss 3.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.9\n",
            "INFO: Epoch 113: loss 3.21 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.721 | clip 0.35\n",
            "INFO: Epoch 113: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 114: loss 3.196 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.582 | clip 0.25\n",
            "INFO: Epoch 114: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.5\n",
            "INFO: Epoch 115: loss 3.192 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.798 | clip 0.35\n",
            "INFO: Epoch 115: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 116: loss 3.172 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.812 | clip 0.2\n",
            "INFO: Epoch 116: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 117: loss 3.176 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.913 | clip 0.4\n",
            "INFO: Epoch 117: valid_loss 3.25 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.8\n",
            "INFO: Epoch 118: loss 3.153 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.786 | clip 0.3\n",
            "INFO: Epoch 118: valid_loss 3.24 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.6\n",
            "INFO: Epoch 119: loss 3.151 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.776 | clip 0.4\n",
            "INFO: Epoch 119: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.4\n",
            "INFO: Epoch 120: loss 3.138 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.977 | clip 0.3\n",
            "INFO: Epoch 120: valid_loss 3.23 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25.2\n",
            "INFO: Epoch 121: loss 3.132 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.586 | clip 0.35\n",
            "INFO: Epoch 121: valid_loss 3.22 | num_tokens 9.14 | batch_size 500 | valid_perplexity 25\n",
            "INFO: Epoch 122: loss 3.11 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.72 | clip 0.25\n",
            "INFO: Epoch 122: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 123: loss 3.112 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.644 | clip 0.35\n",
            "INFO: Epoch 123: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 124: loss 3.099 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.077 | clip 0.25\n",
            "INFO: Epoch 124: valid_loss 3.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.5\n",
            "INFO: Epoch 125: loss 3.093 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.662 | clip 0.35\n",
            "INFO: Epoch 125: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.1\n",
            "INFO: Epoch 126: loss 3.078 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.748 | clip 0.3\n",
            "INFO: Epoch 126: valid_loss 3.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24\n",
            "INFO: Epoch 127: loss 3.073 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.533 | clip 0.3\n",
            "INFO: Epoch 127: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 128: loss 3.059 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.507 | clip 0.25\n",
            "INFO: Epoch 128: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.6\n",
            "INFO: Epoch 129: loss 3.052 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.565 | clip 0.3\n",
            "INFO: Epoch 129: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.4\n",
            "INFO: Epoch 130: loss 3.041 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.573 | clip 0.3\n",
            "INFO: Epoch 130: valid_loss 3.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.2\n",
            "INFO: Epoch 131: loss 3.039 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.433 | clip 0.3\n",
            "INFO: Epoch 131: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 132: loss 3.026 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.551 | clip 0.25\n",
            "INFO: Epoch 132: valid_loss 3.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23\n",
            "INFO: Epoch 133: loss 3.017 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.49 | clip 0.35\n",
            "INFO: Epoch 133: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 134: loss 3.004 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.544 | clip 0.3\n",
            "INFO: Epoch 134: valid_loss 3.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.6\n",
            "INFO: Epoch 135: loss 3.003 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.614 | clip 0.3\n",
            "INFO: Epoch 135: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.5\n",
            "INFO: Epoch 136: loss 2.989 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.794 | clip 0.25\n",
            "INFO: Epoch 136: valid_loss 3.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.3\n",
            "INFO: Epoch 137: loss 2.986 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.646 | clip 0.3\n",
            "INFO: Epoch 137: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 138: loss 2.973 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.911 | clip 0.3\n",
            "INFO: Epoch 138: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 139: loss 2.965 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.593 | clip 0.25\n",
            "INFO: Epoch 139: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.9\n",
            "INFO: Epoch 140: loss 2.959 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.115 | clip 0.25\n",
            "INFO: Epoch 140: valid_loss 3.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.8\n",
            "INFO: Epoch 141: loss 2.953 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.644 | clip 0.3\n",
            "INFO: Epoch 141: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6\n",
            "INFO: Epoch 142: loss 2.94 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.735 | clip 0.3\n",
            "INFO: Epoch 142: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.4\n",
            "INFO: Epoch 143: loss 2.937 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.596 | clip 0.35\n",
            "INFO: Epoch 143: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 144: loss 2.923 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.173 | clip 0.3\n",
            "INFO: Epoch 144: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.2\n",
            "INFO: Epoch 145: loss 2.92 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.687 | clip 0.3\n",
            "INFO: Epoch 145: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 146: loss 2.91 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.164 | clip 0.25\n",
            "INFO: Epoch 146: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 147: loss 2.91 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.838 | clip 0.35\n",
            "INFO: Epoch 147: valid_loss 3.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.9\n",
            "INFO: Epoch 148: loss 2.893 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.673 | clip 0.25\n",
            "INFO: Epoch 148: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 149: loss 2.886 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.602 | clip 0.3\n",
            "INFO: Epoch 149: valid_loss 3.02 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 150: loss 2.877 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.809 | clip 0.3\n",
            "INFO: Epoch 150: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4\n",
            "INFO: Epoch 151: loss 2.871 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.571 | clip 0.25\n",
            "INFO: Epoch 151: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.3\n",
            "INFO: Epoch 152: loss 2.864 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.892 | clip 0.25\n",
            "INFO: Epoch 152: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.1\n",
            "INFO: Epoch 153: loss 2.859 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.615 | clip 0.35\n",
            "INFO: Epoch 153: valid_loss 3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20\n",
            "INFO: Epoch 154: loss 2.847 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.104 | clip 0.3\n",
            "INFO: Epoch 154: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.9\n",
            "INFO: Epoch 155: loss 2.842 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.63 | clip 0.35\n",
            "INFO: Epoch 155: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 156: loss 2.832 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.155 | clip 0.35\n",
            "INFO: Epoch 156: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 157: loss 2.83 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.722 | clip 0.3\n",
            "INFO: Epoch 157: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 158: loss 2.819 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.146 | clip 0.3\n",
            "INFO: Epoch 158: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.4\n",
            "INFO: Epoch 159: loss 2.814 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.89 | clip 0.35\n",
            "INFO: Epoch 159: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.3\n",
            "INFO: Epoch 160: loss 2.8 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.253 | clip 0.25\n",
            "INFO: Epoch 160: valid_loss 2.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 161: loss 2.807 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.981 | clip 0.35\n",
            "INFO: Epoch 161: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 162: loss 2.791 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.843 | clip 0.3\n",
            "INFO: Epoch 162: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19\n",
            "INFO: Epoch 163: loss 2.786 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.625 | clip 0.3\n",
            "INFO: Epoch 163: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 164: loss 2.775 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.973 | clip 0.35\n",
            "INFO: Epoch 164: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 165: loss 2.774 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.817 | clip 0.4\n",
            "INFO: Epoch 165: valid_loss 2.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 166: loss 2.762 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.191 | clip 0.3\n",
            "INFO: Epoch 166: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.6\n",
            "INFO: Epoch 167: loss 2.756 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.944 | clip 0.35\n",
            "INFO: Epoch 167: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 168: loss 2.747 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.479 | clip 0.35\n",
            "INFO: Epoch 168: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 169: loss 2.745 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.981 | clip 0.35\n",
            "INFO: Epoch 169: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.3\n",
            "INFO: Epoch 170: loss 2.728 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.408 | clip 0.3\n",
            "INFO: Epoch 170: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 171: loss 2.729 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.838 | clip 0.3\n",
            "INFO: Epoch 171: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.1\n",
            "INFO: Epoch 172: loss 2.718 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.322 | clip 0.35\n",
            "INFO: Epoch 172: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 173: loss 2.713 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.714 | clip 0.35\n",
            "INFO: Epoch 173: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 174: loss 2.703 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.278 | clip 0.4\n",
            "INFO: Epoch 174: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.8\n",
            "INFO: Epoch 175: loss 2.7 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.886 | clip 0.35\n",
            "INFO: Epoch 175: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 176: loss 2.687 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.402 | clip 0.4\n",
            "INFO: Epoch 176: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 177: loss 2.689 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.01 | clip 0.45\n",
            "INFO: Epoch 177: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 178: loss 2.674 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.161 | clip 0.25\n",
            "INFO: Epoch 178: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 179: loss 2.672 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.745 | clip 0.3\n",
            "INFO: Epoch 179: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 180: loss 2.661 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.963 | clip 0.3\n",
            "INFO: Epoch 180: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 181: loss 2.66 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.883 | clip 0.35\n",
            "INFO: Epoch 181: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 182: loss 2.643 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.912 | clip 0.3\n",
            "INFO: Epoch 182: valid_loss 2.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.1\n",
            "INFO: Epoch 183: loss 2.646 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.916 | clip 0.4\n",
            "INFO: Epoch 183: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 184: loss 2.631 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.758 | clip 0.25\n",
            "INFO: Epoch 184: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 185: loss 2.633 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.917 | clip 0.45\n",
            "INFO: Epoch 185: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 186: loss 2.622 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.006 | clip 0.4\n",
            "INFO: Epoch 186: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.7\n",
            "INFO: Epoch 187: loss 2.61 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.981 | clip 0.3\n",
            "INFO: Epoch 187: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 188: loss 2.609 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.693 | clip 0.25\n",
            "INFO: Epoch 188: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 189: loss 2.603 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.945 | clip 0.35\n",
            "INFO: Epoch 189: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 190: loss 2.601 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.84 | clip 0.4\n",
            "INFO: Epoch 190: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 191: loss 2.59 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.021 | clip 0.4\n",
            "INFO: Epoch 191: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 192: loss 2.585 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.989 | clip 0.4\n",
            "INFO: Epoch 192: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 193: loss 2.575 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.07 | clip 0.4\n",
            "INFO: Epoch 193: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 194: loss 2.579 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.171 | clip 0.4\n",
            "INFO: Epoch 194: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 195: loss 2.567 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.078 | clip 0.4\n",
            "INFO: Epoch 195: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 196: loss 2.565 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.896 | clip 0.4\n",
            "INFO: Epoch 196: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 197: loss 2.55 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.361 | clip 0.4\n",
            "INFO: Epoch 197: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 198: loss 2.551 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.18 | clip 0.5\n",
            "INFO: Epoch 198: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 199: loss 2.543 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.567 | clip 0.35\n",
            "INFO: Epoch 199: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 200: loss 2.542 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.137 | clip 0.45\n",
            "INFO: Epoch 200: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.8\n",
            "INFO: Epoch 201: loss 2.535 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.264 | clip 0.4\n",
            "INFO: Epoch 201: valid_loss 2.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 202: loss 2.528 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.074 | clip 0.45\n",
            "INFO: Epoch 202: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 203: loss 2.517 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.357 | clip 0.4\n",
            "INFO: Epoch 203: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 204: loss 2.519 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.079 | clip 0.45\n",
            "INFO: Epoch 204: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 205: loss 2.508 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.928 | clip 0.45\n",
            "INFO: Epoch 205: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 206: loss 2.503 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.924 | clip 0.35\n",
            "INFO: Epoch 206: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 207: loss 2.488 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.036 | clip 0.4\n",
            "INFO: Epoch 207: valid_loss 2.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.3\n",
            "INFO: Epoch 208: loss 2.488 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.016 | clip 0.45\n",
            "INFO: Epoch 208: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 209: loss 2.482 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.303 | clip 0.4\n",
            "INFO: Epoch 209: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 210: loss 2.475 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.113 | clip 0.5\n",
            "INFO: Epoch 210: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 211: loss 2.469 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.18 | clip 0.45\n",
            "INFO: Epoch 211: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 212: loss 2.467 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.979 | clip 0.45\n",
            "INFO: Epoch 212: valid_loss 2.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 213: loss 2.458 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.282 | clip 0.35\n",
            "INFO: Epoch 213: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 214: loss 2.454 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.977 | clip 0.45\n",
            "INFO: Epoch 214: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 215: loss 2.441 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.313 | clip 0.4\n",
            "INFO: Epoch 215: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 216: loss 2.446 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.109 | clip 0.4\n",
            "INFO: Epoch 216: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 217: loss 2.433 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.293 | clip 0.4\n",
            "INFO: Epoch 217: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 218: loss 2.432 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.169 | clip 0.4\n",
            "INFO: Epoch 218: valid_loss 2.69 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 219: loss 2.422 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.075 | clip 0.45\n",
            "INFO: Epoch 219: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 220: loss 2.427 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.102 | clip 0.45\n",
            "INFO: Epoch 220: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 221: loss 2.411 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.94 | clip 0.45\n",
            "INFO: Epoch 221: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 222: loss 2.41 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.022 | clip 0.4\n",
            "INFO: Epoch 222: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 223: loss 2.403 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.258 | clip 0.35\n",
            "INFO: Epoch 223: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 224: loss 2.403 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.054 | clip 0.45\n",
            "INFO: Epoch 224: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 225: loss 2.394 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.046 | clip 0.4\n",
            "INFO: Epoch 225: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 226: loss 2.388 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.179 | clip 0.45\n",
            "INFO: Epoch 226: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 227: loss 2.378 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.245 | clip 0.45\n",
            "INFO: Epoch 227: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 228: loss 2.383 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.156 | clip 0.4\n",
            "INFO: Epoch 228: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.2\n",
            "INFO: Epoch 229: loss 2.369 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.291 | clip 0.45\n",
            "INFO: Epoch 229: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 230: loss 2.37 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.133 | clip 0.5\n",
            "INFO: Epoch 230: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 231: loss 2.362 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.012 | clip 0.45\n",
            "INFO: Epoch 231: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 232: loss 2.355 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.085 | clip 0.55\n",
            "INFO: Epoch 232: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 233: loss 2.349 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.256 | clip 0.4\n",
            "INFO: Epoch 233: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 234: loss 2.345 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 3.897 | clip 0.4\n",
            "INFO: Epoch 234: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 235: loss 2.338 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.555 | clip 0.4\n",
            "INFO: Epoch 235: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 236: loss 2.344 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.308 | clip 0.5\n",
            "INFO: Epoch 236: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 237: loss 2.33 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.12 | clip 0.45\n",
            "INFO: Epoch 237: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 238: loss 2.329 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.185 | clip 0.5\n",
            "INFO: Epoch 238: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 239: loss 2.319 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.254 | clip 0.5\n",
            "INFO: Epoch 239: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 240: loss 2.316 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.16 | clip 0.55\n",
            "INFO: Epoch 240: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 241: loss 2.307 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.262 | clip 0.4\n",
            "INFO: Epoch 241: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 242: loss 2.304 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.096 | clip 0.35\n",
            "INFO: Epoch 242: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 243: loss 2.3 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.44 | clip 0.45\n",
            "INFO: Epoch 243: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 244: loss 2.293 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.18 | clip 0.55\n",
            "INFO: Epoch 244: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 245: loss 2.289 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.421 | clip 0.5\n",
            "INFO: Epoch 245: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 246: loss 2.291 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.284 | clip 0.5\n",
            "INFO: Epoch 246: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 247: loss 2.279 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.237 | clip 0.5\n",
            "INFO: Epoch 247: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 248: loss 2.274 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.279 | clip 0.45\n",
            "INFO: Epoch 248: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 249: loss 2.265 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.297 | clip 0.4\n",
            "INFO: Epoch 249: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 250: loss 2.261 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.332 | clip 0.55\n",
            "INFO: Epoch 250: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 251: loss 2.258 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.162 | clip 0.4\n",
            "INFO: Epoch 251: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 252: loss 2.256 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.38 | clip 0.5\n",
            "INFO: Epoch 252: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 253: loss 2.255 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.207 | clip 0.4\n",
            "INFO: Epoch 253: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 254: loss 2.243 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.777 | clip 0.5\n",
            "INFO: Epoch 254: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 255: loss 2.243 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.384 | clip 0.45\n",
            "INFO: Epoch 255: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 256: loss 2.237 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.574 | clip 0.4\n",
            "INFO: Epoch 256: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 257: loss 2.236 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.387 | clip 0.5\n",
            "INFO: Epoch 257: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 258: loss 2.227 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.274 | clip 0.45\n",
            "INFO: Epoch 258: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 259: loss 2.226 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.417 | clip 0.55\n",
            "INFO: Epoch 259: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 260: loss 2.215 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.099 | clip 0.4\n",
            "INFO: Epoch 260: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 261: loss 2.211 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.458 | clip 0.5\n",
            "INFO: Epoch 261: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 262: loss 2.208 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.355 | clip 0.4\n",
            "INFO: Epoch 262: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 263: loss 2.203 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.365 | clip 0.45\n",
            "INFO: Epoch 263: valid_loss 2.55 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 264: loss 2.204 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.317 | clip 0.5\n",
            "INFO: Epoch 264: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 265: loss 2.199 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.253 | clip 0.5\n",
            "INFO: Epoch 265: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 266: loss 2.186 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.635 | clip 0.45\n",
            "INFO: Epoch 266: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 267: loss 2.19 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.377 | clip 0.5\n",
            "INFO: Epoch 267: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 268: loss 2.186 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.619 | clip 0.45\n",
            "INFO: Epoch 268: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 269: loss 2.175 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.336 | clip 0.45\n",
            "INFO: Epoch 269: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 270: loss 2.17 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.484 | clip 0.5\n",
            "INFO: Epoch 270: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 271: loss 2.17 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.456 | clip 0.55\n",
            "INFO: Epoch 271: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 272: loss 2.167 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.627 | clip 0.45\n",
            "INFO: Epoch 272: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 273: loss 2.169 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.412 | clip 0.55\n",
            "INFO: Epoch 273: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 274: loss 2.158 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.615 | clip 0.5\n",
            "INFO: Epoch 274: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 275: loss 2.155 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.52 | clip 0.5\n",
            "INFO: Epoch 275: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 276: loss 2.144 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.641 | clip 0.5\n",
            "INFO: Epoch 276: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 277: loss 2.146 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.544 | clip 0.55\n",
            "INFO: Epoch 277: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 278: loss 2.138 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.826 | clip 0.45\n",
            "INFO: Epoch 278: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 279: loss 2.141 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.556 | clip 0.55\n",
            "INFO: Epoch 279: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 280: loss 2.129 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.702 | clip 0.45\n",
            "INFO: Epoch 280: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 281: loss 2.134 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.634 | clip 0.6\n",
            "INFO: Epoch 281: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 282: loss 2.123 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.411 | clip 0.5\n",
            "INFO: Epoch 282: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 283: loss 2.124 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.718 | clip 0.55\n",
            "INFO: Epoch 283: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 284: loss 2.116 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.408 | clip 0.5\n",
            "INFO: Epoch 284: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 285: loss 2.109 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.555 | clip 0.55\n",
            "INFO: Epoch 285: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 286: loss 2.105 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.353 | clip 0.45\n",
            "INFO: Epoch 286: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 287: loss 2.098 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.721 | clip 0.5\n",
            "INFO: Epoch 287: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 288: loss 2.099 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.407 | clip 0.45\n",
            "INFO: Epoch 288: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 289: loss 2.093 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.518 | clip 0.5\n",
            "INFO: Epoch 289: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 290: loss 2.09 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.447 | clip 0.55\n",
            "INFO: Epoch 290: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 291: loss 2.08 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.636 | clip 0.5\n",
            "INFO: Epoch 291: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 292: loss 2.082 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.446 | clip 0.6\n",
            "INFO: Epoch 292: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 293: loss 2.081 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.469 | clip 0.5\n",
            "INFO: Epoch 293: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 294: loss 2.072 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.614 | clip 0.5\n",
            "INFO: Epoch 294: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 295: loss 2.071 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.439 | clip 0.55\n",
            "INFO: Epoch 295: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 296: loss 2.063 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.514 | clip 0.5\n",
            "INFO: Epoch 296: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 297: loss 2.062 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.533 | clip 0.55\n",
            "INFO: Epoch 297: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 298: loss 2.056 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.452 | clip 0.55\n",
            "INFO: Epoch 298: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 299: loss 2.049 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.438 | clip 0.55\n",
            "INFO: Epoch 299: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 300: loss 2.052 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.938 | clip 0.55\n",
            "INFO: Epoch 300: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 301: loss 2.045 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.637 | clip 0.55\n",
            "INFO: Epoch 301: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 302: loss 2.037 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.411 | clip 0.55\n",
            "INFO: Epoch 302: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 303: loss 2.034 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.523 | clip 0.55\n",
            "INFO: Epoch 303: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 304: loss 2.03 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.377 | clip 0.5\n",
            "INFO: Epoch 304: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 305: loss 2.03 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.665 | clip 0.5\n",
            "INFO: Epoch 305: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 306: loss 2.027 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.431 | clip 0.6\n",
            "INFO: Epoch 306: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 307: loss 2.024 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.791 | clip 0.5\n",
            "INFO: Epoch 307: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 308: loss 2.018 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.525 | clip 0.6\n",
            "INFO: Epoch 308: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 309: loss 2.014 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.824 | clip 0.5\n",
            "INFO: Epoch 309: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 310: loss 2.013 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.513 | clip 0.55\n",
            "INFO: Epoch 310: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 311: loss 2.007 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.676 | clip 0.5\n",
            "INFO: Epoch 311: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 312: loss 1.999 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.437 | clip 0.5\n",
            "INFO: Epoch 312: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 313: loss 2.003 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.885 | clip 0.55\n",
            "INFO: Epoch 313: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 314: loss 2 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.501 | clip 0.6\n",
            "INFO: Epoch 314: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 315: loss 1.991 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.481 | clip 0.5\n",
            "INFO: Epoch 315: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 316: loss 1.992 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.681 | clip 0.55\n",
            "INFO: Epoch 316: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 317: loss 1.982 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.537 | clip 0.5\n",
            "INFO: Epoch 317: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 318: loss 1.984 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.885 | clip 0.55\n",
            "INFO: Epoch 318: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 319: loss 1.978 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.793 | clip 0.5\n",
            "INFO: Epoch 319: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 320: loss 1.969 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.668 | clip 0.55\n",
            "INFO: Epoch 320: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 321: loss 1.968 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.52 | clip 0.5\n",
            "INFO: Epoch 321: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 322: loss 1.963 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.593 | clip 0.55\n",
            "INFO: Epoch 322: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 323: loss 1.963 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.605 | clip 0.6\n",
            "INFO: Epoch 323: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 324: loss 1.955 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.77 | clip 0.5\n",
            "INFO: Epoch 324: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 325: loss 1.951 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.609 | clip 0.5\n",
            "INFO: Epoch 325: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 326: loss 1.952 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.401 | clip 0.55\n",
            "INFO: Epoch 326: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 327: loss 1.952 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.831 | clip 0.6\n",
            "INFO: Epoch 327: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 328: loss 1.943 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.496 | clip 0.5\n",
            "INFO: Epoch 328: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 329: loss 1.939 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.96 | clip 0.55\n",
            "INFO: Epoch 329: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 330: loss 1.944 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.781 | clip 0.55\n",
            "INFO: Epoch 330: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 331: loss 1.931 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.714 | clip 0.55\n",
            "INFO: Epoch 331: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 332: loss 1.93 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.601 | clip 0.55\n",
            "INFO: Epoch 332: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 333: loss 1.931 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.703 | clip 0.5\n",
            "INFO: Epoch 333: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 334: loss 1.924 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.763 | clip 0.55\n",
            "INFO: Epoch 334: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 335: loss 1.921 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.85 | clip 0.5\n",
            "INFO: Epoch 335: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 336: loss 1.914 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.817 | clip 0.55\n",
            "INFO: Epoch 336: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 337: loss 1.91 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.948 | clip 0.5\n",
            "INFO: Epoch 337: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 338: loss 1.913 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.897 | clip 0.6\n",
            "INFO: Epoch 338: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 339: loss 1.907 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.725 | clip 0.5\n",
            "INFO: Epoch 339: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 340: loss 1.908 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.656 | clip 0.55\n",
            "INFO: Epoch 340: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 341: loss 1.902 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.884 | clip 0.55\n",
            "INFO: Epoch 341: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 342: loss 1.897 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.028 | clip 0.6\n",
            "INFO: Epoch 342: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 343: loss 1.896 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.685 | clip 0.55\n",
            "INFO: Epoch 343: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 344: loss 1.895 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.788 | clip 0.55\n",
            "INFO: Epoch 344: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 345: loss 1.884 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.71 | clip 0.65\n",
            "INFO: Epoch 345: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 346: loss 1.886 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.894 | clip 0.5\n",
            "INFO: Epoch 346: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 347: loss 1.876 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.554 | clip 0.55\n",
            "INFO: Epoch 347: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 348: loss 1.878 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 5.005 | clip 0.55\n",
            "INFO: Epoch 348: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 349: loss 1.876 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.738 | clip 0.6\n",
            "INFO: Epoch 349: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 350: loss 1.867 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.85 | clip 0.6\n",
            "INFO: Epoch 350: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 351: loss 1.867 | lr 0.0003 | num_tokens 9.1 | batch_size 500 | grad_norm 4.743 | clip 0.55\n",
            "INFO: Epoch 351: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E22 bs64+lr0.0005+dropout0.3"
      ],
      "metadata": {
        "id": "-q7z40MHCf40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_22/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 64 \\\n",
        "    --lr 0.0005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKUAGifACmsI",
        "outputId": "d8420b6a-f20a-4f10-a7d7-0539529d9ba5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_22/checkpoints --cuda --batch-size 64 --lr 0.0005\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 64, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_22/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 5.848 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.15 | clip 0.9108\n",
            "INFO: Epoch 000: valid_loss 5.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 172\n",
            "INFO: Epoch 001: loss 4.842 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.056 | clip 0.9108\n",
            "INFO: Epoch 001: valid_loss 4.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 145\n",
            "INFO: Epoch 002: loss 4.685 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.796 | clip 0.879\n",
            "INFO: Epoch 002: valid_loss 4.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 120\n",
            "INFO: Epoch 003: loss 4.516 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.447 | clip 0.8726\n",
            "INFO: Epoch 003: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101\n",
            "INFO: Epoch 004: loss 4.372 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.129 | clip 0.8662\n",
            "INFO: Epoch 004: valid_loss 4.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.9\n",
            "INFO: Epoch 005: loss 4.247 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.88 | clip 0.8599\n",
            "INFO: Epoch 005: valid_loss 4.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 75.7\n",
            "INFO: Epoch 006: loss 4.122 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.896 | clip 0.8599\n",
            "INFO: Epoch 006: valid_loss 4.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67.2\n",
            "INFO: Epoch 007: loss 4.017 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.81 | clip 0.8599\n",
            "INFO: Epoch 007: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.4\n",
            "INFO: Epoch 008: loss 3.928 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.798 | clip 0.8535\n",
            "INFO: Epoch 008: valid_loss 4.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 55.3\n",
            "INFO: Epoch 009: loss 3.832 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.775 | clip 0.8599\n",
            "INFO: Epoch 009: valid_loss 3.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 52.7\n",
            "INFO: Epoch 010: loss 3.758 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.888 | clip 0.8471\n",
            "INFO: Epoch 010: valid_loss 3.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 49.5\n",
            "INFO: Epoch 011: loss 3.692 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.855 | clip 0.8726\n",
            "INFO: Epoch 011: valid_loss 3.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47\n",
            "INFO: Epoch 012: loss 3.628 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.873 | clip 0.8854\n",
            "INFO: Epoch 012: valid_loss 3.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 44.8\n",
            "INFO: Epoch 013: loss 3.566 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 5.859 | clip 0.8854\n",
            "INFO: Epoch 013: valid_loss 3.73 | num_tokens 9.14 | batch_size 500 | valid_perplexity 41.9\n",
            "INFO: Epoch 014: loss 3.502 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.026 | clip 0.879\n",
            "INFO: Epoch 014: valid_loss 3.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 39.3\n",
            "INFO: Epoch 015: loss 3.44 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.001 | clip 0.9045\n",
            "INFO: Epoch 015: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.3\n",
            "INFO: Epoch 016: loss 3.38 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.048 | clip 0.9045\n",
            "INFO: Epoch 016: valid_loss 3.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.8\n",
            "INFO: Epoch 017: loss 3.327 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.185 | clip 0.9045\n",
            "INFO: Epoch 017: valid_loss 3.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 33.4\n",
            "INFO: Epoch 018: loss 3.271 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.169 | clip 0.9236\n",
            "INFO: Epoch 018: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.4\n",
            "INFO: Epoch 019: loss 3.218 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.218 | clip 0.9299\n",
            "INFO: Epoch 019: valid_loss 3.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 30.1\n",
            "INFO: Epoch 020: loss 3.173 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.262 | clip 0.9554\n",
            "INFO: Epoch 020: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.9\n",
            "INFO: Epoch 021: loss 3.121 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.351 | clip 0.9236\n",
            "INFO: Epoch 021: valid_loss 3.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 27.4\n",
            "INFO: Epoch 022: loss 3.077 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.488 | clip 0.949\n",
            "INFO: Epoch 022: valid_loss 3.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.6\n",
            "INFO: Epoch 023: loss 3.035 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.661 | clip 0.9554\n",
            "INFO: Epoch 023: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.3\n",
            "INFO: Epoch 024: loss 3.003 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.638 | clip 0.9618\n",
            "INFO: Epoch 024: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.3\n",
            "INFO: Epoch 025: loss 2.952 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.589 | clip 0.9618\n",
            "INFO: Epoch 025: valid_loss 3.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.7\n",
            "INFO: Epoch 026: loss 2.909 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.723 | clip 0.9618\n",
            "INFO: Epoch 026: valid_loss 3.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.1\n",
            "INFO: Epoch 027: loss 2.882 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.908 | clip 0.9682\n",
            "INFO: Epoch 027: valid_loss 3.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22.2\n",
            "INFO: Epoch 028: loss 2.839 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.92 | clip 0.9554\n",
            "INFO: Epoch 028: valid_loss 3.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.6\n",
            "INFO: Epoch 029: loss 2.805 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.893 | clip 0.9618\n",
            "INFO: Epoch 029: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.1\n",
            "INFO: Epoch 030: loss 2.768 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 6.916 | clip 0.9682\n",
            "INFO: Epoch 030: valid_loss 3.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21.3\n",
            "INFO: Epoch 031: loss 2.744 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.298 | clip 0.9873\n",
            "INFO: Epoch 031: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.4\n",
            "INFO: Epoch 032: loss 2.71 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.313 | clip 0.9873\n",
            "INFO: Epoch 032: valid_loss 2.98 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.6\n",
            "INFO: Epoch 033: loss 2.676 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.254 | clip 0.949\n",
            "INFO: Epoch 033: valid_loss 2.94 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.9\n",
            "INFO: Epoch 034: loss 2.644 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.426 | clip 0.9682\n",
            "INFO: Epoch 034: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.2\n",
            "INFO: Epoch 035: loss 2.618 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.366 | clip 0.9873\n",
            "INFO: Epoch 035: valid_loss 2.9 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.2\n",
            "INFO: Epoch 036: loss 2.589 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.364 | clip 0.9873\n",
            "INFO: Epoch 036: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.9\n",
            "INFO: Epoch 037: loss 2.561 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.349 | clip 0.9873\n",
            "INFO: Epoch 037: valid_loss 2.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.6\n",
            "INFO: Epoch 038: loss 2.534 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.471 | clip 1\n",
            "INFO: Epoch 038: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.2\n",
            "INFO: Epoch 039: loss 2.507 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.565 | clip 0.9873\n",
            "INFO: Epoch 039: valid_loss 2.85 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 040: loss 2.488 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.754 | clip 0.9936\n",
            "INFO: Epoch 040: valid_loss 2.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.4\n",
            "INFO: Epoch 041: loss 2.453 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.668 | clip 0.9873\n",
            "INFO: Epoch 041: valid_loss 2.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.5\n",
            "INFO: Epoch 042: loss 2.431 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.818 | clip 0.9873\n",
            "INFO: Epoch 042: valid_loss 2.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 043: loss 2.407 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.862 | clip 0.9873\n",
            "INFO: Epoch 043: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 044: loss 2.383 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 7.879 | clip 0.9873\n",
            "INFO: Epoch 044: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 045: loss 2.357 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.074 | clip 0.9936\n",
            "INFO: Epoch 045: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 046: loss 2.341 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.001 | clip 1\n",
            "INFO: Epoch 046: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 047: loss 2.311 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.054 | clip 0.9936\n",
            "INFO: Epoch 047: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 048: loss 2.3 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.362 | clip 1\n",
            "INFO: Epoch 048: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 049: loss 2.274 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.353 | clip 0.9936\n",
            "INFO: Epoch 049: valid_loss 2.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 050: loss 2.25 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.363 | clip 1\n",
            "INFO: Epoch 050: valid_loss 2.67 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 051: loss 2.233 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.162 | clip 1\n",
            "INFO: Epoch 051: valid_loss 2.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 052: loss 2.209 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.262 | clip 1\n",
            "INFO: Epoch 052: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 053: loss 2.187 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.363 | clip 1\n",
            "INFO: Epoch 053: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 054: loss 2.173 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.554 | clip 1\n",
            "INFO: Epoch 054: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 055: loss 2.154 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.389 | clip 1\n",
            "INFO: Epoch 055: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 056: loss 2.13 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.407 | clip 1\n",
            "INFO: Epoch 056: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 057: loss 2.118 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.586 | clip 1\n",
            "INFO: Epoch 057: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 058: loss 2.102 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.564 | clip 1\n",
            "INFO: Epoch 058: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 059: loss 2.081 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.679 | clip 1\n",
            "INFO: Epoch 059: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 060: loss 2.063 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.719 | clip 1\n",
            "INFO: Epoch 060: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 061: loss 2.044 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.708 | clip 1\n",
            "INFO: Epoch 061: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 062: loss 2.027 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.628 | clip 1\n",
            "INFO: Epoch 062: valid_loss 2.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 063: loss 2.009 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.694 | clip 1\n",
            "INFO: Epoch 063: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 064: loss 1.992 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.893 | clip 0.9936\n",
            "INFO: Epoch 064: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 065: loss 1.977 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.823 | clip 1\n",
            "INFO: Epoch 065: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 066: loss 1.965 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.857 | clip 1\n",
            "INFO: Epoch 066: valid_loss 2.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.6\n",
            "INFO: Epoch 067: loss 1.954 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.907 | clip 1\n",
            "INFO: Epoch 067: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.1\n",
            "INFO: Epoch 068: loss 1.935 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.911 | clip 0.9936\n",
            "INFO: Epoch 068: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 069: loss 1.923 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.985 | clip 0.9936\n",
            "INFO: Epoch 069: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 070: loss 1.901 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.001 | clip 1\n",
            "INFO: Epoch 070: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.8\n",
            "INFO: Epoch 071: loss 1.893 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.017 | clip 0.9936\n",
            "INFO: Epoch 071: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 072: loss 1.879 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.033 | clip 1\n",
            "INFO: Epoch 072: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 073: loss 1.863 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.208 | clip 1\n",
            "INFO: Epoch 073: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 074: loss 1.853 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 8.99 | clip 1\n",
            "INFO: Epoch 074: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 075: loss 1.834 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.265 | clip 1\n",
            "INFO: Epoch 075: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 076: loss 1.825 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.128 | clip 0.9936\n",
            "INFO: Epoch 076: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 077: loss 1.811 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.194 | clip 0.9873\n",
            "INFO: Epoch 077: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 078: loss 1.798 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.324 | clip 1\n",
            "INFO: Epoch 078: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 079: loss 1.788 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.266 | clip 1\n",
            "INFO: Epoch 079: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 080: loss 1.771 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.573 | clip 0.9936\n",
            "INFO: Epoch 080: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 081: loss 1.769 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.285 | clip 1\n",
            "INFO: Epoch 081: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 082: loss 1.746 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.47 | clip 0.9936\n",
            "INFO: Epoch 082: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 083: loss 1.743 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.404 | clip 1\n",
            "INFO: Epoch 083: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 084: loss 1.729 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.312 | clip 0.9936\n",
            "INFO: Epoch 084: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 085: loss 1.711 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.338 | clip 0.9936\n",
            "INFO: Epoch 085: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 086: loss 1.708 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.436 | clip 1\n",
            "INFO: Epoch 086: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 087: loss 1.695 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.636 | clip 0.9936\n",
            "INFO: Epoch 087: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 088: loss 1.682 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.532 | clip 1\n",
            "INFO: Epoch 088: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 089: loss 1.673 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.807 | clip 1\n",
            "INFO: Epoch 089: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 090: loss 1.665 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.519 | clip 1\n",
            "INFO: Epoch 090: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 091: loss 1.651 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.469 | clip 1\n",
            "INFO: Epoch 091: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 092: loss 1.643 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.704 | clip 1\n",
            "INFO: Epoch 092: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 093: loss 1.632 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.626 | clip 1\n",
            "INFO: Epoch 093: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 094: loss 1.624 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.502 | clip 1\n",
            "INFO: Epoch 094: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 095: loss 1.612 | lr 0.0005 | num_tokens 9.309 | batch_size 63.69 | grad_norm 9.613 | clip 1\n",
            "INFO: Epoch 095: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E23 bs100+lr0.0005+dropout0.3+lexical"
      ],
      "metadata": {
        "id": "dbzKajYoIfNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared_base \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiment_23/checkpoints \\\n",
        "    --cuda \\\n",
        "    --batch-size 100 \\\n",
        "    --lr 0.0005 \\\n",
        "    --decoder-use-lexical-model True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8yA_MzoIes_",
        "outputId": "0c96e177-91ff-4f8b-880a-5f8f7694756a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared_base --source-lang fr --target-lang en --save-dir assignments/03/experiment_23/checkpoints --cuda --batch-size 100 --lr 0.0005 --decoder-use-lexical-model True\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 100, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_23/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'decoder_use_lexical_model': 'True', 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Epoch 000: loss 6.219 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.507 | clip 0.86\n",
            "INFO: Epoch 000: valid_loss 5.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 169\n",
            "INFO: Epoch 001: loss 4.971 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.593 | clip 0.91\n",
            "INFO: Epoch 001: valid_loss 5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 148\n",
            "INFO: Epoch 002: loss 4.805 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.487 | clip 0.78\n",
            "INFO: Epoch 002: valid_loss 4.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 133\n",
            "INFO: Epoch 003: loss 4.699 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.126 | clip 0.79\n",
            "INFO: Epoch 003: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 115\n",
            "INFO: Epoch 004: loss 4.585 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.828 | clip 0.77\n",
            "INFO: Epoch 004: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 99\n",
            "INFO: Epoch 005: loss 4.46 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.682 | clip 0.74\n",
            "INFO: Epoch 005: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.4\n",
            "INFO: Epoch 006: loss 4.355 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.551 | clip 0.73\n",
            "INFO: Epoch 006: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.7\n",
            "INFO: Epoch 007: loss 4.258 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.387 | clip 0.73\n",
            "INFO: Epoch 007: valid_loss 4.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 73.9\n",
            "INFO: Epoch 008: loss 4.164 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.311 | clip 0.72\n",
            "INFO: Epoch 008: valid_loss 4.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 67\n",
            "INFO: Epoch 009: loss 4.075 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.173 | clip 0.69\n",
            "INFO: Epoch 009: valid_loss 4.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 61.5\n",
            "INFO: Epoch 010: loss 3.997 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.158 | clip 0.68\n",
            "INFO: Epoch 010: valid_loss 4.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 57.1\n",
            "INFO: Epoch 011: loss 3.923 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.092 | clip 0.69\n",
            "INFO: Epoch 011: valid_loss 3.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 53.2\n",
            "INFO: Epoch 012: loss 3.851 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.091 | clip 0.67\n",
            "INFO: Epoch 012: valid_loss 3.93 | num_tokens 9.14 | batch_size 500 | valid_perplexity 50.9\n",
            "INFO: Epoch 013: loss 3.792 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.097 | clip 0.67\n",
            "INFO: Epoch 013: valid_loss 3.87 | num_tokens 9.14 | batch_size 500 | valid_perplexity 47.9\n",
            "INFO: Epoch 014: loss 3.723 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.115 | clip 0.67\n",
            "INFO: Epoch 014: valid_loss 3.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 45.2\n",
            "INFO: Epoch 015: loss 3.669 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.126 | clip 0.65\n",
            "INFO: Epoch 015: valid_loss 3.76 | num_tokens 9.14 | batch_size 500 | valid_perplexity 42.8\n",
            "INFO: Epoch 016: loss 3.607 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.087 | clip 0.67\n",
            "INFO: Epoch 016: valid_loss 3.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 40.9\n",
            "INFO: Epoch 017: loss 3.558 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.114 | clip 0.69\n",
            "INFO: Epoch 017: valid_loss 3.66 | num_tokens 9.14 | batch_size 500 | valid_perplexity 38.9\n",
            "INFO: Epoch 018: loss 3.502 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.064 | clip 0.73\n",
            "INFO: Epoch 018: valid_loss 3.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 37.4\n",
            "INFO: Epoch 019: loss 3.453 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.055 | clip 0.71\n",
            "INFO: Epoch 019: valid_loss 3.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 35.6\n",
            "INFO: Epoch 020: loss 3.403 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.104 | clip 0.71\n",
            "INFO: Epoch 020: valid_loss 3.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 34.1\n",
            "INFO: Epoch 021: loss 3.354 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.105 | clip 0.72\n",
            "INFO: Epoch 021: valid_loss 3.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 32.5\n",
            "INFO: Epoch 022: loss 3.307 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.175 | clip 0.76\n",
            "INFO: Epoch 022: valid_loss 3.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 31.3\n",
            "INFO: Epoch 023: loss 3.265 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.147 | clip 0.72\n",
            "INFO: Epoch 023: valid_loss 3.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 29.7\n",
            "INFO: Epoch 024: loss 3.216 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.182 | clip 0.74\n",
            "INFO: Epoch 024: valid_loss 3.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.8\n",
            "INFO: Epoch 025: loss 3.175 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.208 | clip 0.75\n",
            "INFO: Epoch 025: valid_loss 3.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 28.2\n",
            "INFO: Epoch 026: loss 3.138 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.232 | clip 0.79\n",
            "INFO: Epoch 026: valid_loss 3.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.2\n",
            "INFO: Epoch 027: loss 3.095 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.246 | clip 0.79\n",
            "INFO: Epoch 027: valid_loss 3.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 26.1\n",
            "INFO: Epoch 028: loss 3.057 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.298 | clip 0.79\n",
            "INFO: Epoch 028: valid_loss 3.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.8\n",
            "INFO: Epoch 029: loss 3.017 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.281 | clip 0.76\n",
            "INFO: Epoch 029: valid_loss 3.19 | num_tokens 9.14 | batch_size 500 | valid_perplexity 24.2\n",
            "INFO: Epoch 030: loss 2.981 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.365 | clip 0.75\n",
            "INFO: Epoch 030: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 031: loss 2.947 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.449 | clip 0.79\n",
            "INFO: Epoch 031: valid_loss 3.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 23.5\n",
            "INFO: Epoch 032: loss 2.925 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.554 | clip 0.81\n",
            "INFO: Epoch 032: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 033: loss 2.879 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.418 | clip 0.82\n",
            "INFO: Epoch 033: valid_loss 3.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 034: loss 2.854 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.55 | clip 0.81\n",
            "INFO: Epoch 034: valid_loss 3.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 21\n",
            "INFO: Epoch 035: loss 2.822 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.584 | clip 0.82\n",
            "INFO: Epoch 035: valid_loss 3.03 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.7\n",
            "INFO: Epoch 036: loss 2.794 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.596 | clip 0.84\n",
            "INFO: Epoch 036: valid_loss 3.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 20.2\n",
            "INFO: Epoch 037: loss 2.767 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.682 | clip 0.85\n",
            "INFO: Epoch 037: valid_loss 2.99 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 038: loss 2.742 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.636 | clip 0.81\n",
            "INFO: Epoch 038: valid_loss 2.95 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.1\n",
            "INFO: Epoch 039: loss 2.707 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.69 | clip 0.8\n",
            "INFO: Epoch 039: valid_loss 2.97 | num_tokens 9.14 | batch_size 500 | valid_perplexity 19.5\n",
            "INFO: Epoch 040: loss 2.688 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.752 | clip 0.83\n",
            "INFO: Epoch 040: valid_loss 2.92 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 041: loss 2.658 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.702 | clip 0.82\n",
            "INFO: Epoch 041: valid_loss 2.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18.4\n",
            "INFO: Epoch 042: loss 2.634 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.871 | clip 0.85\n",
            "INFO: Epoch 042: valid_loss 2.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 043: loss 2.61 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.843 | clip 0.83\n",
            "INFO: Epoch 043: valid_loss 2.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.7\n",
            "INFO: Epoch 044: loss 2.583 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.989 | clip 0.83\n",
            "INFO: Epoch 044: valid_loss 2.86 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17.5\n",
            "INFO: Epoch 045: loss 2.566 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.038 | clip 0.87\n",
            "INFO: Epoch 045: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 17\n",
            "INFO: Epoch 046: loss 2.543 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 5.938 | clip 0.84\n",
            "INFO: Epoch 046: valid_loss 2.82 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.8\n",
            "INFO: Epoch 047: loss 2.517 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.07 | clip 0.85\n",
            "INFO: Epoch 047: valid_loss 2.83 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.9\n",
            "INFO: Epoch 048: loss 2.501 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.116 | clip 0.88\n",
            "INFO: Epoch 048: valid_loss 2.79 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16.3\n",
            "INFO: Epoch 049: loss 2.478 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.104 | clip 0.85\n",
            "INFO: Epoch 049: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 16\n",
            "INFO: Epoch 050: loss 2.454 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.162 | clip 0.88\n",
            "INFO: Epoch 050: valid_loss 2.77 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 051: loss 2.434 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.252 | clip 0.86\n",
            "INFO: Epoch 051: valid_loss 2.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.7\n",
            "INFO: Epoch 052: loss 2.412 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.226 | clip 0.87\n",
            "INFO: Epoch 052: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 053: loss 2.39 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.279 | clip 0.84\n",
            "INFO: Epoch 053: valid_loss 2.74 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 054: loss 2.373 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.252 | clip 0.89\n",
            "INFO: Epoch 054: valid_loss 2.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15.2\n",
            "INFO: Epoch 055: loss 2.355 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.525 | clip 0.84\n",
            "INFO: Epoch 055: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 056: loss 2.334 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.352 | clip 0.84\n",
            "INFO: Epoch 056: valid_loss 2.7 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.8\n",
            "INFO: Epoch 057: loss 2.317 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.365 | clip 0.88\n",
            "INFO: Epoch 057: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 058: loss 2.301 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.362 | clip 0.9\n",
            "INFO: Epoch 058: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.6\n",
            "INFO: Epoch 059: loss 2.277 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.491 | clip 0.89\n",
            "INFO: Epoch 059: valid_loss 2.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14.5\n",
            "INFO: Epoch 060: loss 2.268 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.467 | clip 0.93\n",
            "INFO: Epoch 060: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 061: loss 2.243 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.46 | clip 0.86\n",
            "INFO: Epoch 061: valid_loss 2.64 | num_tokens 9.14 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 062: loss 2.23 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.603 | clip 0.88\n",
            "INFO: Epoch 062: valid_loss 2.63 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 063: loss 2.209 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.626 | clip 0.91\n",
            "INFO: Epoch 063: valid_loss 2.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 064: loss 2.198 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.603 | clip 0.89\n",
            "INFO: Epoch 064: valid_loss 2.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 065: loss 2.176 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.716 | clip 0.89\n",
            "INFO: Epoch 065: valid_loss 2.6 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 066: loss 2.161 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.666 | clip 0.91\n",
            "INFO: Epoch 066: valid_loss 2.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 067: loss 2.148 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.898 | clip 0.91\n",
            "INFO: Epoch 067: valid_loss 2.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 068: loss 2.136 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.853 | clip 0.92\n",
            "INFO: Epoch 068: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 069: loss 2.113 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.855 | clip 0.92\n",
            "INFO: Epoch 069: valid_loss 2.57 | num_tokens 9.14 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 070: loss 2.101 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.837 | clip 0.9\n",
            "INFO: Epoch 070: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 071: loss 2.084 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.873 | clip 0.91\n",
            "INFO: Epoch 071: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 072: loss 2.072 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.936 | clip 0.91\n",
            "INFO: Epoch 072: valid_loss 2.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.7\n",
            "INFO: Epoch 073: loss 2.059 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.015 | clip 0.93\n",
            "INFO: Epoch 073: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.4\n",
            "INFO: Epoch 074: loss 2.045 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.933 | clip 0.93\n",
            "INFO: Epoch 074: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 075: loss 2.027 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.026 | clip 0.9\n",
            "INFO: Epoch 075: valid_loss 2.52 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.5\n",
            "INFO: Epoch 076: loss 2.013 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.07 | clip 0.92\n",
            "INFO: Epoch 076: valid_loss 2.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.3\n",
            "INFO: Epoch 077: loss 2.004 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 6.967 | clip 0.93\n",
            "INFO: Epoch 077: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 078: loss 1.989 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.025 | clip 0.94\n",
            "INFO: Epoch 078: valid_loss 2.5 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12.2\n",
            "INFO: Epoch 079: loss 1.975 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.174 | clip 0.93\n",
            "INFO: Epoch 079: valid_loss 2.49 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 080: loss 1.966 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.111 | clip 0.94\n",
            "INFO: Epoch 080: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 081: loss 1.948 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.158 | clip 0.93\n",
            "INFO: Epoch 081: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 12\n",
            "INFO: Epoch 082: loss 1.941 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.158 | clip 0.94\n",
            "INFO: Epoch 082: valid_loss 2.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 083: loss 1.929 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.276 | clip 0.92\n",
            "INFO: Epoch 083: valid_loss 2.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.9\n",
            "INFO: Epoch 084: loss 1.916 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.172 | clip 0.95\n",
            "INFO: Epoch 084: valid_loss 2.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.7\n",
            "INFO: Epoch 085: loss 1.899 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.299 | clip 0.94\n",
            "INFO: Epoch 085: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 086: loss 1.893 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.248 | clip 0.93\n",
            "INFO: Epoch 086: valid_loss 2.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.5\n",
            "INFO: Epoch 087: loss 1.87 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.37 | clip 0.94\n",
            "INFO: Epoch 087: valid_loss 2.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.6\n",
            "INFO: Epoch 088: loss 1.872 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.284 | clip 0.94\n",
            "INFO: Epoch 088: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 089: loss 1.852 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.246 | clip 0.95\n",
            "INFO: Epoch 089: valid_loss 2.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.4\n",
            "INFO: Epoch 090: loss 1.844 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.486 | clip 0.95\n",
            "INFO: Epoch 090: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 091: loss 1.833 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.422 | clip 0.95\n",
            "INFO: Epoch 091: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.2\n",
            "INFO: Epoch 092: loss 1.817 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.447 | clip 0.95\n",
            "INFO: Epoch 092: valid_loss 2.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.3\n",
            "INFO: Epoch 093: loss 1.812 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.45 | clip 0.95\n",
            "INFO: Epoch 093: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 094: loss 1.798 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.622 | clip 0.96\n",
            "INFO: Epoch 094: valid_loss 2.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11.1\n",
            "INFO: Epoch 095: loss 1.794 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.393 | clip 0.94\n",
            "INFO: Epoch 095: valid_loss 2.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 11\n",
            "INFO: Epoch 096: loss 1.773 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.527 | clip 0.92\n",
            "INFO: Epoch 096: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 097: loss 1.77 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.487 | clip 0.96\n",
            "INFO: Epoch 097: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 098: loss 1.764 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.858 | clip 0.98\n",
            "INFO: Epoch 098: valid_loss 2.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.9\n",
            "INFO: Epoch 099: loss 1.753 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.531 | clip 0.95\n",
            "INFO: Epoch 099: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 100: loss 1.735 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.72 | clip 0.94\n",
            "INFO: Epoch 100: valid_loss 2.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.8\n",
            "INFO: Epoch 101: loss 1.733 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.549 | clip 0.95\n",
            "INFO: Epoch 101: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 102: loss 1.72 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.737 | clip 0.96\n",
            "INFO: Epoch 102: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 103: loss 1.709 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.746 | clip 0.93\n",
            "INFO: Epoch 103: valid_loss 2.37 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.7\n",
            "INFO: Epoch 104: loss 1.701 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.64 | clip 0.95\n",
            "INFO: Epoch 104: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 105: loss 1.689 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.747 | clip 0.95\n",
            "INFO: Epoch 105: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 106: loss 1.684 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.716 | clip 0.97\n",
            "INFO: Epoch 106: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.6\n",
            "INFO: Epoch 107: loss 1.67 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.696 | clip 0.97\n",
            "INFO: Epoch 107: valid_loss 2.36 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 108: loss 1.67 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.699 | clip 0.95\n",
            "INFO: Epoch 108: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 109: loss 1.658 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.611 | clip 0.98\n",
            "INFO: Epoch 109: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
            "INFO: Epoch 110: loss 1.649 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.67 | clip 0.92\n",
            "INFO: Epoch 110: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 111: loss 1.642 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.674 | clip 0.96\n",
            "INFO: Epoch 111: valid_loss 2.35 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.5\n",
            "INFO: Epoch 112: loss 1.632 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.795 | clip 0.97\n",
            "INFO: Epoch 112: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 113: loss 1.621 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.74 | clip 0.96\n",
            "INFO: Epoch 113: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 114: loss 1.608 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.846 | clip 0.94\n",
            "INFO: Epoch 114: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 115: loss 1.61 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.866 | clip 0.96\n",
            "INFO: Epoch 115: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 116: loss 1.595 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.706 | clip 0.95\n",
            "INFO: Epoch 116: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 117: loss 1.592 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.046 | clip 0.91\n",
            "INFO: Epoch 117: valid_loss 2.33 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.3\n",
            "INFO: Epoch 118: loss 1.587 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.833 | clip 0.98\n",
            "INFO: Epoch 118: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 119: loss 1.571 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.905 | clip 0.95\n",
            "INFO: Epoch 119: valid_loss 2.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.2\n",
            "INFO: Epoch 120: loss 1.567 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.828 | clip 0.96\n",
            "INFO: Epoch 120: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 121: loss 1.56 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.824 | clip 0.95\n",
            "INFO: Epoch 121: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10\n",
            "INFO: Epoch 122: loss 1.554 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.848 | clip 0.94\n",
            "INFO: Epoch 122: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 123: loss 1.545 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.964 | clip 0.96\n",
            "INFO: Epoch 123: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 124: loss 1.537 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.862 | clip 0.95\n",
            "INFO: Epoch 124: valid_loss 2.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.93\n",
            "INFO: Epoch 125: loss 1.527 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.916 | clip 0.94\n",
            "INFO: Epoch 125: valid_loss 2.31 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.1\n",
            "INFO: Epoch 126: loss 1.525 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.95 | clip 0.93\n",
            "INFO: Epoch 126: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.83\n",
            "INFO: Epoch 127: loss 1.517 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.891 | clip 0.96\n",
            "INFO: Epoch 127: valid_loss 2.3 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.94\n",
            "INFO: Epoch 128: loss 1.514 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.924 | clip 0.95\n",
            "INFO: Epoch 128: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.9\n",
            "INFO: Epoch 129: loss 1.502 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.015 | clip 0.95\n",
            "INFO: Epoch 129: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.81\n",
            "INFO: Epoch 130: loss 1.486 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.925 | clip 0.97\n",
            "INFO: Epoch 130: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.85\n",
            "INFO: Epoch 131: loss 1.493 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.948 | clip 0.96\n",
            "INFO: Epoch 131: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.77\n",
            "INFO: Epoch 132: loss 1.477 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.968 | clip 0.93\n",
            "INFO: Epoch 132: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.83\n",
            "INFO: Epoch 133: loss 1.473 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.987 | clip 0.96\n",
            "INFO: Epoch 133: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.72\n",
            "INFO: Epoch 134: loss 1.469 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.023 | clip 0.96\n",
            "INFO: Epoch 134: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.68\n",
            "INFO: Epoch 135: loss 1.466 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.973 | clip 0.95\n",
            "INFO: Epoch 135: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.77\n",
            "INFO: Epoch 136: loss 1.459 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.137 | clip 0.97\n",
            "INFO: Epoch 136: valid_loss 2.29 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.86\n",
            "INFO: Epoch 137: loss 1.449 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.973 | clip 0.97\n",
            "INFO: Epoch 137: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.67\n",
            "INFO: Epoch 138: loss 1.453 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.007 | clip 0.95\n",
            "INFO: Epoch 138: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.67\n",
            "INFO: Epoch 139: loss 1.437 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 7.957 | clip 0.92\n",
            "INFO: Epoch 139: valid_loss 2.26 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.62\n",
            "INFO: Epoch 140: loss 1.43 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.236 | clip 0.96\n",
            "INFO: Epoch 140: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.76\n",
            "INFO: Epoch 141: loss 1.426 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.014 | clip 0.97\n",
            "INFO: Epoch 141: valid_loss 2.27 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.7\n",
            "INFO: Epoch 142: loss 1.419 | lr 0.0005 | num_tokens 9.1 | batch_size 100 | grad_norm 8.066 | clip 0.94\n",
            "INFO: Epoch 142: valid_loss 2.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.81\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "FgK8ezgw-kAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E1 bs1000"
      ],
      "metadata": {
        "id": "nokHew0yM_lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_bs/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_bs/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "id": "sQ3CcJqJNytv",
        "outputId": "33ca9a98-c281-4eab-8c94-797ad15d51b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:40:41] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_bs/checkpoints/checkpoint_best.pt --output assignments/03/experiment_bs/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 22:40:41] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_bs/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_bs/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_bs/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:40:41] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:40:41] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:40:42] Loaded a model from checkpoint assignments/03/experiment_bs/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_bs/translations.txt \\\n",
        "assignments/03/experiment_bs/translations.p.txt en"
      ],
      "metadata": {
        "id": "QyJNg-hyNy1T"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E3 bs1000+lr0.003+layer3"
      ],
      "metadata": {
        "id": "29UOxdJMNswu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_all/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_all/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHI5qP1g-FB7",
        "outputId": "79d7dc25-4dfd-4911-ca64-53707f745e71"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:41:33] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_all/checkpoints/checkpoint_best.pt --output assignments/03/experiment_all/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 22:41:33] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_all/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 3, 'decoder_num_layers': 3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_all/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_all/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:41:33] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:41:33] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:41:34] Loaded a model from checkpoint assignments/03/experiment_all/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_all/translations.txt \\\n",
        "assignments/03/experiment_all/translations.p.txt en"
      ],
      "metadata": {
        "id": "vWiKk_xB-L1s"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E4 bs1000+lr0.003+layer2"
      ],
      "metadata": {
        "id": "rLlkLOVZND8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_4/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_4/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 16"
      ],
      "metadata": {
        "id": "tRcPaqesMh3Q",
        "outputId": "c8724a1e-79d8-4e1b-d37a-65308ff1ab05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:43:18] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_4/checkpoints/checkpoint_best.pt --output assignments/03/experiment_4/translations.txt --cuda --batch-size 16\n",
            "[2024-11-06 22:43:18] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_4/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_4/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_4/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:43:18] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:43:18] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:43:19] Loaded a model from checkpoint assignments/03/experiment_4/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_4/translations.txt \\\n",
        "assignments/03/experiment_4/translations.p.txt en"
      ],
      "metadata": {
        "id": "NeGOnG8lRlQ1"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E5 bs1000+lr0.003+layer1"
      ],
      "metadata": {
        "id": "ujng2uqBUvrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_5/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_5/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 16"
      ],
      "metadata": {
        "id": "H3x3yQ7kUu7y",
        "outputId": "1d8cf8f6-c2de-46e3-ab50-10391c010075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:45:42] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_5/checkpoints/checkpoint_best.pt --output assignments/03/experiment_5/translations.txt --cuda --batch-size 16\n",
            "[2024-11-06 22:45:42] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.03, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_5/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_5/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_5/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:45:42] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:45:42] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:45:44] Loaded a model from checkpoint assignments/03/experiment_5/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_5/translations.txt \\\n",
        "assignments/03/experiment_5/translations.p.txt en"
      ],
      "metadata": {
        "id": "bUW4PLSuU2K3"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E6 bs1000+lr0.01+layer1"
      ],
      "metadata": {
        "id": "FsWiFsXEbIZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_6/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_6/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 16"
      ],
      "metadata": {
        "id": "5Cjb1DVPbHxp",
        "outputId": "26b9af05-448c-45e6-a548-d08feb9fab82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:47:45] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_6/checkpoints/checkpoint_best.pt --output assignments/03/experiment_6/translations.txt --cuda --batch-size 16\n",
            "[2024-11-06 22:47:45] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.01, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_6/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_6/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_6/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:47:45] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:47:45] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:47:46] Loaded a model from checkpoint assignments/03/experiment_6/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_6/translations.txt \\\n",
        "assignments/03/experiment_6/translations.p.txt en"
      ],
      "metadata": {
        "id": "9m2gTAKxbRTq"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E7 bs1000+lr0.001+layer2+dropout0.25"
      ],
      "metadata": {
        "id": "5jKPMxdCet1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_7/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_7/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "id": "Wm5_ToBFezS7",
        "outputId": "158efd8a-56eb-453c-c556-bdf69000db8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:03:38] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_7/checkpoints/checkpoint_best.pt --output assignments/03/experiment_7/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:03:38] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0024, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_7/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_7/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_7/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:03:38] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:03:38] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:03:39] Loaded a model from checkpoint assignments/03/experiment_7/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_7/translations.txt \\\n",
        "assignments/03/experiment_7/translations.p.txt en"
      ],
      "metadata": {
        "id": "7LNui7Hbe0zm"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E8 bs1000+lr0.0024+layer2"
      ],
      "metadata": {
        "id": "CCvFTJUE__34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_8/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_8/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOEHdmX-__S6",
        "outputId": "24073c76-0a46-4a96-98a6-60c214eb1375"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:50:33] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_8/checkpoints/checkpoint_best.pt --output assignments/03/experiment_8/translations.txt --cuda --batch-size 16\n",
            "[2024-11-06 22:50:33] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0024, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_8/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_8/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_8/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:50:33] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:50:33] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:50:35] Loaded a model from checkpoint assignments/03/experiment_8/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_8/translations.txt \\\n",
        "assignments/03/experiment_8/translations.p.txt en"
      ],
      "metadata": {
        "id": "mstP6awPAEFz"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E9 bs1000+lr0.0024"
      ],
      "metadata": {
        "id": "mHA3Hpt9DMRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_9/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_9/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oyug3MsDSxR",
        "outputId": "6e824cbd-0762-42e3-97f2-6956db4396c4"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:52:52] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_9/checkpoints/checkpoint_best.pt --output assignments/03/experiment_9/translations.txt --cuda --batch-size 16\n",
            "[2024-11-06 22:52:52] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0024, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_9/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_9/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_9/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:52:52] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:52:52] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:52:53] Loaded a model from checkpoint assignments/03/experiment_9/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_9/translations.txt \\\n",
        "assignments/03/experiment_9/translations.p.txt en"
      ],
      "metadata": {
        "id": "RR6UT5NCDVCw"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E10 bs500+lr0.001"
      ],
      "metadata": {
        "id": "jFIjWKcXGrzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_10/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_10/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PuvLpVXGxCg",
        "outputId": "2e1e1d0d-718b-40b7-d987-f571a0eb3112"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:54:49] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_10/checkpoints/checkpoint_best.pt --output assignments/03/experiment_10/translations.txt --cuda --batch-size 16\n",
            "[2024-11-06 22:54:49] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.001, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_10/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_10/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_10/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:54:49] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:54:49] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:54:51] Loaded a model from checkpoint assignments/03/experiment_10/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_10/translations.txt \\\n",
        "assignments/03/experiment_10/translations.p.txt en"
      ],
      "metadata": {
        "id": "PlPWZBWdGyTS"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E11 bs1000+lr0.0005"
      ],
      "metadata": {
        "id": "YqRcJuPiIgYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_11/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_11/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuATOhogIj1Z",
        "outputId": "1d71df59-b89e-44ba-deec-343f7e7afe32"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:56:49] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_11/checkpoints/checkpoint_best.pt --output assignments/03/experiment_11/translations.txt --cuda --batch-size 16\n",
            "[2024-11-06 22:56:49] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_11/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_11/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_11/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:56:49] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:56:49] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:56:50] Loaded a model from checkpoint assignments/03/experiment_11/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_11/translations.txt \\\n",
        "assignments/03/experiment_11/translations.p.txt en"
      ],
      "metadata": {
        "id": "Cey3AzfDInMf"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E12 bs1000+lr0.0008"
      ],
      "metadata": {
        "id": "J49y3PbCNRHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_12/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_12/translations_128.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jd8kWJnUqGU",
        "outputId": "8c118b28-a136-467c-ff7d-1bdc5cefd667"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:00:49] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_12/checkpoints/checkpoint_best.pt --output assignments/03/experiment_12/translations_128.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:00:49] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0008, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_12/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_12/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_12/translations_128.txt', 'max_len': 128}\n",
            "[2024-11-06 23:00:49] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:00:49] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:00:50] Loaded a model from checkpoint assignments/03/experiment_12/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_12/translations_128.txt \\\n",
        "assignments/03/experiment_12/translations_128.p.txt en"
      ],
      "metadata": {
        "id": "tEK3xWitUwPo"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E13 bs1000+lr0.0005+layer2"
      ],
      "metadata": {
        "id": "orkbTiszRXw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_13/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_13/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnsVeIGdRccW",
        "outputId": "06ffaf64-96c0-491e-c292-a4bd1378e1b9"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:01:13] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_13/checkpoints/checkpoint_best.pt --output assignments/03/experiment_13/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:01:13] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_13/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 2, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_13/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_13/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:01:13] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:01:13] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:01:14] Loaded a model from checkpoint assignments/03/experiment_13/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_13/translations.txt \\\n",
        "assignments/03/experiment_13/translations.p.txt en"
      ],
      "metadata": {
        "id": "-ENUMgLvUHFi"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E14 bs500+lr0.0005"
      ],
      "metadata": {
        "id": "nQAeu842TD5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_14/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_14/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plpM5DgITI3n",
        "outputId": "0e0f7ad6-fe4f-4d07-887a-a87fe307a779"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:01:40] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_14/checkpoints/checkpoint_best.pt --output assignments/03/experiment_14/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:01:40] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_14/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_14/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_14/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:01:40] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:01:40] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:01:41] Loaded a model from checkpoint assignments/03/experiment_14/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_14/translations.txt \\\n",
        "assignments/03/experiment_14/translations.p.txt en"
      ],
      "metadata": {
        "id": "51jsbjMlaq-A"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E15 bs1000+lr0.0005"
      ],
      "metadata": {
        "id": "Xsns__3Pbj2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_15/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_15/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiG3FFu-bn9L",
        "outputId": "303f14e6-65c1-4be6-87d5-39dbd44688f8"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:02:04] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_15/checkpoints/checkpoint_best.pt --output assignments/03/experiment_15/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:02:04] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_15/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_15/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_15/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:02:04] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:02:04] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:02:05] Loaded a model from checkpoint assignments/03/experiment_15/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_15/translations.txt \\\n",
        "assignments/03/experiment_15/translations.p.txt en"
      ],
      "metadata": {
        "id": "aLFQ8oNMboC5"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E16 bs1000+lr0.0005+dropout0.5"
      ],
      "metadata": {
        "id": "O_plGa6UlruR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_16/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_16/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U5Zi-c_lyJr",
        "outputId": "ac4f3954-7d04-44a3-d829-7cdceee5ea0f"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:02:28] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_16/checkpoints/checkpoint_best.pt --output assignments/03/experiment_16/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:02:28] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_16/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.5, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.5, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_16/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_16/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:02:28] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:02:28] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:02:29] Loaded a model from checkpoint assignments/03/experiment_16/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_16/translations.txt \\\n",
        "assignments/03/experiment_16/translations.p.txt en"
      ],
      "metadata": {
        "id": "LRyfhG1Hl0aW"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E17  bs1000+lr0.0005+dropout0.30"
      ],
      "metadata": {
        "id": "HFfPPrzQmxzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_17/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_17/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSg5vV-XnUEB",
        "outputId": "86c14353-ef2e-4a23-e6bb-a922ca972a08"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:02:50] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_17/checkpoints/checkpoint_best.pt --output assignments/03/experiment_17/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:02:50] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_17/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_17/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_17/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:02:50] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:02:50] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:02:51] Loaded a model from checkpoint assignments/03/experiment_17/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_17/translations.txt \\\n",
        "assignments/03/experiment_17/translations.p.txt en"
      ],
      "metadata": {
        "id": "81hz8n9gnUQC"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E18 bs1000+lr0.0004+dropout0.3"
      ],
      "metadata": {
        "id": "mshKT-PQz-y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_18/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_18/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd5IrcSr0DuT",
        "outputId": "0262c595-3eea-442b-9556-c945f7219aec"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:03:14] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_18/checkpoints/checkpoint_best.pt --output assignments/03/experiment_18/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:03:14] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0004, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_18/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_18/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_18/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:03:14] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:03:14] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:03:16] Loaded a model from checkpoint assignments/03/experiment_18/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_18/translations.txt \\\n",
        "assignments/03/experiment_18/translations.p.txt en"
      ],
      "metadata": {
        "id": "8uCqrw_Y0D1M"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E19 bs100+lr0.0005+dropout0.3"
      ],
      "metadata": {
        "id": "Mbo8Ydww5Y4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_19/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_19/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAWLjrbr5dax",
        "outputId": "36d065b9-5ab9-4b68-d1c2-999a6cc6392d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:39:02] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_19/checkpoints/checkpoint_best.pt --output assignments/03/experiment_19/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 22:39:02] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_19/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_19/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_19/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:39:02] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 22:39:02] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 22:39:03] Loaded a model from checkpoint assignments/03/experiment_19/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_19/translations.txt \\\n",
        "assignments/03/experiment_19/translations.p.txt en"
      ],
      "metadata": {
        "id": "n0q9khu55hJt"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E20 bs100+lr0.0005+dropout0.25"
      ],
      "metadata": {
        "id": "bnHXYXnP70lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_21/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_21/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecWRpzmd7zf1",
        "outputId": "c3833881-ca14-47bf-927e-9012e4e4d87b"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:04:04] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_21/checkpoints/checkpoint_best.pt --output assignments/03/experiment_21/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:04:04] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_21/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_21/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_21/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:04:04] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:04:04] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:04:05] Loaded a model from checkpoint assignments/03/experiment_21/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_21/translations.txt \\\n",
        "assignments/03/experiment_21/translations.p.txt en"
      ],
      "metadata": {
        "id": "4ONilN8x78es"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E22 bs64+lr0.0005+dropout0.3.  "
      ],
      "metadata": {
        "id": "z5p6fNA8H5ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_22/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_22/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHXrxTTXIFHh",
        "outputId": "9966f332-a4cb-4660-d9cd-2972dbc5ec74"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:06:20] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_22/checkpoints/checkpoint_best.pt --output assignments/03/experiment_22/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:06:20] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_22/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_22/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_22/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:06:20] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:06:20] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:06:21] Loaded a model from checkpoint assignments/03/experiment_22/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_22/translations.txt \\\n",
        "assignments/03/experiment_22/translations.p.txt en"
      ],
      "metadata": {
        "id": "Wj5e7A2mILAR"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E23 bs100+lr0.0005+dropout0.3+lexical"
      ],
      "metadata": {
        "id": "WJU52nHeO-fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python translate.py \\\n",
        "--data data/en-fr/prepared_base  \\\n",
        "--dicts data/en-fr/prepared_base  \\\n",
        "--checkpoint-path assignments/03/experiment_23/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiment_23/translations.txt \\\n",
        "--cuda \\\n",
        "--batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3TxvR7VPGSi",
        "outputId": "4142f764-619b-4a43-dfdc-23300330e27b"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 23:06:51] COMMAND: translate.py --data data/en-fr/prepared_base --dicts data/en-fr/prepared_base --checkpoint-path assignments/03/experiment_23/checkpoints/checkpoint_best.pt --output assignments/03/experiment_23/translations.txt --cuda --batch-size 128\n",
            "[2024-11-06 23:06:51] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared_base', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0005, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiment_23/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'decoder_use_lexical_model': 'True', 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared_base', 'checkpoint_path': 'assignments/03/experiment_23/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiment_23/translations.txt', 'max_len': 128}\n",
            "[2024-11-06 23:06:51] Loaded a source dictionary (fr) with 4000 words\n",
            "[2024-11-06 23:06:51] Loaded a target dictionary (en) with 4000 words\n",
            "[2024-11-06 23:06:52] Loaded a model from checkpoint assignments/03/experiment_23/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiment_23/translations.txt \\\n",
        "assignments/03/experiment_23/translations.p.txt en"
      ],
      "metadata": {
        "id": "zQUzEu2zPLxr"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU"
      ],
      "metadata": {
        "id": "aXxomgx2GsAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BA bs1+lr0.0003+layer1 16.8"
      ],
      "metadata": {
        "id": "C6SNPrSsHuUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/baseline/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "id": "EWRDAEARHwNM",
        "outputId": "b1340145-1b62-493d-f6f6-9a5488321244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 16.8,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"44.0/22.1/12.2/6.7 (BP = 1.000 ratio = 1.341 hyp_len = 5218 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E23 bs100+lr0.0005+dropout0.3+lexical 18.7"
      ],
      "metadata": {
        "id": "cQFgEdLrPQW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_23/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjFg7UODPP7N",
        "outputId": "00b9d247-ee21-42a6-f5a7-b3ff4b239d5d"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 18.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"52.1/24.8/13.5/7.1 (BP = 1.000 ratio = 1.042 hyp_len = 4054 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E22 bs64+lr0.0005+dropout0.3 16.8"
      ],
      "metadata": {
        "id": "m9dKYKYSINUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_22/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks55r4y1IS-s",
        "outputId": "def155b9-6b8f-4d94-b07e-f19fe064630d"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 16.8,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"49.3/22.4/11.8/6.1 (BP = 1.000 ratio = 1.088 hyp_len = 4235 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E21 bs500+lr0.0005+dropout0.3 15.0"
      ],
      "metadata": {
        "id": "h2t00w_nCNSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_21/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IueT5YuaCKyV",
        "outputId": "5fe62965-a3cb-40c4-d00c-cb112755627d"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 15.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"48.5/20.2/10.3/5.0 (BP = 1.000 ratio = 1.015 hyp_len = 3949 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E20 bs100+lr0.0005+dropout0.25 15.2"
      ],
      "metadata": {
        "id": "diU80OAT8F8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_20/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diCzJFu_8ONW",
        "outputId": "40dbe2aa-902f-48db-b1d6-fa8f2408d2e7"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 15.2,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"48.2/20.5/10.2/5.2 (BP = 1.000 ratio = 1.064 hyp_len = 4140 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E19 bs100+lr0.0005+dropout0.3 18.7 ------best"
      ],
      "metadata": {
        "id": "UaViwCyN6AoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_19/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNqfE94G6E6X",
        "outputId": "d351d56f-2f76-42cb-ed63-823de3d257fd"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 18.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"52.1/24.8/13.5/7.1 (BP = 1.000 ratio = 1.042 hyp_len = 4054 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E8 bs1000+lr0.0004 dropout 15.2"
      ],
      "metadata": {
        "id": "nQ7HdbCL08Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_18/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIttbc0N1A5O",
        "outputId": "1b517cb7-7a20-4398-8338-811318f9a128"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 15.2,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"49.0/20.3/10.3/5.2 (BP = 0.999 ratio = 0.999 hyp_len = 3888 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E17 bs1000+lr0.0005+dropout0.30 15.7"
      ],
      "metadata": {
        "id": "m26A0GZdrsMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_17/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrCUtPjlryOK",
        "outputId": "0a807782-a464-4357-e921-fd25b30b0f63"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 15.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"49.9/21.2/10.9/5.4 (BP = 0.999 ratio = 0.999 hyp_len = 3889 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E16 bs1000+lr0.0005+dropout0.5 7.7"
      ],
      "metadata": {
        "id": "AExSipeYluUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_16/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weZ5V6ZCmhYv",
        "outputId": "70036dd3-965b-44e1-fd66-9078cd9e9600"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 7.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"41.2/11.6/4.6/1.6 (BP = 1.000 ratio = 1.000 hyp_len = 3891 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E15 bs500+lr0.0003 16.7"
      ],
      "metadata": {
        "id": "_sOZgI0Ybumq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_15/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGTyFOgDbzfq",
        "outputId": "9f36e15f-e300-4f29-8fd5-fa5bca08044a"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 16.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"49.9/22.1/11.8/6.0 (BP = 1.000 ratio = 1.010 hyp_len = 3932 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E14 bs500+lr0.0005 13.6"
      ],
      "metadata": {
        "id": "FzgV33Ina3gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_14/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKxKkt1ma8TX",
        "outputId": "91869a43-31cf-454c-9f52-9f294838f81e"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 13.6,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"47.8/18.6/9.2/4.2 (BP = 1.000 ratio = 1.016 hyp_len = 3955 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##E13 bs1000+lr0.0005+layer2 0.6"
      ],
      "metadata": {
        "id": "gTu_x2GcTVrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_13/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv-0EaImTajI",
        "outputId": "63c7b8ef-0272-490e-9f6c-745230da4087"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 0.6,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"21.8/1.2/0.3/0.0 (BP = 0.851 ratio = 0.861 hyp_len = 3350 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E12 bs1000+lr0.0008 13.7"
      ],
      "metadata": {
        "id": "779VxfZdN4qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_12/translations_128.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VlsC5RDU60p",
        "outputId": "abc8c520-abdb-4358-968b-7782669954db"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 13.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"47.5/18.7/9.2/4.4 (BP = 1.000 ratio = 1.002 hyp_len = 3899 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E11 bs1000+lr0.0005 16.8"
      ],
      "metadata": {
        "id": "ErpyT2kOM0hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_11/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77G2kcEqM7Mj",
        "outputId": "f5713ea1-8585-4766-fec9-0c88c2854535"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 16.8,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"50.7/22.0/11.6/6.3 (BP = 0.993 ratio = 0.993 hyp_len = 3865 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E10 bs500+lr0.001 13.5"
      ],
      "metadata": {
        "id": "cmmejSx0Huzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_10/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv6SqwhzHuHl",
        "outputId": "b8bc6799-2d1b-4248-b5ea-4a8ca57af47c"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 13.5,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"48.3/19.3/9.5/4.3 (BP = 0.963 ratio = 0.964 hyp_len = 3750 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b9NW4pqkILsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E9 bs1000+lr0.0024 13.9"
      ],
      "metadata": {
        "id": "Yc3v6zUEEU5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_9/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJdDphA4EY0-",
        "outputId": "1201eaa0-218d-4a77-a2d7-fc377ec2ed2c"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 13.9,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"49.7/20.0/10.1/4.9 (BP = 0.928 ratio = 0.930 hyp_len = 3621 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E8 bs1000+lr0.024+layer2 0.00"
      ],
      "metadata": {
        "id": "7pbR-DRcALSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_8/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-uEGr10AQ_G",
        "outputId": "1a9305cd-9661-4184-f2f8-e976e0e231b6"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 0.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"8.3/0.1/0.1/0.0 (BP = 0.203 ratio = 0.385 hyp_len = 1500 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E7 bs1000+lr0.001+layer2 5.4\n"
      ],
      "metadata": {
        "id": "dhRxBy15fmIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_7/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "id": "mF3dI-8RfsKw",
        "outputId": "2d90b2b6-9419-4529-b0ad-897f92796d8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 5.4,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"37.1/8.6/3.0/0.9 (BP = 1.000 ratio = 1.084 hyp_len = 4219 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E6 bs1000+lr0.01+layer1 3.5"
      ],
      "metadata": {
        "id": "bppOOjYzb-cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_6/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "id": "LF-NOVyOcCGx",
        "outputId": "f8dc6bd1-5f56-47a6-a2bb-8707c6e35a4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 3.5,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"31.9/5.2/1.7/0.5 (BP = 1.000 ratio = 1.010 hyp_len = 3930 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E5 bs1000+lr0.003+layer1 15.5"
      ],
      "metadata": {
        "id": "IjOVNOqEU9N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_5/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "id": "MIjfp8LSTQ87",
        "outputId": "a8358da6-cd88-40ec-a035-9b69ba6198a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 15.5,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"51.8/21.6/11.9/6.5 (BP = 0.903 ratio = 0.908 hyp_len = 3532 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E4 bs1000+lr0.003+layer2 0.00"
      ],
      "metadata": {
        "id": "R4TYkr2kTSP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_4/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "id": "ctK4jITHTays",
        "outputId": "d1f7cdf8-7875-40ab-d9a8-73a576bec634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 0.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"8.3/0.1/0.1/0.0 (BP = 0.203 ratio = 0.385 hyp_len = 1499 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E3 bs1000+lr0.003+layer3 0.1\n",
        "--batch-size 1000 \\\n",
        "--lr 0.003 \\\n",
        "--encoder-num-layers 3 \\\n",
        "--decoder-num-layers 3"
      ],
      "metadata": {
        "id": "JWdAcHDoG-ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_all/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "id": "ydq8P73iHB1w",
        "outputId": "395e2168-ed2a-451e-f42a-a0e0d02e8968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 0.1,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"33.3/0.0/0.0/0.0 (BP = 0.318 ratio = 0.466 hyp_len = 1815 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E2 bs1000+lr0.001+layer1 15.1\n",
        "--batch-size 1000 \\\n",
        "--lr 0.001 \\"
      ],
      "metadata": {
        "id": "EJu3DmOrGu73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_bslr/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsw8XvYH-R3k",
        "outputId": "7bcd25bc-9bd5-4dbd-d8a0-35503109fbfa"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 15.1,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"51.3/21.3/11.2/5.5 (BP = 0.932 ratio = 0.934 hyp_len = 3636 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E1 bs1000 13.3"
      ],
      "metadata": {
        "id": "UO2zNF1hNmHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiment_bs/translations.p.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kjXxfcX75W9",
        "outputId": "46b706a2-47e4-4512-b071-49865290067a"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 13.3,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"47.0/18.6/8.9/4.0 (BP = 1.000 ratio = 1.021 hyp_len = 3972 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFnJsE6vjf8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}