{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSzgTsIMr2Bp",
        "outputId": "ae4c58b5-28ca-4335-9d3a-6c5f1a796514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/atmt_2024\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/atmt_2024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "970fTOt3noz0",
        "outputId": "907898a6-7e4d-49e0-8a29-c6fefbb2b7e0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBRCivkL33m_",
        "outputId": "8a0cdd0e-f47f-4fef-991e-5d4ecad0f70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assignments  fr_en_translations.pp.txt\t__pycache__\t  seq2seq\t     translate.py\n",
            "bpe_code.py  LICENSE\t\t\tREADME.md\t  share\n",
            "data\t     moses_scripts\t\trequirements.txt  train.py\n",
            "data-bin     preprocess.py\t\tscripts\t\t  translate_beam.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-yKEnji38VA",
        "outputId": "5e8fdb1b-eb55-4c37-dfda-35d7c52a08f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Collecting sacrebleu (from -r requirements.txt (line 3))\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.0.1 (from -r requirements.txt (line 4))\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r requirements.txt (line 4)) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 4)) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 4)) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 4)) (3.30.5)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 4))\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting portalocker (from sacrebleu->-r requirements.txt (line 3))\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r requirements.txt (line 3)) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r requirements.txt (line 3)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->-r requirements.txt (line 3))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r requirements.txt (line 3)) (5.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r requirements.txt (line 4)) (1.3.0)\n",
            "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, portalocker, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, colorama, sacrebleu, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 portalocker-2.10.1 sacrebleu-2.4.3 torch-2.0.1 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install subword_nmt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHxCA-H45UMr",
        "outputId": "95bea72e-8bcf-4747-dad9-d365f98356ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting subword_nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting mock (from subword_nmt)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword_nmt) (4.66.6)\n",
            "Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: mock, subword_nmt\n",
            "Successfully installed mock-5.1.0 subword_nmt-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocess"
      ],
      "metadata": {
        "id": "WEXcKd86tU4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab5000"
      ],
      "metadata": {
        "id": "FMdVHHektYuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash assignments/03/preprocess_data.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8z-ffKetW7y",
        "outputId": "48334d35-8b43-433a-eb5a-4a5c91aa520d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 18:27:47] COMMAND: preprocess.py --target-lang en --source-lang fr --dest-dir /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//prepared/ --train-prefix /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/train --valid-prefix /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/valid --test-prefix /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/test --tiny-train-prefix /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/tiny_train --threshold-src 1 --threshold-tgt 1 --num-words-src 5000 --num-words-tgt 5000\n",
            "[2024-11-06 18:27:47] Arguments: {'source_lang': 'fr', 'target_lang': 'en', 'train_prefix': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/train', 'tiny_train_prefix': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/tiny_train', 'valid_prefix': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/valid', 'test_prefix': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/test', 'dest_dir': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//prepared/', 'threshold_src': 1, 'num_words_src': 5000, 'threshold_tgt': 1, 'num_words_tgt': 5000, 'vocab_src': None, 'vocab_trg': None, 'quiet': False}\n",
            "[2024-11-06 18:27:47] COMMAND: preprocess.py --target-lang en --source-lang fr --dest-dir /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//prepared/ --train-prefix /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/train --valid-prefix /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/valid --test-prefix /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/test --tiny-train-prefix /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/tiny_train --threshold-src 1 --threshold-tgt 1 --num-words-src 5000 --num-words-tgt 5000\n",
            "[2024-11-06 18:27:47] Arguments: {'source_lang': 'fr', 'target_lang': 'en', 'train_prefix': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/train', 'tiny_train_prefix': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/tiny_train', 'valid_prefix': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/valid', 'test_prefix': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/test', 'dest_dir': '/content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//prepared/', 'threshold_src': 1, 'num_words_src': 5000, 'threshold_tgt': 1, 'num_words_tgt': 5000, 'vocab_src': None, 'vocab_trg': None, 'quiet': False}\n",
            "[2024-11-06 18:31:33] Built a source dictionary (fr) with 5000 words\n",
            "[2024-11-06 18:35:19] Built a target dictionary (en) with 5000 words\n",
            "[2024-11-06 18:39:11] Built a binary dataset for /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/train.fr: 10000 sentences, 133978 tokens, 0.755% replaced by unknown token\n",
            "[2024-11-06 18:39:35] Built a binary dataset for /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/tiny_train.fr: 1000 sentences, 13680 tokens, 1.067% replaced by unknown token\n",
            "[2024-11-06 18:39:47] Built a binary dataset for /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/valid.fr: 500 sentences, 6910 tokens, 1.143% replaced by unknown token\n",
            "[2024-11-06 18:39:59] Built a binary dataset for /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/test.fr: 500 sentences, 6915 tokens, 1.417% replaced by unknown token\n",
            "[2024-11-06 18:43:50] Built a binary dataset for /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/train.en: 10000 sentences, 111339 tokens, 0.083% replaced by unknown token\n",
            "[2024-11-06 18:44:13] Built a binary dataset for /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/tiny_train.en: 1000 sentences, 11398 tokens, 0.491% replaced by unknown token\n",
            "[2024-11-06 18:44:25] Built a binary dataset for /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/valid.en: 500 sentences, 5724 tokens, 0.507% replaced by unknown token\n",
            "[2024-11-06 18:44:37] Built a binary dataset for /content/drive/MyDrive/atmt_2024/assignments/03/../../data/en-fr//preprocessed/test.en: 500 sentences, 5803 tokens, 0.707% replaced by unknown token\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training the model"
      ],
      "metadata": {
        "id": "TkhwjE3Y6GX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab:4000"
      ],
      "metadata": {
        "id": "61bnnwPFoAf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiments/checkpoints \\\n",
        "    --cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOqWc4yKyT77",
        "outputId": "ad71654d-d204-4ba9-db53-312ccc1f1cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/experiments/checkpoints --cuda\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiments/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Loaded checkpoint assignments/03/experiments/checkpoints/checkpoint_last.pt\n",
            "INFO: Epoch 021: loss 2.292 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.6 | clip 0.9994\n",
            "INFO: Epoch 021: valid_loss 2.75 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15.6\n",
            "INFO: Epoch 022: loss 2.265 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 55.02 | clip 0.9996\n",
            "INFO: Epoch 022: valid_loss 2.71 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15.1\n",
            "INFO: Epoch 023: loss 2.243 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.77 | clip 0.9997\n",
            "INFO: Epoch 023: valid_loss 2.7 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 024: loss 2.224 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.58 | clip 0.9995\n",
            "INFO: Epoch 024: valid_loss 2.71 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 025: loss 2.203 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.52 | clip 0.9995\n",
            "INFO: Epoch 025: valid_loss 2.66 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 026: loss 2.182 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.23 | clip 0.9991\n",
            "INFO: Epoch 026: valid_loss 2.66 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 027: loss 2.159 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.93 | clip 0.9995\n",
            "INFO: Epoch 027: valid_loss 2.67 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 028: loss 2.145 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.84 | clip 0.9997\n",
            "INFO: Epoch 028: valid_loss 2.63 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 029: loss 2.12 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.95 | clip 0.9989\n",
            "INFO: Epoch 029: valid_loss 2.64 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14\n",
            "INFO: Epoch 030: loss 2.102 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.99 | clip 0.9991\n",
            "INFO: Epoch 030: valid_loss 2.61 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 031: loss 2.088 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.09 | clip 0.9993\n",
            "INFO: Epoch 031: valid_loss 2.62 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.8\n",
            "INFO: Epoch 032: loss 2.071 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.09 | clip 0.999\n",
            "INFO: Epoch 032: valid_loss 2.59 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.3\n",
            "INFO: Epoch 033: loss 2.053 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.02 | clip 0.9988\n",
            "INFO: Epoch 033: valid_loss 2.57 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 034: loss 2.046 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.07 | clip 0.9984\n",
            "INFO: Epoch 034: valid_loss 2.58 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 035: loss 2.028 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.78 | clip 0.999\n",
            "INFO: Epoch 035: valid_loss 2.57 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 036: loss 2.015 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 54.19 | clip 0.999\n",
            "INFO: Epoch 036: valid_loss 2.57 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab:5000"
      ],
      "metadata": {
        "id": "_EK5A_oUoEmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --data data/en-fr/prepared \\\n",
        "    --source-lang fr \\\n",
        "    --target-lang en \\\n",
        "    --save-dir assignments/03/experiments_bpe_1/checkpoints \\\n",
        "    --cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21JgySuqoGam",
        "outputId": "4ffdf206-fe40-4e4a-b879-82fcf56d7123"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/experiments_bpe_1/checkpoints --cuda\n",
            "INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiments_bpe_1/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 5000 words\n",
            "INFO: Loaded a target dictionary (en) with 5000 words\n",
            "INFO: Built a model with 1565576 parameters\n",
            "INFO: Epoch 000: loss 4.737 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 34.68 | clip 0.9992\n",
            "INFO: Epoch 000: valid_loss 4.7 | num_tokens 11.4 | batch_size 500 | valid_perplexity 110\n",
            "INFO: Epoch 001: loss 4.075 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 37.81 | clip 1\n",
            "INFO: Epoch 001: valid_loss 4.32 | num_tokens 11.4 | batch_size 500 | valid_perplexity 75.2\n",
            "INFO: Epoch 002: loss 3.756 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 41.67 | clip 1\n",
            "INFO: Epoch 002: valid_loss 4.16 | num_tokens 11.4 | batch_size 500 | valid_perplexity 63.9\n",
            "INFO: Epoch 003: loss 3.567 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 44.76 | clip 1\n",
            "INFO: Epoch 003: valid_loss 3.95 | num_tokens 11.4 | batch_size 500 | valid_perplexity 51.7\n",
            "INFO: Epoch 004: loss 3.433 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 46.45 | clip 1\n",
            "INFO: Epoch 004: valid_loss 3.79 | num_tokens 11.4 | batch_size 500 | valid_perplexity 44.1\n",
            "INFO: Epoch 005: loss 3.313 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 48.08 | clip 1\n",
            "INFO: Epoch 005: valid_loss 3.67 | num_tokens 11.4 | batch_size 500 | valid_perplexity 39.4\n",
            "INFO: Epoch 006: loss 3.22 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 49.24 | clip 1\n",
            "INFO: Epoch 006: valid_loss 3.65 | num_tokens 11.4 | batch_size 500 | valid_perplexity 38.4\n",
            "INFO: Epoch 007: loss 3.134 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 50.19 | clip 1\n",
            "INFO: Epoch 007: valid_loss 3.56 | num_tokens 11.4 | batch_size 500 | valid_perplexity 35.1\n",
            "INFO: Epoch 008: loss 3.057 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 50.94 | clip 1\n",
            "INFO: Epoch 008: valid_loss 3.4 | num_tokens 11.4 | batch_size 500 | valid_perplexity 29.9\n",
            "INFO: Epoch 009: loss 2.989 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52 | clip 1\n",
            "INFO: Epoch 009: valid_loss 3.42 | num_tokens 11.4 | batch_size 500 | valid_perplexity 30.6\n",
            "INFO: Epoch 010: loss 2.916 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.38 | clip 1\n",
            "INFO: Epoch 010: valid_loss 3.29 | num_tokens 11.4 | batch_size 500 | valid_perplexity 26.9\n",
            "INFO: Epoch 011: loss 2.861 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.49 | clip 0.9999\n",
            "INFO: Epoch 011: valid_loss 3.2 | num_tokens 11.4 | batch_size 500 | valid_perplexity 24.6\n",
            "INFO: Epoch 012: loss 2.815 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.14 | clip 1\n",
            "INFO: Epoch 012: valid_loss 3.17 | num_tokens 11.4 | batch_size 500 | valid_perplexity 23.8\n",
            "INFO: Epoch 013: loss 2.751 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.04 | clip 1\n",
            "INFO: Epoch 013: valid_loss 3.12 | num_tokens 11.4 | batch_size 500 | valid_perplexity 22.7\n",
            "INFO: Epoch 014: loss 2.716 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.8 | clip 0.9999\n",
            "INFO: Epoch 014: valid_loss 3.09 | num_tokens 11.4 | batch_size 500 | valid_perplexity 22\n",
            "INFO: Epoch 015: loss 2.671 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.48 | clip 1\n",
            "INFO: Epoch 015: valid_loss 3.05 | num_tokens 11.4 | batch_size 500 | valid_perplexity 21.2\n",
            "INFO: Epoch 016: loss 2.629 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.47 | clip 0.9999\n",
            "INFO: Epoch 016: valid_loss 3.02 | num_tokens 11.4 | batch_size 500 | valid_perplexity 20.6\n",
            "INFO: Epoch 017: loss 2.598 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.52 | clip 0.9998\n",
            "INFO: Epoch 017: valid_loss 3.02 | num_tokens 11.4 | batch_size 500 | valid_perplexity 20.5\n",
            "INFO: Epoch 018: loss 2.567 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.43 | clip 1\n",
            "INFO: Epoch 018: valid_loss 2.99 | num_tokens 11.4 | batch_size 500 | valid_perplexity 19.8\n",
            "INFO: Epoch 019: loss 2.54 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.59 | clip 1\n",
            "INFO: Epoch 019: valid_loss 2.93 | num_tokens 11.4 | batch_size 500 | valid_perplexity 18.7\n",
            "INFO: Epoch 020: loss 2.51 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.47 | clip 0.9995\n",
            "INFO: Epoch 020: valid_loss 2.92 | num_tokens 11.4 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 021: loss 2.487 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.38 | clip 0.9997\n",
            "INFO: Epoch 021: valid_loss 2.93 | num_tokens 11.4 | batch_size 500 | valid_perplexity 18.8\n",
            "INFO: Epoch 022: loss 2.461 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.11 | clip 0.9992\n",
            "INFO: Epoch 022: valid_loss 2.92 | num_tokens 11.4 | batch_size 500 | valid_perplexity 18.5\n",
            "INFO: Epoch 023: loss 2.449 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.07 | clip 0.9998\n",
            "INFO: Epoch 023: valid_loss 2.89 | num_tokens 11.4 | batch_size 500 | valid_perplexity 18\n",
            "INFO: Epoch 024: loss 2.425 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.91 | clip 0.9998\n",
            "INFO: Epoch 024: valid_loss 2.85 | num_tokens 11.4 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 025: loss 2.402 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.97 | clip 0.9998\n",
            "INFO: Epoch 025: valid_loss 2.85 | num_tokens 11.4 | batch_size 500 | valid_perplexity 17.4\n",
            "INFO: Epoch 026: loss 2.386 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.64 | clip 0.9996\n",
            "INFO: Epoch 026: valid_loss 2.85 | num_tokens 11.4 | batch_size 500 | valid_perplexity 17.3\n",
            "INFO: Epoch 027: loss 2.365 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.04 | clip 0.9998\n",
            "INFO: Epoch 027: valid_loss 2.81 | num_tokens 11.4 | batch_size 500 | valid_perplexity 16.6\n",
            "INFO: Epoch 028: loss 2.349 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53 | clip 0.9996\n",
            "INFO: Epoch 028: valid_loss 2.78 | num_tokens 11.4 | batch_size 500 | valid_perplexity 16.1\n",
            "INFO: Epoch 029: loss 2.333 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.37 | clip 0.9998\n",
            "INFO: Epoch 029: valid_loss 2.79 | num_tokens 11.4 | batch_size 500 | valid_perplexity 16.2\n",
            "INFO: Epoch 030: loss 2.315 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.9 | clip 0.9996\n",
            "INFO: Epoch 030: valid_loss 2.77 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 031: loss 2.296 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.06 | clip 0.9997\n",
            "INFO: Epoch 031: valid_loss 2.74 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15.5\n",
            "INFO: Epoch 032: loss 2.283 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.77 | clip 0.9996\n",
            "INFO: Epoch 032: valid_loss 2.74 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 033: loss 2.271 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.83 | clip 0.9994\n",
            "INFO: Epoch 033: valid_loss 2.76 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15.9\n",
            "INFO: Epoch 034: loss 2.256 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.24 | clip 0.9995\n",
            "INFO: Epoch 034: valid_loss 2.73 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15.4\n",
            "INFO: Epoch 035: loss 2.245 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.1 | clip 0.999\n",
            "INFO: Epoch 035: valid_loss 2.7 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.9\n",
            "INFO: Epoch 036: loss 2.229 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.93 | clip 0.9992\n",
            "INFO: Epoch 036: valid_loss 2.69 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.7\n",
            "INFO: Epoch 037: loss 2.213 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.1 | clip 0.9996\n",
            "INFO: Epoch 037: valid_loss 2.71 | num_tokens 11.4 | batch_size 500 | valid_perplexity 15\n",
            "INFO: Epoch 038: loss 2.206 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.91 | clip 0.999\n",
            "INFO: Epoch 038: valid_loss 2.67 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.4\n",
            "INFO: Epoch 039: loss 2.192 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.22 | clip 0.9991\n",
            "INFO: Epoch 039: valid_loss 2.66 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.3\n",
            "INFO: Epoch 040: loss 2.184 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.89 | clip 0.9989\n",
            "INFO: Epoch 040: valid_loss 2.64 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 041: loss 2.172 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.03 | clip 0.9991\n",
            "INFO: Epoch 041: valid_loss 2.63 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 042: loss 2.16 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.93 | clip 0.9989\n",
            "INFO: Epoch 042: valid_loss 2.65 | num_tokens 11.4 | batch_size 500 | valid_perplexity 14.1\n",
            "INFO: Epoch 043: loss 2.151 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.03 | clip 0.998\n",
            "INFO: Epoch 043: valid_loss 2.62 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 044: loss 2.136 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.85 | clip 0.9993\n",
            "INFO: Epoch 044: valid_loss 2.63 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.9\n",
            "INFO: Epoch 045: loss 2.121 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.6 | clip 0.9986\n",
            "INFO: Epoch 045: valid_loss 2.62 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 046: loss 2.117 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.8 | clip 0.9986\n",
            "INFO: Epoch 046: valid_loss 2.61 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.6\n",
            "INFO: Epoch 047: loss 2.11 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.92 | clip 0.9989\n",
            "INFO: Epoch 047: valid_loss 2.61 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.7\n",
            "INFO: Epoch 048: loss 2.098 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.98 | clip 0.999\n",
            "INFO: Epoch 048: valid_loss 2.6 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.5\n",
            "INFO: Epoch 049: loss 2.082 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.42 | clip 0.9988\n",
            "INFO: Epoch 049: valid_loss 2.58 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.2\n",
            "INFO: Epoch 050: loss 2.08 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.77 | clip 0.9991\n",
            "INFO: Epoch 050: valid_loss 2.59 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.4\n",
            "INFO: Epoch 051: loss 2.077 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.95 | clip 0.9991\n",
            "INFO: Epoch 051: valid_loss 2.57 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13.1\n",
            "INFO: Epoch 052: loss 2.064 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.01 | clip 0.9984\n",
            "INFO: Epoch 052: valid_loss 2.57 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 053: loss 2.058 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 53.2 | clip 0.9985\n",
            "INFO: Epoch 053: valid_loss 2.57 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 054: loss 2.047 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.84 | clip 0.9987\n",
            "INFO: Epoch 054: valid_loss 2.56 | num_tokens 11.4 | batch_size 500 | valid_perplexity 13\n",
            "INFO: Epoch 055: loss 2.036 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.83 | clip 0.9991\n",
            "INFO: Epoch 055: valid_loss 2.55 | num_tokens 11.4 | batch_size 500 | valid_perplexity 12.8\n",
            "INFO: Epoch 056: loss 2.034 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.82 | clip 0.9992\n",
            "INFO: Epoch 056: valid_loss 2.56 | num_tokens 11.4 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 057: loss 2.023 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.6 | clip 0.9979\n",
            "INFO: Epoch 057: valid_loss 2.56 | num_tokens 11.4 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: Epoch 058: loss 2.014 | lr 0.0003 | num_tokens 11.13 | batch_size 1 | grad_norm 52.72 | clip 0.998\n",
            "INFO: Epoch 058: valid_loss 2.55 | num_tokens 11.4 | batch_size 500 | valid_perplexity 12.9\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate"
      ],
      "metadata": {
        "id": "UG6jrHL56J12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab4000"
      ],
      "metadata": {
        "id": "O8YW_DKmp6IA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "translare: The resulting translations will be saved in a file provided to the command line argument\n",
        "--output."
      ],
      "metadata": {
        "id": "N9Bi8qcy6cYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/atmt_2024/translate.py \\\n",
        "--data data/en-fr/prepared \\\n",
        "--dicts data/en-fr/prepared \\\n",
        "--checkpoint-path assignments/03/experiments/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiments/fr_en_translations.txt \\\n",
        "--cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIduhBa054YU",
        "outputId": "bf73da5d-adb7-4aae-a8c4-183625224fb8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:47:49] COMMAND: /content/drive/MyDrive/atmt_2024/translate.py --data data/en-fr/prepared --dicts data/en-fr/prepared --checkpoint-path assignments/03/experiments/checkpoints/checkpoint_best.pt --output assignments/03/experiments/fr_en_translations.txt --cuda\n",
            "[2024-11-06 22:47:49] Arguments: {'cuda': True, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiments/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared', 'checkpoint_path': 'assignments/03/experiments/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiments/fr_en_translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:47:49] Loaded a source dictionary (fr) with 5000 words\n",
            "[2024-11-06 22:47:49] Loaded a target dictionary (en) with 5000 words\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/atmt_2024/translate.py\", line 121, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/atmt_2024/translate.py\", line 61, in main\n",
            "    model.load_state_dict(state_dict['model'])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for LSTMModel:\n",
            "\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([4000, 64]) from checkpoint, the shape in current model is torch.Size([5000, 64]).\n",
            "\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([4000, 64]) from checkpoint, the shape in current model is torch.Size([5000, 64]).\n",
            "\tsize mismatch for decoder.final_projection.weight: copying a param with shape torch.Size([4000, 128]) from checkpoint, the shape in current model is torch.Size([5000, 128]).\n",
            "\tsize mismatch for decoder.final_projection.bias: copying a param with shape torch.Size([4000]) from checkpoint, the shape in current model is torch.Size([5000]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Postprocess"
      ],
      "metadata": {
        "id": "oDZjIROQ6R_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiments/fr_en_translations.txt \\\n",
        "assignments/03/experiments/fr_en_translations.p.txt en"
      ],
      "metadata": {
        "id": "7o_ictXI6SZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove special characters"
      ],
      "metadata": {
        "id": "4a9HZZAMCxMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -r 's/(@@ )|(@@ ?$)//g' assignments/03/experiments/fr_en_translations.p.txt > assignments/03/experiments/fr_en_translations.pp.txt\n"
      ],
      "metadata": {
        "id": "7EkPqic9Cxov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove html"
      ],
      "metadata": {
        "id": "TmaskLNvFoxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import html\n",
        "import re\n",
        "\n",
        "def postprocessing_html(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "        text = infile.read()\n",
        "\n",
        "    # unescaped_content = html.unescape(text)\n",
        "    # 1. 替换 HTML 实体和分词标记\n",
        "    text = re.sub(r\"&apos;\", \"'\", text)  # 替换 &apos;\n",
        "    text = re.sub(r\"(@@ )|(@@)|(@-@)\", \"\", text)  # 移除 BPE 分词标记 @@\n",
        "    text = re.sub(r\"(&quot; )|(&quot;)|( &quot;)\",'\"', text)\n",
        "\n",
        "    # 2. 修复缩写中的多余空格\n",
        "    text = re.sub(r\"\\b(\\w+)\\s'(\\w+)\\b\", r\"\\1'\\2\", text)\n",
        "\n",
        "    # 3. 移除重复的单词\n",
        "    # text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)  # 重复单词\n",
        "    # text = re.sub(r'(\\b\\w+\\b)( \\1\\b)+', r'\\1', text)  # 连续重复短语\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        outfile.write(text)\n",
        "\n",
        "postprocessing_html('assignments/03/experiments/fr_en_translations.pp.txt', 'assignments/03/experiments/fr_en_translations.ppp.txt')"
      ],
      "metadata": {
        "id": "9SZXZVP0FoJY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab5000"
      ],
      "metadata": {
        "id": "kErxRKh5p9Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/atmt_2024/translate.py \\\n",
        "--data data/en-fr/prepared \\\n",
        "--dicts data/en-fr/prepared \\\n",
        "--checkpoint-path assignments/03/experiments_bpe_1/checkpoints/checkpoint_best.pt \\\n",
        "--output assignments/03/experiments_bpe_1/fr_en_translations.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS022dIOqIUD",
        "outputId": "17d7c28b-fe15-43b2-c8c4-f8dd17b5bf0a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-11-06 22:22:06] COMMAND: /content/drive/MyDrive/atmt_2024/translate.py --data data/en-fr/prepared --dicts data/en-fr/prepared --checkpoint-path assignments/03/experiments_bpe_1/checkpoints/checkpoint_best.pt --output assignments/03/experiments_bpe_1/fr_en_translations.txt\n",
            "[2024-11-06 22:22:06] Arguments: {'cuda': False, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': 'assignments/03/experiments_bpe_1/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.3, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.3, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': 'data/en-fr/prepared', 'checkpoint_path': 'assignments/03/experiments_bpe_1/checkpoints/checkpoint_best.pt', 'output': 'assignments/03/experiments_bpe_1/fr_en_translations.txt', 'max_len': 128}\n",
            "[2024-11-06 22:22:06] Loaded a source dictionary (fr) with 5000 words\n",
            "[2024-11-06 22:22:06] Loaded a target dictionary (en) with 5000 words\n",
            "[2024-11-06 22:22:06] Loaded a model from checkpoint assignments/03/experiments_bpe_1/checkpoints/checkpoint_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/postprocess.sh \\\n",
        "assignments/03/experiments_bpe_1/fr_en_translations.txt \\\n",
        "assignments/03/experiments_bpe_1/fr_en_translations.p.txt en"
      ],
      "metadata": {
        "id": "DvfbhHJwqScZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -r 's/(@@ )|(@@ ?$)//g' assignments/03/experiments_bpe_1/fr_en_translations.p.txt > assignments/03/experiments_bpe_1/fr_en_translations.pp.txt"
      ],
      "metadata": {
        "id": "ILefcRrAqgoA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "postprocessing_html('assignments/03/experiments_bpe_1/fr_en_translations.pp.txt', 'assignments/03/experiments_bpe_1/fr_en_translations.ppp.txt')"
      ],
      "metadata": {
        "id": "Fya_tBsOqaZG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQdou4Euqavo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU"
      ],
      "metadata": {
        "id": "JApbAxVkn2uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab4000"
      ],
      "metadata": {
        "id": "pSc2VdaFp_pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiments/fr_en_translations.ppp.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPmqgZqiqyFo",
        "outputId": "691fb169-4f62-4b7a-9064-822c46f87d55"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 17.2,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"46.6/22.4/12.3/6.8 (BP = 1.000 ratio = 1.199 hyp_len = 4668 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab5000"
      ],
      "metadata": {
        "id": "gm9MmUSsqCym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "assignments/03/experiments_bpe_1/fr_en_translations.ppp.txt \\\n",
        "| sacrebleu data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc5t_zfEqtIi",
        "outputId": "d8eb6e72-2399-4a72-9472-bf88940ed593"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 18.9,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"48.6/24.1/13.9/7.9 (BP = 1.000 ratio = 1.173 hyp_len = 4564 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}